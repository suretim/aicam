{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a9c438-69d3-427f-a61b-0aedac236429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contrastive] Epoch 1/10, loss=0.4867\n",
      "[Contrastive] Epoch 2/10, loss=0.5248\n",
      "[Contrastive] Epoch 3/10, loss=0.2792\n",
      "[Contrastive] Epoch 4/10, loss=0.2793\n",
      "[Contrastive] Epoch 5/10, loss=0.2837\n",
      "[Contrastive] Epoch 6/10, loss=0.1264\n",
      "[Contrastive] Epoch 7/10, loss=0.7225\n",
      "[Contrastive] Epoch 8/10, loss=0.1968\n",
      "[Contrastive] Epoch 9/10, loss=0.2291\n",
      "[Contrastive] Epoch 10/10, loss=0.1127\n",
      "[Meta] Epoch 1/20, loss=1.1468, acc=0.0100\n",
      "[Meta] Epoch 2/20, loss=1.0843, acc=0.4500\n",
      "[Meta] Epoch 3/20, loss=1.0389, acc=0.9500\n",
      "[Meta] Epoch 4/20, loss=0.9904, acc=0.9700\n",
      "[Meta] Epoch 5/20, loss=0.9461, acc=0.9500\n",
      "[Meta] Epoch 6/20, loss=0.8898, acc=0.9800\n",
      "[Meta] Epoch 7/20, loss=0.8528, acc=0.9500\n",
      "[Meta] Epoch 8/20, loss=0.8277, acc=0.9200\n",
      "[Meta] Epoch 9/20, loss=0.7704, acc=0.9500\n",
      "[Meta] Epoch 10/20, loss=0.7275, acc=0.9400\n",
      "[Meta] Epoch 11/20, loss=0.6835, acc=0.9500\n",
      "[Meta] Epoch 12/20, loss=0.6144, acc=0.9600\n",
      "[Meta] Epoch 13/20, loss=0.5727, acc=0.9600\n",
      "[Meta] Epoch 14/20, loss=0.5344, acc=0.9500\n",
      "[Meta] Epoch 15/20, loss=0.5355, acc=0.9100\n",
      "[Meta] Epoch 16/20, loss=0.4477, acc=0.9500\n",
      "[Meta] Epoch 17/20, loss=0.4566, acc=0.9300\n",
      "[Meta] Epoch 18/20, loss=0.4033, acc=0.9300\n",
      "[Meta] Epoch 19/20, loss=0.3746, acc=0.9300\n",
      "[Meta] Epoch 20/20, loss=0.2574, acc=0.9800\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpq9lilzz4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpq9lilzz4/assets\n",
      "2025-08-19 07:00:55.510862: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-19 07:00:55.510899: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-19 07:00:55.511015: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpq9lilzz4\n",
      "2025-08-19 07:00:55.514462: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-19 07:00:55.514477: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpq9lilzz4\n",
      "2025-08-19 07:00:55.525885: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-19 07:00:55.562703: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpq9lilzz4\n",
      "2025-08-19 07:00:55.594813: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 83796 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite: lstm_encoder_contrastive.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpa9b6qmsi/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpa9b6qmsi/assets\n",
      "2025-08-19 07:00:59.439613: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-19 07:00:59.439645: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-19 07:00:59.439761: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpa9b6qmsi\n",
      "2025-08-19 07:00:59.444125: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-19 07:00:59.444142: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpa9b6qmsi\n",
      "2025-08-19 07:00:59.457786: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-19 07:00:59.500172: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpa9b6qmsi\n",
      "2025-08-19 07:00:59.535591: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 95829 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite: meta_lstm_classifier.tflite\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Meta-learning pipeline with HVAC-aware features and flowering-period focus.\n",
    "- Expects CSV columns: temp, humid, light, ac, heater, dehum, hum, label\n",
    "- Sliding windows -> contrastive learning (unlabeled) + FOMAML with LLL + EWC (labeled)\n",
    "- Encoder: LSTM on continuous features only (temp/humid/light)\n",
    "- Additional HVAC features: mean on/off rate + toggle rate (abs(diff)) over time\n",
    "- Gradient boost on flowering period with abnormal HVAC toggling\n",
    "- TFLite export restricted to TFLITE_BUILTINS\n",
    "\"\"\"\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================\n",
    "# Hyperparameters\n",
    "# =============================\n",
    "DATA_GLOB = \"../../../../lll_data/*.csv\"\n",
    "SEQ_LEN = 64\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_CONTRASTIVE = 10\n",
    "EPOCHS_META = 20\n",
    "INNER_LR = 1e-2\n",
    "META_LR = 1e-3\n",
    "NUM_CLASSES = 3\n",
    "NUM_TASKS = 5\n",
    "SUPPORT_SIZE = 10\n",
    "QUERY_SIZE = 20\n",
    "REPLAY_CAPACITY = 1000\n",
    "REPLAY_WEIGHT = 0.3\n",
    "LAMBDA_EWC = 1e-3\n",
    "FLOWERING_WEIGHT = 2.0  # gradient boost upper bound for flowering-focus\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# =============================\n",
    "# 1) Load CSV -> Sliding windows\n",
    "# =============================\n",
    "X_labeled_list, y_labeled_list = [], []\n",
    "X_unlabeled_list = []\n",
    "\n",
    "for file in sorted(glob.glob(DATA_GLOB)):\n",
    "    df = pd.read_csv(file).fillna(-1)\n",
    "    data = df.values.astype(np.float32)\n",
    "    feats, labels = data[:, :-1], data[:, -1]\n",
    "    for i in range(len(data) - SEQ_LEN + 1):\n",
    "        w_x = feats[i:i + SEQ_LEN]\n",
    "        w_y = labels[i + SEQ_LEN - 1]\n",
    "        if w_y == -1:\n",
    "            X_unlabeled_list.append(w_x)\n",
    "        else:\n",
    "            X_labeled_list.append(w_x)\n",
    "            y_labeled_list.append(int(w_y))\n",
    "\n",
    "X_unlabeled = np.array(X_unlabeled_list, dtype=np.float32) if len(X_unlabeled_list) > 0 else np.empty((0,), dtype=np.float32)\n",
    "if len(X_labeled_list) > 0:\n",
    "    X_labeled = np.array(X_labeled_list, dtype=np.float32)\n",
    "    y_labeled = np.array(y_labeled_list, dtype=np.int32)\n",
    "else:\n",
    "    # fallback shape (not ideal; expect at least unlabeled data present)\n",
    "    X_labeled = np.empty((0, SEQ_LEN, X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7), dtype=np.float32)\n",
    "    y_labeled = np.empty((0,), dtype=np.int32)\n",
    "\n",
    "NUM_FEATS = X_labeled.shape[2] if X_labeled.size > 0 else (X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7)\n",
    "\n",
    "# Verify hvac columns exist\n",
    "if NUM_FEATS < 7:\n",
    "    raise ValueError(\"Expected at least 7 features per timestep: [temp, humid, light, ac, heater, dehum, hum]. Found: %d\" % NUM_FEATS)\n",
    "print(\"NUM_FEATS:\",NUM_FEATS)\n",
    "# Index conventions\n",
    "CONT_IDX = [0, 1, 2]   # temp, humid, light\n",
    "HVAC_IDX = [3, 4, 5, 6]  # ac, heater, dehum, hum\n",
    "\n",
    "# =============================\n",
    "# 2) Contrastive learning\n",
    "# =============================\n",
    "def augment_window(x):\n",
    "    \"\"\"Only perturb continuous channels to keep binary HVAC channels intact.\"\"\"\n",
    "    x_aug = x.copy()\n",
    "    x_aug[:, CONT_IDX] = x[:, CONT_IDX] + np.random.normal(0, 0.01, x[:, CONT_IDX].shape).astype(np.float32)\n",
    "    return x_aug\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for w in X:\n",
    "        anchors.append(w)\n",
    "        positives.append(augment_window(w))\n",
    "    return np.stack(anchors).astype(np.float32), np.stack(positives).astype(np.float32)\n",
    "\n",
    "class NTXentLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.2):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    def call(self, z_i, z_j):\n",
    "        z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "        z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "        logits = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_i)[0])\n",
    "        loss_i = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_j = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        return tf.reduce_mean(loss_i + loss_j)\n",
    "\n",
    "# =============================\n",
    "# 2) LSTM Encoder (unroll=True, continuous only)\n",
    "# =============================\n",
    "def build_lstm_encoder(seq_len, num_feats, feature_dim=FEATURE_DIM):\n",
    "    inp = layers.Input(shape=(seq_len, num_feats))\n",
    "    x_cont = layers.Lambda(lambda z: z[:, :, :3])(inp)  # [B,T,3]\n",
    "    x = layers.LSTM(feature_dim, unroll=True)(x_cont)\n",
    "    out = layers.Dense(feature_dim, activation=\"relu\")(x)\n",
    "    return models.Model(inp, out, name=\"lstm_encoder\")\n",
    "\n",
    "# =============================\n",
    "# 3) Meta Model (Encoder + HVAC features)\n",
    "# =============================\n",
    "def build_meta_model(encoder, num_classes=NUM_CLASSES):\n",
    "    inp = layers.Input(shape=(SEQ_LEN, NUM_FEATS))   # expect >=7\n",
    "    z_enc = encoder(inp)  # [B, FEATURE_DIM]\n",
    "\n",
    "    # HVAC slice\n",
    "    hvac = layers.Lambda(lambda z: z[:, :, 3:7])(inp)   # [B,T,4]\n",
    "    hvac_mean = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1))(hvac)  # [B,4]\n",
    "\n",
    "    # Toggle rate via abs(diff)\n",
    "    hvac_shift = layers.Lambda(lambda z: z[:, 1:, :])(hvac)      # [B,T-1,4]\n",
    "    hvac_prev  = layers.Lambda(lambda z: z[:, :-1, :])(hvac)     # [B,T-1,4]\n",
    "    hvac_diff  = layers.Lambda(lambda t: tf.abs(t[0] - t[1]))([hvac_shift, hvac_prev])  # [B,T-1,4]\n",
    "    hvac_toggle_rate = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1))(hvac_diff)    # [B,4]\n",
    "\n",
    "    hvac_feat = layers.Concatenate()([hvac_mean, hvac_toggle_rate])  # [B,8]\n",
    "    hvac_feat = layers.Dense(16, activation=\"relu\")(hvac_feat)\n",
    "\n",
    "    x = layers.Concatenate()([z_enc, hvac_feat])\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"meta_lstm_classifier\")\n",
    "\n",
    "# Build models\n",
    "lstm_encoder = build_lstm_encoder(SEQ_LEN, NUM_FEATS, FEATURE_DIM)\n",
    "contrastive_opt = tf.keras.optimizers.Adam()\n",
    "ntxent = NTXentLoss(temperature=0.2)\n",
    "\n",
    "# Provide unlabeled fallback if none\n",
    "if X_unlabeled.size == 0:\n",
    "    X_unlabeled = np.random.randn(200, SEQ_LEN, NUM_FEATS).astype(np.float32)\n",
    "\n",
    "anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "contrast_ds = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(2048).batch(BATCH_SIZE)\n",
    "\n",
    "# Train contrastive\n",
    "contrastive_loss_history = []\n",
    "for ep in range(EPOCHS_CONTRASTIVE):\n",
    "    for a, p in contrast_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            za = lstm_encoder(a, training=True)\n",
    "            zp = lstm_encoder(p, training=True)\n",
    "            loss = ntxent(za, zp)\n",
    "        grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "        contrastive_opt.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "    contrastive_loss_history.append(float(loss.numpy()))\n",
    "    print(f\"[Contrastive] Epoch {ep+1}/{EPOCHS_CONTRASTIVE}, loss={float(loss.numpy()):.4f}\")\n",
    "\n",
    "# Meta model\n",
    "meta_model = build_meta_model(lstm_encoder, NUM_CLASSES)\n",
    "meta_optimizer = tf.keras.optimizers.Adam(META_LR)\n",
    "\n",
    "def sample_tasks(X, y, num_tasks=NUM_TASKS, support_size=SUPPORT_SIZE, query_size=QUERY_SIZE):\n",
    "    tasks = []\n",
    "    n = len(X)\n",
    "    if n < support_size + query_size:\n",
    "        raise ValueError(f\"Not enough labeled samples to build tasks: need {support_size+query_size}, got {n}\")\n",
    "    for _ in range(num_tasks):\n",
    "        idx = np.random.choice(n, support_size + query_size, replace=False)\n",
    "        X_support, y_support = X[idx[:support_size]], y[idx[:support_size]]\n",
    "        X_query, y_query = X[idx[support_size:]], y[idx[support_size:]]\n",
    "        tasks.append((X_support, y_support, X_query, y_query))\n",
    "    return tasks\n",
    "\n",
    "def inner_update(model, X_support, y_support, lr_inner=INNER_LR):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds_support = model(X_support, training=True)\n",
    "        loss_support = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_support, preds_support))\n",
    "    grads_inner = tape.gradient(loss_support, model.trainable_variables)\n",
    "    updated_vars = [w - lr_inner * g for w, g in zip(model.trainable_variables, grads_inner)]\n",
    "    return updated_vars\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=REPLAY_CAPACITY):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.n_seen = 0\n",
    "    def add(self, X, y):\n",
    "        for xi, yi in zip(X, y):\n",
    "            self.n_seen += 1\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append((xi, yi))\n",
    "            else:\n",
    "                r = np.random.randint(0, self.n_seen)\n",
    "                if r < self.capacity:\n",
    "                    self.buffer[r] = (xi, yi)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        X_s, y_s = zip(*[self.buffer[i] for i in idxs])\n",
    "        return np.array(X_s), np.array(y_s)\n",
    "\n",
    "memory = ReplayBuffer(capacity=REPLAY_CAPACITY)\n",
    "\n",
    "# ===== Helpers for flowering focus =====\n",
    "def is_flowering_seq(x_seq, light_idx=2, th_light=550.0):\n",
    "    light_mean = float(np.mean(x_seq[:, light_idx]))\n",
    "    return light_mean >= th_light\n",
    "\n",
    "def hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15):\n",
    "    hv = x_seq[:, hvac_slice]  # [T,4]\n",
    "    if hv.shape[0] < 2:\n",
    "        return 0.0, False\n",
    "    diff = np.abs(hv[1:] - hv[:-1])   # [T-1,4]\n",
    "    rate = float(diff.mean())\n",
    "    return rate, rate >= th_toggle\n",
    "\n",
    "def outer_update_with_lll(meta_model, meta_optimizer, tasks,\n",
    "                          lr_inner=INNER_LR, replay_weight=REPLAY_WEIGHT,\n",
    "                          lambda_ewc=LAMBDA_EWC, prev_weights=None):\n",
    "    meta_grads = [tf.zeros_like(v) for v in meta_model.trainable_variables]\n",
    "    query_acc_list, query_loss_list = [], []\n",
    "\n",
    "    for X_support, y_support, X_query, y_query in tasks:\n",
    "        orig_vars = [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "        # inner update\n",
    "        updated_vars = inner_update(meta_model, X_support, y_support)\n",
    "        for var, upd in zip(meta_model.trainable_variables, updated_vars):\n",
    "            var.assign(upd)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds_q = meta_model(X_query, training=True)\n",
    "            loss_q = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_query, preds_q))\n",
    "            loss_total = loss_q\n",
    "\n",
    "            # replay\n",
    "            if len(memory) >= 8:\n",
    "                X_old, y_old = memory.sample(batch_size=32)\n",
    "                preds_old = meta_model(X_old, training=True)\n",
    "                replay_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_old, preds_old))\n",
    "                loss_total = (1 - replay_weight) * loss_total + replay_weight * replay_loss\n",
    "\n",
    "            # EWC (L2 to prev weights as proxy)\n",
    "            if prev_weights is not None:\n",
    "                ewc_loss = 0.0\n",
    "                for w, pw in zip(meta_model.trainable_variables, prev_weights):\n",
    "                    ewc_loss += tf.reduce_sum(tf.square(w - pw))\n",
    "                loss_total += lambda_ewc * ewc_loss\n",
    "\n",
    "        grads = tape.gradient(loss_total, meta_model.trainable_variables)\n",
    "\n",
    "        # ===== Flowering + HVAC toggling gradient boost =====\n",
    "        flowering_mask = []\n",
    "        toggle_scores = []\n",
    "        for i in range(len(X_query)):\n",
    "            x_seq = X_query[i]  # [T,D]\n",
    "            flw = is_flowering_seq(x_seq, light_idx=2, th_light=550.0)\n",
    "            tscore, tabove = hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15)\n",
    "            flowering_mask.append(bool(flw and tabove))\n",
    "            toggle_scores.append(tscore)\n",
    "\n",
    "        if any(flowering_mask):\n",
    "            ratio = sum(flowering_mask) / len(flowering_mask)\n",
    "            mean_toggle = np.mean([t for m,t in zip(flowering_mask, toggle_scores) if m]) if any(flowering_mask) else 0.0\n",
    "            toggle_boost = min(1.0 + float(mean_toggle)*2.0, FLOWERING_WEIGHT)\n",
    "            boost = 1.0 + (FLOWERING_WEIGHT - 1.0) * ratio\n",
    "            total_boost = float(min(boost * toggle_boost, FLOWERING_WEIGHT))\n",
    "            grads = [g * total_boost for g in grads]\n",
    "\n",
    "        meta_grads = [mg + g / len(tasks) for mg, g in zip(meta_grads, grads)]\n",
    "\n",
    "        q_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(preds_q, axis=1), y_query), tf.float32))\n",
    "        query_acc_list.append(float(q_acc.numpy()))\n",
    "        query_loss_list.append(float(loss_q.numpy()))\n",
    "\n",
    "        # restore original vars\n",
    "        for var, orig in zip(meta_model.trainable_variables, orig_vars):\n",
    "            var.assign(orig)\n",
    "\n",
    "        # update memory\n",
    "        memory.add(X_support, y_support)\n",
    "        memory.add(X_query, y_query)\n",
    "\n",
    "    meta_optimizer.apply_gradients(zip(meta_grads, meta_model.trainable_variables))\n",
    "    return float(np.mean(query_loss_list)), float(np.mean(query_acc_list)), [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "# ======= Train meta-learning =======\n",
    "meta_loss_history, meta_acc_history = [], []\n",
    "prev_weights = None\n",
    "\n",
    "if X_labeled.size > 0:\n",
    "    for ep in range(EPOCHS_META):\n",
    "        tasks = sample_tasks(X_labeled, y_labeled)\n",
    "        loss, acc, prev_weights = outer_update_with_lll(meta_model, meta_optimizer, tasks, prev_weights=prev_weights)\n",
    "        meta_loss_history.append(loss)\n",
    "        meta_acc_history.append(acc)\n",
    "        print(f\"[Meta] Epoch {ep+1}/{EPOCHS_META}, loss={loss:.4f}, acc={acc:.4f}\")\n",
    "else:\n",
    "    print(\"Skip meta-learning: no labeled data.\")\n",
    "'''\n",
    "# =============================\n",
    "# 4) Visualization\n",
    "# =============================\n",
    "plt.figure()\n",
    "plt.plot(contrastive_loss_history, label=\"Contrastive Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Contrastive Learning\")\n",
    "plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "\n",
    "if meta_loss_history:\n",
    "    plt.figure()\n",
    "    plt.plot(meta_loss_history, label=\"Query Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"FOMAML + LLL + EWC Loss\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(meta_acc_history, label=\"Query Accuracy\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"FOMAML + LLL + EWC Accuracy\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "'''\n",
    "# =============================\n",
    "# 5) TFLite export (BUILTINS only)\n",
    "# =============================\n",
    "def save_tflite(model, out_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved TFLite:\", out_path)\n",
    "\n",
    "# Save models\n",
    "save_tflite(lstm_encoder, \"lstm_encoder_contrastive.tflite\")\n",
    "if X_labeled.size > 0:\n",
    "    save_tflite(meta_model, \"meta_lstm_classifier.tflite\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c85936-adf6-4cdb-98cb-cf58719592d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv2)",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
