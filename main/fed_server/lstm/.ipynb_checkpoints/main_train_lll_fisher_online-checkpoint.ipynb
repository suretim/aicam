{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93ab9b20-b715-4f33-a24f-5cfab6d51a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contrastive] Epoch 1/10, loss=0.9898\n",
      "[Contrastive] Epoch 2/10, loss=0.5418\n",
      "[Contrastive] Epoch 3/10, loss=0.5915\n",
      "[Contrastive] Epoch 4/10, loss=0.2044\n",
      "[Contrastive] Epoch 5/10, loss=0.2235\n",
      "[Contrastive] Epoch 6/10, loss=0.7443\n",
      "[Contrastive] Epoch 7/10, loss=0.1826\n",
      "[Contrastive] Epoch 8/10, loss=0.2345\n",
      "[Contrastive] Epoch 9/10, loss=0.1065\n",
      "[Contrastive] Epoch 10/10, loss=0.8159\n",
      "âœ… Encoder mode: freeze, last_n=N/A\n",
      "\n",
      "ğŸ” [Encoder trainable layers]\n",
      "\n",
      "ğŸ” [Meta model trainable layers]\n",
      "hvac_dense           âœ… trainable\n",
      "meta_dense_64        âœ… trainable\n",
      "meta_dense_32        âœ… trainable\n",
      "Layer 09: hvac_dense           âœ… trainable\n",
      "Layer 11: meta_dense_64        âœ… trainable\n",
      "Layer 12: meta_dense_32        âœ… trainable\n",
      "[Meta] Epoch 1/20, loss=1.1594, acc=0.0300\n",
      "[Meta] Epoch 2/20, loss=1.1195, acc=0.0600\n",
      "[Meta] Epoch 3/20, loss=1.0994, acc=0.4600\n",
      "[Meta] Epoch 4/20, loss=1.0600, acc=0.5800\n",
      "[Meta] Epoch 5/20, loss=1.0472, acc=0.5000\n",
      "[Meta] Epoch 6/20, loss=1.0238, acc=0.9600\n",
      "[Meta] Epoch 7/20, loss=0.9970, acc=0.9700\n",
      "[Meta] Epoch 8/20, loss=0.9656, acc=0.9600\n",
      "[Meta] Epoch 9/20, loss=0.9504, acc=0.9600\n",
      "[Meta] Epoch 10/20, loss=0.9326, acc=0.9600\n",
      "[Meta] Epoch 11/20, loss=0.8993, acc=0.9600\n",
      "[Meta] Epoch 12/20, loss=0.8932, acc=0.9400\n",
      "[Meta] Epoch 13/20, loss=0.8684, acc=0.9400\n",
      "[Meta] Epoch 14/20, loss=0.8449, acc=0.9200\n",
      "[Meta] Epoch 15/20, loss=0.8239, acc=0.9600\n",
      "[Meta] Epoch 16/20, loss=0.7860, acc=0.9800\n",
      "[Meta] Epoch 17/20, loss=0.7668, acc=0.9800\n",
      "[Meta] Epoch 18/20, loss=0.7419, acc=0.9700\n",
      "[Meta] Epoch 19/20, loss=0.7264, acc=0.9700\n",
      "[Meta] Epoch 20/20, loss=0.7022, acc=0.9800\n",
      "layer_shapes.json saved!\n",
      "âœ… Trainable weights + Fisher matrix saved to ewc_assets\n",
      "  Total arrays saved: 12\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp9wxs56_p/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9wxs56_p/assets\n",
      "2025-08-22 13:00:06.054167: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-22 13:00:06.054233: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-22 13:00:06.054397: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp9wxs56_p\n",
      "2025-08-22 13:00:06.056299: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-22 13:00:06.056318: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp9wxs56_p\n",
      "2025-08-22 13:00:06.061688: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-22 13:00:06.081824: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp9wxs56_p\n",
      "2025-08-22 13:00:06.094994: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 40595 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite: lstm_encoder_contrastive.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpu2igp4bn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpu2igp4bn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite: meta_lstm_classifier.tflite\n",
      "Trainable: meta_lstm_classifier/meta_dense_64/BiasAdd/ReadVariableOp, index=1, shape=[64]\n",
      "Trainable: meta_lstm_classifier/meta_dense_32/BiasAdd/ReadVariableOp, index=2, shape=[32]\n",
      "Trainable: meta_lstm_classifier/hvac_dense/BiasAdd/ReadVariableOp, index=5, shape=[16]\n",
      "Trainable: meta_lstm_classifier/hvac_dense/MatMul1, index=18, shape=[16  8]\n",
      "Trainable: meta_lstm_classifier/meta_dense_64/MatMul, index=22, shape=[64 80]\n",
      "Trainable: meta_lstm_classifier/meta_dense_32/MatMul, index=23, shape=[32 64]\n",
      "trainable_tensor_indices = [1, 2, 5, 18, 22, 23]\n",
      "meta_lstm_classifier.tflite Done.\n",
      "âœ… æ¸¬è©¦å‰å‘è¼¸å‡º shape: (1, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 13:00:08.226736: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-22 13:00:08.226810: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-22 13:00:08.226999: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpu2igp4bn\n",
      "2025-08-22 13:00:08.230027: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-22 13:00:08.230055: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpu2igp4bn\n",
      "2025-08-22 13:00:08.239346: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-22 13:00:08.270839: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpu2igp4bn\n",
      "2025-08-22 13:00:08.288502: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 61502 microseconds.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Meta-learning pipeline with HVAC-aware features and flowering-period focus.\n",
    "- Expects CSV columns: temp, humid, light, ac, heater, dehum, hum, label\n",
    "- Sliding windows -> contrastive learning (unlabeled) + FOMAML with LLL + EWC (labeled)\n",
    "- Encoder: LSTM on continuous features only (temp/humid/light)\n",
    "- Additional HVAC features: mean on/off rate + toggle rate (abs(diff)) over time\n",
    "- Gradient boost on flowering period with abnormal HVAC toggling\n",
    "- TFLite export restricted to TFLITE_BUILTINS\n",
    "- Includes Fisher matrix computation and loading for EWC\n",
    "\"\"\"\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "import argparse \n",
    "import datetime\n",
    "import json\n",
    "# =============================\n",
    "# Hyperparameters\n",
    "# =============================\n",
    "DATA_GLOB = \"../../../../lll_data/*.csv\"\n",
    "SEQ_LEN = 10\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_CONTRASTIVE = 10\n",
    "EPOCHS_META = 20\n",
    "INNER_LR = 1e-2\n",
    "META_LR = 1e-3\n",
    "NUM_CLASSES = 3\n",
    "NUM_TASKS = 5\n",
    "SUPPORT_SIZE = 10\n",
    "QUERY_SIZE = 20\n",
    "REPLAY_CAPACITY = 1000\n",
    "REPLAY_WEIGHT = 0.3\n",
    "LAMBDA_EWC = 1e-3\n",
    "NUM_FEATS=7\n",
    "#ENCODER_MODE = finetune  freeze last_n\n",
    "ENCODER_MODE =\"freeze\"\n",
    "LAST_N= 1\n",
    "FLOWERING_WEIGHT = 2.0  # gradient boost upper bound for flowering-focus\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "# Index conventions\n",
    "CONT_IDX = [0, 1, 2]   # temp, humid, light\n",
    "HVAC_IDX = [3, 4, 5, 6]  # ac, heater, dehum, hum\n",
    " \n",
    "\n",
    "def make_indices(model_path=\"meta_lstm_classifier.tflite\", header_path=\"trainable_tensor_indices.h\"):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "    trainable_indices = []\n",
    "\n",
    "    for t in tensor_details:\n",
    "        name = t['name']\n",
    "        idx  = t['index']\n",
    "        shape = t['shape']\n",
    "\n",
    "        # åƒ…é¸ Dense å±¤æˆ– hvac_dense çš„ kernel/bias\n",
    "        if (\"meta_dense\" in name or \"hvac_dense\" in name):\n",
    "            # éæ¿¾æ‰ fused/activation tensor\n",
    "            if \"Relu\" in name or \";\" in name:\n",
    "                continue\n",
    "\n",
    "            # bias ä¸€èˆ¬æ˜¯ 1Dï¼Œkernel ä¸€èˆ¬æ˜¯ 2D\n",
    "            if len(shape) == 1 or len(shape) == 2:\n",
    "                print(f\"Trainable: {name}, index={idx}, shape={shape}\")\n",
    "                trainable_indices.append(idx)\n",
    "\n",
    "    # ç”Ÿæˆ C å¤´æ–‡ä»¶\n",
    "    with open(header_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\")\n",
    "        f.write(f\"const int trainable_tensor_indices[] = {{{', '.join(map(str, trainable_indices))}}};\\n\")\n",
    "        f.write(f\"const int trainable_tensor_count = {len(trainable_indices)};\\n\")\n",
    "    print(\"trainable_tensor_indices =\", trainable_indices)\n",
    "    return trainable_indices\n",
    "  \n",
    "def generate_trainable_tensor_indices0(model, tflite_model_path, header_path=\"trainable_tensor_indices.h\"):\n",
    "    variable_names = [v.name for v in model.trainable_variables]\n",
    "    print(\"Python trainable variables:\")\n",
    "    for i, name in enumerate(variable_names):\n",
    "        print(i, name)\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "    trainable_tensor_indices = []\n",
    "\n",
    "    for v_name in variable_names:\n",
    "        v_name_clean = v_name.split(':')[0]  # å»æ‰ \":0\"\n",
    "        matched = False\n",
    "\n",
    "        v_last = v_name_clean.split('/')[-1]  # kernel æˆ– bias\n",
    "        for t in tensor_details:\n",
    "            t_last = t['name'].split('/')[-1]\n",
    "            if v_last == t_last:\n",
    "                trainable_tensor_indices.append(t['index'])\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            print(f\"Warning: variable {v_name_clean} not found in tflite tensors!\")\n",
    "\n",
    "    print(\"trainable_tensor_indices =\", trainable_tensor_indices)\n",
    "\n",
    "    # å¯é€‰ï¼šç”Ÿæˆ C å¤´æ–‡ä»¶\n",
    "    with open(header_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\")\n",
    "        f.write(f\"const int trainable_tensor_indices[] = {{{', '.join(map(str, trainable_tensor_indices))}}};\\n\")\n",
    "        f.write(f\"const int trainable_tensor_count = {len(trainable_tensor_indices)};\\n\")\n",
    " \n",
    "\n",
    "\n",
    "def build_csv_data(data_glob):    \n",
    "    \n",
    "    # =============================\n",
    "    # 1) Load CSV -> Sliding windows\n",
    "    # =============================\n",
    "    X_labeled_list, y_labeled_list = [], []\n",
    "    X_unlabeled_list = []\n",
    "    \n",
    "    for file in sorted(glob.glob(data_glob)):\n",
    "        df = pd.read_csv(file).fillna(-1)\n",
    "        data = df.values.astype(np.float32)\n",
    "        feats, labels = data[:, :-1], data[:, -1]\n",
    "        for i in range(len(data) - SEQ_LEN + 1):\n",
    "            w_x = feats[i:i + SEQ_LEN]\n",
    "            w_y = labels[i + SEQ_LEN - 1]\n",
    "            if w_y == -1:\n",
    "                X_unlabeled_list.append(w_x)\n",
    "            else:\n",
    "                X_labeled_list.append(w_x)\n",
    "                y_labeled_list.append(int(w_y))\n",
    "    \n",
    "    X_unlabeled = np.array(X_unlabeled_list, dtype=np.float32) if len(X_unlabeled_list) > 0 else np.empty((0,), dtype=np.float32)\n",
    "    if len(X_labeled_list) > 0:\n",
    "        X_labeled = np.array(X_labeled_list, dtype=np.float32)\n",
    "        y_labeled = np.array(y_labeled_list, dtype=np.int32)\n",
    "    else:\n",
    "        X_labeled = np.empty((0, SEQ_LEN, X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7), dtype=np.float32)\n",
    "        y_labeled = np.empty((0,), dtype=np.int32)\n",
    "    \n",
    "    NUM_FEATS = X_labeled.shape[2] if X_labeled.size > 0 else (X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7)\n",
    "    \n",
    "    if NUM_FEATS < 7:\n",
    "        raise ValueError(\"Expected at least 7 features per timestep: [temp, humid, light, ac, heater, dehum, hum]. Found: %d\" % NUM_FEATS)\n",
    "\n",
    "    # Provide unlabeled fallback if none\n",
    "    if X_unlabeled.size == 0:\n",
    "        X_unlabeled = np.random.randn(200, SEQ_LEN, NUM_FEATS).astype(np.float32)\n",
    " \n",
    "    return X_unlabeled,X_labeled, y_labeled\n",
    "# =============================\n",
    "# 2) Contrastive learning\n",
    "# =============================\n",
    "def augment_window(x):\n",
    "    \"\"\"Only perturb continuous channels to keep binary HVAC channels intact.\"\"\"\n",
    "    x_aug = x.copy()\n",
    "    x_aug[:, CONT_IDX] = x[:, CONT_IDX] + np.random.normal(0, 0.01, x[:, CONT_IDX].shape).astype(np.float32)\n",
    "    return x_aug\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for w in X:\n",
    "        anchors.append(w)\n",
    "        positives.append(augment_window(w))\n",
    "    return np.stack(anchors).astype(np.float32), np.stack(positives).astype(np.float32)\n",
    "\n",
    "class NTXentLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.2):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    def call(self, z_i, z_j):\n",
    "        z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "        z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "        logits = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_i)[0])\n",
    "        loss_i = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_j = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        return tf.reduce_mean(loss_i + loss_j)\n",
    "\n",
    "# =============================\n",
    "# 3) LSTM Encoder (unroll=True, continuous only)\n",
    "# =============================\n",
    "   \n",
    "def build_lstm_encoder(seq_len, num_feats, feature_dim=FEATURE_DIM):\n",
    "    inp = layers.Input(shape=(seq_len, num_feats), name=\"encoder_input\")\n",
    "    x_cont = layers.Lambda(lambda z: z[:, :, :3], name=\"encoder_lambda\")(inp)  # [B,T,3]\n",
    "    x = layers.LSTM(feature_dim, unroll=True, name=\"encoder_lstm\")(x_cont)\n",
    "    out = layers.Dense(feature_dim, activation=\"relu\", name=\"encoder_dense\")(x)\n",
    "    return models.Model(inp, out, name=\"lstm_encoder\")\n",
    "# =============================\n",
    "# 4) Meta Model (Encoder + HVAC features)\n",
    "# =============================\n",
    "def build_meta_model(encoder, num_classes=NUM_CLASSES):\n",
    "    inp = layers.Input(shape=(SEQ_LEN, NUM_FEATS), name=\"meta_input\")\n",
    "    z_enc = encoder(inp)  # [B, FEATURE_DIM]\n",
    "\n",
    "    # HVAC slice\n",
    "    hvac = layers.Lambda(lambda z: z[:, :, 3:7], name=\"hvac_slice\")(inp)   # [B,T,4]\n",
    "    hvac_mean = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1), name=\"hvac_mean\")(hvac)  # [B,4]\n",
    "\n",
    "    # Toggle rate via abs(diff)\n",
    "    hvac_shift = layers.Lambda(lambda z: z[:, 1:, :], name=\"hvac_shift\")(hvac)      # [B,T-1,4]\n",
    "    hvac_prev  = layers.Lambda(lambda z: z[:, :-1, :], name=\"hvac_prev\")(hvac)     # [B,T-1,4]\n",
    "    hvac_diff  = layers.Lambda(lambda t: tf.abs(t[0] - t[1]), name=\"hvac_diff\")([hvac_shift, hvac_prev])  # [B,T-1,4]\n",
    "    hvac_toggle_rate = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1), name=\"hvac_toggle_rate\")(hvac_diff)    # [B,4]\n",
    "\n",
    "    hvac_feat = layers.Concatenate(name=\"hvac_concat\")([hvac_mean, hvac_toggle_rate])  # [B,8]\n",
    "    hvac_feat = layers.Dense(16, activation=\"relu\", name=\"hvac_dense\")(hvac_feat)\n",
    "\n",
    "    x = layers.Concatenate(name=\"encoder_hvac_concat\")([z_enc, hvac_feat])\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"meta_dense_64\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\", name=\"meta_dense_32\")(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\", name=\"meta_out\")(x)\n",
    "\n",
    "    return models.Model(inp, out, name=\"meta_lstm_classifier\")\n",
    "\n",
    "\n",
    "def sample_tasks(X, y, num_tasks=NUM_TASKS, support_size=SUPPORT_SIZE, query_size=QUERY_SIZE):\n",
    "    tasks = []\n",
    "    n = len(X)\n",
    "    if n < support_size + query_size:\n",
    "        raise ValueError(f\"Not enough labeled samples to build tasks: need {support_size+query_size}, got {n}\")\n",
    "    for _ in range(num_tasks):\n",
    "        idx = np.random.choice(n, support_size + query_size, replace=False)\n",
    "        X_support, y_support = X[idx[:support_size]], y[idx[:support_size]]\n",
    "        X_query, y_query = X[idx[support_size:]], y[idx[support_size:]]\n",
    "        tasks.append((X_support, y_support, X_query, y_query))\n",
    "    return tasks\n",
    "\n",
    "def inner_update(model, X_support, y_support, lr_inner=INNER_LR):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds_support = model(X_support, training=True)\n",
    "        loss_support = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_support, preds_support))\n",
    "    grads_inner = tape.gradient(loss_support, model.trainable_variables)\n",
    "    updated_vars = [w - lr_inner * g for w, g in zip(model.trainable_variables, grads_inner)]\n",
    "    return updated_vars\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=REPLAY_CAPACITY):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.n_seen = 0\n",
    "    def add(self, X, y):\n",
    "        for xi, yi in zip(X, y):\n",
    "            self.n_seen += 1\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append((xi, yi))\n",
    "            else:\n",
    "                r = np.random.randint(0, self.n_seen)\n",
    "                if r < self.capacity:\n",
    "                    self.buffer[r] = (xi, yi)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        X_s, y_s = zip(*[self.buffer[i] for i in idxs])\n",
    "        return np.array(X_s), np.array(y_s)\n",
    "\n",
    "\n",
    "# ===== Fisher Matrix Computation for EWC =====\n",
    "def compute_fisher_matrix(model, X, y, num_samples=100):\n",
    "    fisher = [tf.zeros_like(w) for w in model.trainable_variables]\n",
    "    \n",
    "    # Sample subset of data\n",
    "    idx = np.random.choice(len(X), min(num_samples, len(X)), replace=False)\n",
    "    X_sample = X[idx]\n",
    "    y_sample = y[idx]\n",
    "    \n",
    "    for x, true_label in zip(X_sample, y_sample):\n",
    "        with tf.GradientTape() as tape:\n",
    "            prob = model(np.expand_dims(x, axis=0))[0, true_label]\n",
    "            log_prob = tf.math.log(prob)\n",
    "        grads = tape.gradient(log_prob, model.trainable_variables)\n",
    "        fisher = [f + tf.square(g) for f, g in zip(fisher, grads)]\n",
    "    #print(\"fisher matrix:\",fisher)\n",
    "    return [f / num_samples for f in fisher]\n",
    "\n",
    "# ===== Helpers for flowering focus =====\n",
    "def is_flowering_seq(x_seq, light_idx=2, th_light=550.0):\n",
    "    light_mean = float(np.mean(x_seq[:, light_idx]))\n",
    "    return light_mean >= th_light\n",
    "\n",
    "def hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15):\n",
    "    hv = x_seq[:, hvac_slice]  # [T,4]\n",
    "    if hv.shape[0] < 2:\n",
    "        return 0.0, False\n",
    "    diff = np.abs(hv[1:] - hv[:-1])   # [T-1,4]\n",
    "    rate = float(diff.mean())\n",
    "    return rate, rate >= th_toggle\n",
    "\n",
    "def outer_update_with_lll(memory,meta_model, meta_optimizer, tasks,\n",
    "                          lr_inner=INNER_LR, replay_weight=REPLAY_WEIGHT,\n",
    "                          lambda_ewc=LAMBDA_EWC, prev_weights=None, fisher_matrix=None):\n",
    "    meta_grads = [tf.zeros_like(v) for v in meta_model.trainable_variables]\n",
    "    query_acc_list, query_loss_list = [], []\n",
    "\n",
    "    for X_support, y_support, X_query, y_query in tasks:\n",
    "        orig_vars = [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "        # inner update\n",
    "        updated_vars = inner_update(meta_model, X_support, y_support,lr_inner=lr_inner,)\n",
    "        for var, upd in zip(meta_model.trainable_variables, updated_vars):\n",
    "            var.assign(upd)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds_q = meta_model(X_query, training=True)\n",
    "            loss_q = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_query, preds_q))\n",
    "            loss_total = loss_q\n",
    "\n",
    "            # replay\n",
    "            if len(memory) >= 8:\n",
    "                X_old, y_old = memory.sample(batch_size=32)\n",
    "                preds_old = meta_model(X_old, training=True)\n",
    "                replay_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_old, preds_old))\n",
    "                loss_total = (1 - replay_weight) * loss_total + replay_weight * replay_loss\n",
    "            \n",
    "            # EWC (using Fisher matrix)\n",
    "            if prev_weights is not None and fisher_matrix is not None:\n",
    "                ewc_loss = 0.0\n",
    "\n",
    "                for w, pw, f in zip(meta_model.trainable_variables, prev_weights, fisher_matrix):\n",
    "                    ewc_loss += tf.reduce_sum(f * tf.square(w - pw))\n",
    "                loss_total += lambda_ewc * ewc_loss\n",
    "                #for i, f in enumerate(prev_weights):\n",
    "                #    print(f\"Fisher matrix for variable {i} has shape: {f.shape}\")\n",
    "\n",
    "        grads = tape.gradient(loss_total, meta_model.trainable_variables)\n",
    "\n",
    "        # ===== Flowering + HVAC toggling gradient boost =====\n",
    "        flowering_mask = []\n",
    "        toggle_scores = []\n",
    "        for i in range(len(X_query)):\n",
    "            x_seq = X_query[i]  # [T,D]\n",
    "            flw = is_flowering_seq(x_seq, light_idx=2, th_light=550.0)\n",
    "            tscore, tabove = hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15)\n",
    "            flowering_mask.append(bool(flw and tabove))\n",
    "            toggle_scores.append(tscore)\n",
    "\n",
    "        if any(flowering_mask):\n",
    "            ratio = sum(flowering_mask) / len(flowering_mask)\n",
    "            mean_toggle = np.mean([t for m,t in zip(flowering_mask, toggle_scores) if m]) if any(flowering_mask) else 0.0\n",
    "            toggle_boost = min(1.0 + float(mean_toggle)*2.0, FLOWERING_WEIGHT)\n",
    "            boost = 1.0 + (FLOWERING_WEIGHT - 1.0) * ratio\n",
    "            total_boost = float(min(boost * toggle_boost, FLOWERING_WEIGHT))\n",
    "            grads = [g * total_boost for g in grads]\n",
    "\n",
    "        meta_grads = [mg + g / len(tasks) for mg, g in zip(meta_grads, grads)]\n",
    "\n",
    "        q_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(preds_q, axis=1), y_query), tf.float32))\n",
    "        query_acc_list.append(float(q_acc.numpy()))\n",
    "        query_loss_list.append(float(loss_q.numpy()))\n",
    "\n",
    "        # restore original vars\n",
    "        for var, orig in zip(meta_model.trainable_variables, orig_vars):\n",
    "            var.assign(orig)\n",
    "\n",
    "        # update memory\n",
    "        memory.add(X_support, y_support)\n",
    "        memory.add(X_query, y_query)\n",
    "\n",
    "    meta_optimizer.apply_gradients(zip(meta_grads, meta_model.trainable_variables))\n",
    "    return float(np.mean(query_loss_list)), float(np.mean(query_acc_list)), [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "# =============================\n",
    "# 5) Save/Load Fisher Matrix and Model Weights\n",
    "# =============================\n",
    " \n",
    "\n",
    "def save_fisher_and_weights(model, fisher_matrix, save_dir=\"ewc_assets\"):\n",
    "    \"\"\"\n",
    "    model: å·²ç¶“ trainable çš„ meta_model\n",
    "    fisher_matrix: å°æ‡‰ model.trainable_variables çš„ Fisher matrix (list of tf.Tensor)\n",
    "    \"\"\"\n",
    "    trainable_vars = model.trainable_variables\n",
    "    weights = [v.numpy() for v in trainable_vars]\n",
    "    fisher = [f.numpy() for f in fisher_matrix]\n",
    "    # å‡è¨­ weights å·²ç¶“å–å¾—\n",
    "    #weights = [w.numpy() for w in model.trainable_weights]\n",
    "    \n",
    "    # ç”Ÿæˆ layer_shapes\n",
    "    layer_shapes = [list(w.shape) for w in weights]\n",
    "    \n",
    "    # å°‡ layer_shapes è½‰æˆ JSON bytes\n",
    "    layer_shapes_json = json.dumps(layer_shapes)\n",
    "    # å¯«å…¥æª”æ¡ˆ [[8, 16], [16], [80, 64], [64], [64, 32], [32]]\n",
    "    with open(os.path.join(save_dir,\"layer_shapes.json\") , \"w\") as f:\n",
    "        f.write(layer_shapes_json)\n",
    "    \n",
    "    print(\"layer_shapes.json saved!\")\n",
    "    layer_shapes_bytes = layer_shapes_json.encode('utf-8')\n",
    "    # å„²å­˜æˆ .npz\n",
    "    np.savez(os.path.join(save_dir,\"ewc_assets.npz\"), *weights, *fisher)\n",
    "    #ewc_buffer = np.concatenate([trainable_weights_flattened..., fisher_matrices_flattened...])\n",
    "\n",
    "    print(f\"âœ… Trainable weights + Fisher matrix saved to {save_dir}\")\n",
    "    print(f\"  Total arrays saved: {len(weights) + len(fisher)}\")\n",
    "\n",
    "# ç¯„ä¾‹\n",
    "# fisher_matrix = compute_fisher_matrix(meta_model, X_labeled, y_labeled)\n",
    "# save_fisher_and_weights(meta_model, fisher_matrix, \"ewc_assets.npz\")\n",
    "\n",
    "\n",
    "def save_ewc_assets(model, fisher_matrix, save_dir=\"ewc_assets\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    model.save_weights(os.path.join(save_dir, \"model_weights.h5\"))\n",
    "    \n",
    "    # Save Fisher matrix\n",
    "    fisher_numpy = [f.numpy() for f in fisher_matrix]\n",
    "    np.savez(os.path.join(save_dir, \"fisher_matrix.npz\"), *fisher_numpy)\n",
    "    \n",
    "    print(f\"EWC assets saved to {save_dir}\")\n",
    "\n",
    "def load_ewc_assets(model, save_dir=\"ewc_assets\"):\n",
    "    # Load model weights\n",
    "    model.load_weights(os.path.join(save_dir, \"model_weights.h5\"))\n",
    "    \n",
    "    # Load Fisher matrix\n",
    "    fisher_data = np.load(os.path.join(save_dir, \"fisher_matrix.npz\"))\n",
    "    fisher_matrix = [tf.constant(arr) for arr in fisher_data.values()]\n",
    "    \n",
    "    print(f\"EWC assets loaded from {save_dir}\")\n",
    "    return model,fisher_matrix\n",
    "\n",
    "# Example of loading (commented out since we just saved)\n",
    "# loaded_fisher = load_ewc_assets(meta_model)\n",
    "\n",
    "# =============================\n",
    "# 6) TFLite export (BUILTINS only)\n",
    "# =============================\n",
    "def save_tflite(model, out_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved TFLite:\", out_path)\n",
    "    \n",
    "def set_trainable_layers(encoder, meta_model, encoder_mode=\"finetune\", last_n=1):\n",
    "    \"\"\"\n",
    "    è¨­å®š encoder èˆ‡ meta_model å±¤çš„ trainable ç‹€æ…‹\n",
    "\n",
    "    encoder_mode: \"finetune\" / \"freeze\" / \"last_n\"\n",
    "    last_n: åªæœ‰åœ¨ encoder_mode=\"last_n\" æ‰æœ‰æ„ç¾©\n",
    "    \"\"\"\n",
    "    # ===== Encoder =====\n",
    "    if encoder_mode == \"finetune\":\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = True\n",
    "    elif encoder_mode == \"freeze\":\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = False\n",
    "    elif encoder_mode == \"last_n\":\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = False\n",
    "        if last_n is not None:\n",
    "            for layer in encoder.layers[-last_n:]:\n",
    "                layer.trainable = True\n",
    "\n",
    "    # ===== Meta Model =====\n",
    "    # å‡è¨­ meta_model çš„ Dense å±¤éƒ½å¯ä»¥å–®ç¨ trainable\n",
    "    for layer in meta_model.layers:\n",
    "        # å¦‚æœæ˜¯ encoder çš„å­æ¨¡å‹ï¼Œä¸æ”¹è®Šï¼ˆç”± encoder ç®¡ç†ï¼‰\n",
    "        #if layer.name.startswith(\"lstm_encoder\") or layer.name.startswith(\"encoder_\"):\n",
    "        #    continue\n",
    "        # å…¶ä»–å±¤å…¨éƒ¨å¯è¨“ç·´\n",
    "        if layer.name.startswith(\"meta_dense\") or layer.name.startswith(\"hvac_dense\"):\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    print(f\"âœ… Encoder mode: {encoder_mode}, last_n={last_n if encoder_mode=='last_n' else 'N/A'}\")\n",
    "\n",
    "    # åˆ—å°å¯¦éš› trainable å±¤ï¼ˆåªåˆ—å‡ºæœ‰æ¬Šé‡çš„ï¼‰\n",
    "    print(\"\\nğŸ” [Encoder trainable layers]\")\n",
    "    for layer in encoder.layers:\n",
    "        if layer.trainable_weights:\n",
    "            print(f\"{layer.name:<20} {'âœ… trainable' if layer.trainable else 'âŒ frozen'}\")\n",
    "\n",
    "    print(\"\\nğŸ” [Meta model trainable layers]\")\n",
    "    for layer in meta_model.layers:\n",
    "        if layer.trainable_weights:\n",
    "            print(f\"{layer.name:<20} {'âœ… trainable' if layer.trainable else 'âŒ frozen'}\")\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"trainable_layers_{timestamp}.txt\"\n",
    "    \n",
    "    log_lines = []\n",
    "    log_lines.append(f\"âš™ï¸ Encoder mode: {ENCODER_MODE}, last_n={LAST_N if ENCODER_MODE=='last_n' else 'N/A'}\\n\")\n",
    "    \n",
    "      \n",
    "    for i, layer in enumerate(meta_model.layers):\n",
    "        # åªé¡¯ç¤ºæœ‰ trainable æ¬Šé‡çš„å±¤\n",
    "        if layer.trainable_weights:\n",
    "            status = \"âœ… trainable\" if layer.trainable else \"âŒ frozen\"\n",
    "            line = f\"Layer {i:02d}: {layer.name:<20} {status}\"\n",
    "            print(line)\n",
    "            log_lines.append(line)\n",
    "\n",
    "    #with open(log_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    #    f.write(\"\\n\".join(log_lines))\n",
    "    \n",
    "    #print(f\"\\nğŸ“ trainable å±¤æ¸…å–®å·²å„²å­˜åˆ° {log_filename}\\n\")  \n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # æ›´æ–°å…¨å±€å˜é‡\n",
    "    global  ENCODER_MODE , LAST_N \n",
    "    ENCODER_MODE = args.encoder_mode \n",
    "    LAST_N = args.last_n \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # ======= Train meta-learning ======= \n",
    "        # Build models\n",
    "        X_unlabeled,X_labeled, y_labeled=build_csv_data(data_glob=DATA_GLOB)\n",
    "        lstm_encoder = build_lstm_encoder(SEQ_LEN, NUM_FEATS, FEATURE_DIM)\n",
    "        contrastive_opt = tf.keras.optimizers.Adam()\n",
    "        ntxent = NTXentLoss(temperature=0.2)\n",
    "        \n",
    "        \n",
    "        anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "        contrast_ds = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(2048).batch(BATCH_SIZE)\n",
    "        \n",
    "        # Train contrastive\n",
    "        contrastive_loss_history = []\n",
    "        for ep in range(EPOCHS_CONTRASTIVE):\n",
    "            for a, p in contrast_ds:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    za = lstm_encoder(a, training=True)\n",
    "                    zp = lstm_encoder(p, training=True)\n",
    "                    loss = ntxent(za, zp)\n",
    "                grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "                contrastive_opt.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "            contrastive_loss_history.append(float(loss.numpy()))\n",
    "            print(f\"[Contrastive] Epoch {ep+1}/{EPOCHS_CONTRASTIVE}, loss={float(loss.numpy()):.4f}\")\n",
    "        \n",
    "        # Meta model\n",
    "        meta_model = build_meta_model(lstm_encoder, NUM_CLASSES)\n",
    "        meta_optimizer = tf.keras.optimizers.Adam(META_LR)\n",
    "        set_trainable_layers(lstm_encoder, meta_model, ENCODER_MODE, LAST_N)\n",
    "\n",
    "        memory = ReplayBuffer(capacity=REPLAY_CAPACITY)\n",
    "        meta_loss_history, meta_acc_history = [], []\n",
    "        prev_weights = None\n",
    "        fisher_matrix = None\n",
    "        \n",
    "        if X_labeled.size > 0:\n",
    "            # Compute Fisher matrix on initial model\n",
    "            fisher_matrix = compute_fisher_matrix(meta_model, X_labeled, y_labeled)\n",
    "            \n",
    "            for ep in range(EPOCHS_META):\n",
    "                tasks = sample_tasks(X_labeled, y_labeled)\n",
    "                loss, acc, prev_weights = outer_update_with_lll(memory=memory,\n",
    "                    meta_model=meta_model, meta_optimizer=meta_optimizer, tasks=tasks, \n",
    "                    lr_inner=INNER_LR,prev_weights=prev_weights, fisher_matrix=fisher_matrix\n",
    "                )\n",
    "                meta_loss_history.append(loss)\n",
    "                meta_acc_history.append(acc)\n",
    "                print(f\"[Meta] Epoch {ep+1}/{EPOCHS_META}, loss={loss:.4f}, acc={acc:.4f}\")\n",
    "        else:\n",
    "            print(\"Skip meta-learning: no labeled data.\")\n",
    "        \n",
    "        # Save assets if we have them\n",
    "        if fisher_matrix is not None:\n",
    "            save_fisher_and_weights(model=meta_model, fisher_matrix=fisher_matrix)\n",
    "            #save_ewc_assets(meta_model, fisher_matrix)\n",
    "\n",
    "\n",
    "        # Save models\n",
    "        save_tflite(lstm_encoder, \"lstm_encoder_contrastive.tflite\")\n",
    "        if X_labeled.size > 0:\n",
    "            save_tflite(meta_model, \"meta_lstm_classifier.tflite\")\n",
    "            make_indices(model_path=\"meta_lstm_classifier.tflite\")\n",
    "            #model = tf.keras.models.load_model(\"your_keras_model.h5\")\n",
    "            #generate_trainable_tensor_indices(meta_model, \"meta_lstm_classifier.tflite\")\n",
    "\n",
    "            print(\"meta_lstm_classifier.tflite Done.\")\n",
    "        \n",
    "        # ============ æ¸¬è©¦è·‘ä¸€æ¬¡å‰å‘ ============\n",
    "         \n",
    "        dummy_x = np.random.rand(1, 10, 7).astype(np.float32)\n",
    "        dummy_y = lstm_encoder(dummy_x)\n",
    "        print(\"âœ… æ¸¬è©¦å‰å‘è¼¸å‡º shape:\", dummy_y.shape)\n",
    "\n",
    "\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Skip meta-learning: no labeled data.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser() \n",
    "    parser.add_argument(\"--encoder_mode\", type=str, default=ENCODER_MODE) \n",
    "    parser.add_argument(\"--last_n\", type=int, default=LAST_N) \n",
    "    #args = parser.parse_args()\n",
    "    # ============ è§£æåƒæ•¸ ============\n",
    "    args, unknown = parser.parse_known_args()\n",
    "  \n",
    "    \n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae8a4be7-c98b-4b9c-ac14-097228760806",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
