{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bcfb4a0-caa2-466a-95a7-eb20f95481e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded EWC assets:\n",
      "- Fisher matrix shape: [TensorShape([3, 256]), TensorShape([64, 256]), TensorShape([256]), TensorShape([64, 64]), TensorShape([64]), TensorShape([8, 16]), TensorShape([16]), TensorShape([80, 64]), TensorShape([64]), TensorShape([64, 32]), TensorShape([32]), TensorShape([32, 3]), TensorShape([3])]\n",
      "- Model weights shape: [TensorShape([3, 256]), TensorShape([64, 256]), TensorShape([256]), TensorShape([64, 64]), TensorShape([64]), TensorShape([8, 16]), TensorShape([16]), TensorShape([80, 64]), TensorShape([64]), TensorShape([64, 32]), TensorShape([32]), TensorShape([32, 3]), TensorShape([3])]\n",
      "EWC initialization complete\n",
      "Generating dummy data for demonstration...\n",
      "\n",
      "Starting training with EWC...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 269\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fisher_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m prev_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training with EWC...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 269\u001b[0m     \u001b[43mtrain_with_ewc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfisher_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCould not start training - EWC assets not loaded properly\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 243\u001b[0m, in \u001b[0;36mtrain_with_ewc\u001b[0;34m(meta_model, X_train, y_train, fisher_matrix, prev_weights)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[1;32m    235\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(task_loss\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    236\u001b[0m epoch_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m() \n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Restore original vars\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, orig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(meta_model\u001b[38;5;241m.\u001b[39mtrainable_variables, orig_vars):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Meta-learning pipeline with EWC using pre-computed Fisher matrix\n",
    "- Loads Fisher matrix from fisher_matrix.npz\n",
    "- Integrates EWC for continual learning\n",
    "- Includes HVAC-aware features and flowering-period focus\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# =============================\n",
    "# Hyperparameters\n",
    "# =============================\n",
    "SEQ_LEN = 64\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_META = 20\n",
    "INNER_LR = 1e-2\n",
    "META_LR = 1e-3\n",
    "NUM_CLASSES = 3\n",
    "NUM_TASKS = 5\n",
    "SUPPORT_SIZE = 10\n",
    "QUERY_SIZE = 20\n",
    "REPLAY_CAPACITY = 1000\n",
    "REPLAY_WEIGHT = 0.3\n",
    "LAMBDA_EWC = 1e-3\n",
    "FLOWERING_WEIGHT = 2.0\n",
    "FISHER_MATRIX_PATH = \"./ewc_assets/fisher_matrix.npz\"\n",
    "MODEL_WEIGHTS_PATH = \"./ewc_assets/model_weights.h5\"\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# =============================\n",
    "# 1) Model Architecture\n",
    "# =============================\n",
    "def build_lstm_encoder(seq_len, num_feats, feature_dim=FEATURE_DIM):\n",
    "    inp = layers.Input(shape=(seq_len, num_feats))\n",
    "    x_cont = layers.Lambda(lambda z: z[:, :, :3])(inp)\n",
    "    x = layers.LSTM(feature_dim, unroll=True)(x_cont)\n",
    "    out = layers.Dense(feature_dim, activation=\"relu\")(x)\n",
    "    return models.Model(inp, out, name=\"lstm_encoder\")\n",
    "\n",
    "def build_meta_model(encoder, num_classes=NUM_CLASSES):\n",
    "    inp = layers.Input(shape=(SEQ_LEN, encoder.input_shape[2]))\n",
    "    z_enc = encoder(inp)\n",
    "\n",
    "    # HVAC features\n",
    "    hvac = layers.Lambda(lambda z: z[:, :, 3:7])(inp)\n",
    "    hvac_mean = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1))(hvac)\n",
    "    hvac_shift = layers.Lambda(lambda z: z[:, 1:, :])(hvac)\n",
    "    hvac_prev = layers.Lambda(lambda z: z[:, :-1, :])(hvac)\n",
    "    hvac_diff = layers.Lambda(lambda t: tf.abs(t[0] - t[1]))([hvac_shift, hvac_prev])\n",
    "    hvac_toggle_rate = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1))(hvac_diff)\n",
    "\n",
    "    hvac_feat = layers.Concatenate()([hvac_mean, hvac_toggle_rate])\n",
    "    hvac_feat = layers.Dense(16, activation=\"relu\")(hvac_feat)\n",
    "\n",
    "    x = layers.Concatenate()([z_enc, hvac_feat])\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"meta_lstm_classifier\")\n",
    "\n",
    "# =============================\n",
    "# 2) Load Fisher Matrix and Model Weights\n",
    "# =============================\n",
    "def load_ewc_assets(model, fisher_path=FISHER_MATRIX_PATH, weights_path=MODEL_WEIGHTS_PATH):\n",
    "    \"\"\"Load pre-computed Fisher matrix and model weights\"\"\"\n",
    "    if not os.path.exists(fisher_path):\n",
    "        raise FileNotFoundError(f\"Fisher matrix file not found: {fisher_path}\")\n",
    "    if not os.path.exists(weights_path):\n",
    "        raise FileNotFoundError(f\"Model weights file not found: {weights_path}\")\n",
    "    \n",
    "    # Load Fisher matrix\n",
    "    fisher_data = np.load(fisher_path)\n",
    "    fisher_matrix = [tf.constant(arr) for arr in fisher_data.values()]\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_weights(weights_path)\n",
    "    prev_weights = [tf.identity(w) for w in model.trainable_variables]\n",
    "    \n",
    "    print(\"Successfully loaded EWC assets:\")\n",
    "    print(f\"- Fisher matrix shape: {[f.shape for f in fisher_matrix]}\")\n",
    "    print(f\"- Model weights shape: {[w.shape for w in prev_weights]}\")\n",
    "    \n",
    "    return fisher_matrix, prev_weights\n",
    "\n",
    "# Initialize models\n",
    "lstm_encoder = build_lstm_encoder(SEQ_LEN, 7)  # Assuming 7 input features\n",
    "meta_model = build_meta_model(lstm_encoder)\n",
    "\n",
    "# Load EWC assets\n",
    "try:\n",
    "    fisher_matrix, prev_weights = load_ewc_assets(meta_model)\n",
    "    print(\"EWC initialization complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading EWC assets: {e}\")\n",
    "    fisher_matrix, prev_weights = None, None\n",
    "\n",
    "# =============================\n",
    "# 3) Meta-Learning with EWC\n",
    "# =============================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=REPLAY_CAPACITY):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.n_seen = 0\n",
    "    \n",
    "    def add(self, X, y):\n",
    "        for xi, yi in zip(X, y):\n",
    "            self.n_seen += 1\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append((xi, yi))\n",
    "            else:\n",
    "                r = np.random.randint(0, self.n_seen)\n",
    "                if r < self.capacity:\n",
    "                    self.buffer[r] = (xi, yi)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        X_s, y_s = zip(*[self.buffer[i] for i in idxs])\n",
    "        return np.array(X_s), np.array(y_s)\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "def sample_tasks(X, y, num_tasks=NUM_TASKS, support_size=SUPPORT_SIZE, query_size=QUERY_SIZE):\n",
    "    tasks = []\n",
    "    n = len(X)\n",
    "    if n < support_size + query_size:\n",
    "        raise ValueError(f\"Not enough samples: need {support_size+query_size}, got {n}\")\n",
    "    for _ in range(num_tasks):\n",
    "        idx = np.random.choice(n, support_size + query_size, replace=False)\n",
    "        X_support, y_support = X[idx[:support_size]], y[idx[:support_size]]\n",
    "        X_query, y_query = X[idx[support_size:]], y[idx[support_size:]]\n",
    "        tasks.append((X_support, y_support, X_query, y_query))\n",
    "    return tasks\n",
    "\n",
    "def inner_update(model, X_support, y_support, lr_inner=INNER_LR):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds_support = model(X_support, training=True)\n",
    "        loss_support = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(y_support, preds_support)\n",
    "        )\n",
    "    grads_inner = tape.gradient(loss_support, model.trainable_variables)\n",
    "    updated_vars = [w - lr_inner * g for w, g in zip(model.trainable_variables, grads_inner)]\n",
    "    return updated_vars\n",
    "\n",
    "def is_flowering_seq(x_seq, light_idx=2, th_light=550.0):\n",
    "    light_mean = float(np.mean(x_seq[:, light_idx]))\n",
    "    return light_mean >= th_light\n",
    "\n",
    "def hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15):\n",
    "    hv = x_seq[:, hvac_slice]\n",
    "    if hv.shape[0] < 2:\n",
    "        return 0.0, False\n",
    "    diff = np.abs(hv[1:] - hv[:-1])\n",
    "    rate = float(diff.mean())\n",
    "    return rate, rate >= th_toggle\n",
    "\n",
    "def train_with_ewc(meta_model, X_train, y_train, fisher_matrix, prev_weights):\n",
    "    meta_optimizer = tf.keras.optimizers.Adam(META_LR)\n",
    "    \n",
    "    for epoch in range(EPOCHS_META):\n",
    "        tasks = sample_tasks(X_train, y_train)\n",
    "        epoch_loss, epoch_acc = 0, 0\n",
    "        \n",
    "        for X_support, y_support, X_query, y_query in tasks:\n",
    "            orig_vars = [tf.identity(w) for w in meta_model.trainable_variables]\n",
    "            \n",
    "            # Inner update\n",
    "            updated_vars = inner_update(meta_model, X_support, y_support)\n",
    "            for var, upd in zip(meta_model.trainable_variables, updated_vars):\n",
    "                var.assign(upd)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Task loss\n",
    "                preds_q = meta_model(X_query, training=True)\n",
    "                task_loss = tf.reduce_mean(\n",
    "                    tf.keras.losses.sparse_categorical_crossentropy(y_query, preds_q)\n",
    "                )\n",
    "                total_loss = task_loss\n",
    "                \n",
    "                # Replay loss\n",
    "                if memory.size() >= 8:\n",
    "                #if len(memory) >= 8:\n",
    "                    X_old, y_old = memory.sample(32)\n",
    "                    preds_old = meta_model(X_old, training=True)\n",
    "                    replay_loss = tf.reduce_mean(\n",
    "                        tf.keras.losses.sparse_categorical_crossentropy(y_old, preds_old)\n",
    "                    )\n",
    "                    total_loss = (1 - REPLAY_WEIGHT) * total_loss + REPLAY_WEIGHT * replay_loss\n",
    "                \n",
    "                # EWC loss\n",
    "                if fisher_matrix is not None and prev_weights is not None:\n",
    "                    ewc_loss = 0.0\n",
    "                    for w, pw, f in zip(meta_model.trainable_variables, prev_weights, fisher_matrix):\n",
    "                        ewc_loss += tf.reduce_sum(f * tf.square(w - pw))\n",
    "                    total_loss += LAMBDA_EWC * ewc_loss\n",
    "                \n",
    "                # Flowering focus\n",
    "                flowering_mask = []\n",
    "                toggle_scores = []\n",
    "                for i in range(len(X_query)):\n",
    "                    x_seq = X_query[i]\n",
    "                    flw = is_flowering_seq(x_seq)\n",
    "                    tscore, tabove = hvac_toggle_score(x_seq)\n",
    "                    flowering_mask.append(bool(flw and tabove))\n",
    "                    toggle_scores.append(tscore)\n",
    "                \n",
    "                if any(flowering_mask):\n",
    "                    ratio = sum(flowering_mask) / len(flowering_mask)\n",
    "                    mean_toggle = np.mean([t for m,t in zip(flowering_mask, toggle_scores) if m]) if any(flowering_mask) else 0.0\n",
    "                    toggle_boost = min(1.0 + float(mean_toggle)*2.0, FLOWERING_WEIGHT)\n",
    "                    boost = 1.0 + (FLOWERING_WEIGHT - 1.0) * ratio\n",
    "                    total_boost = float(min(boost * toggle_boost, FLOWERING_WEIGHT))\n",
    "            \n",
    "            grads = tape.gradient(total_loss, meta_model.trainable_variables)\n",
    "            \n",
    "            if any(flowering_mask):\n",
    "                grads = [g * total_boost for g in grads]\n",
    "            \n",
    "            meta_optimizer.apply_gradients(zip(grads, meta_model.trainable_variables))\n",
    "            \n",
    "            # Metrics\n",
    "            epoch_loss += float(task_loss.numpy())\n",
    "            epoch_acc += float(\n",
    "                tf.reduce_mean(\n",
    "                    tf.cast(\n",
    "                        tf.equal(tf.argmax(preds_q, axis=1), y_query),\n",
    "                        tf.float32\n",
    "                    )\n",
    "                )\n",
    "            ) \n",
    "            \n",
    "            # Restore original vars\n",
    "            for var, orig in zip(meta_model.trainable_variables, orig_vars):\n",
    "                var.assign(orig)\n",
    "            \n",
    "            # Update memory\n",
    "            memory.add(X_support, y_support)\n",
    "            memory.add(X_query, y_query)\n",
    "        \n",
    "        avg_loss = epoch_loss / len(tasks)\n",
    "        avg_acc = epoch_acc / len(tasks)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS_META} - Loss: {avg_loss:.4f} - Acc: {avg_acc:.4f}\")\n",
    "\n",
    "# =============================\n",
    "# 4) Example Usage\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate dummy data if no real data available\n",
    "    # In practice, replace with your actual data loading code\n",
    "    print(\"Generating dummy data for demonstration...\")\n",
    "    X_dummy = np.random.randn(100, SEQ_LEN, 7).astype(np.float32)\n",
    "    y_dummy = np.random.randint(0, NUM_CLASSES, size=100).astype(np.int32)\n",
    "    \n",
    "    if fisher_matrix is not None and prev_weights is not None:\n",
    "        print(\"\\nStarting training with EWC...\")\n",
    "        train_with_ewc(meta_model, X_dummy, y_dummy, fisher_matrix, prev_weights)\n",
    "    else:\n",
    "        print(\"\\nCould not start training - EWC assets not loaded properly\")\n",
    "    \n",
    "    # Save updated model\n",
    "    print(\"\\nSaving updated model...\")\n",
    "    meta_model.save_weights(\"updated_model_weights.h5\")\n",
    "    print(\"Training complete and model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7bce7-0412-48bd-849e-7da5e40637cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv2)",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
