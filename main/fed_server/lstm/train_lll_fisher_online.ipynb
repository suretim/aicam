{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef1b3c1-f8ae-43bb-8ab6-49bc2e4a33e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 11:50:09.310206: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-20 11:50:09.341509: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-20 11:50:09.625751: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-20 11:50:09.627533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-20 11:50:10.989661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contrastive] Epoch 1/10, loss=0.9898\n",
      "[Contrastive] Epoch 2/10, loss=0.5418\n",
      "[Contrastive] Epoch 3/10, loss=0.5915\n",
      "[Contrastive] Epoch 4/10, loss=0.2044\n",
      "[Contrastive] Epoch 5/10, loss=0.2235\n",
      "[Contrastive] Epoch 6/10, loss=0.7443\n",
      "[Contrastive] Epoch 7/10, loss=0.1826\n",
      "[Contrastive] Epoch 8/10, loss=0.2345\n",
      "[Contrastive] Epoch 9/10, loss=0.1065\n",
      "[Contrastive] Epoch 10/10, loss=0.8159\n",
      "[Meta] Epoch 1/20, loss=1.1271, acc=0.1500\n",
      "[Meta] Epoch 2/20, loss=1.0721, acc=0.5800\n",
      "[Meta] Epoch 3/20, loss=1.0344, acc=0.9500\n",
      "[Meta] Epoch 4/20, loss=0.9804, acc=0.9500\n",
      "[Meta] Epoch 5/20, loss=0.9547, acc=0.9100\n",
      "[Meta] Epoch 6/20, loss=0.9108, acc=0.9500\n",
      "[Meta] Epoch 7/20, loss=0.8705, acc=0.9700\n",
      "[Meta] Epoch 8/20, loss=0.8298, acc=0.9600\n",
      "[Meta] Epoch 9/20, loss=0.8004, acc=0.9600\n",
      "[Meta] Epoch 10/20, loss=0.7624, acc=0.9600\n",
      "[Meta] Epoch 11/20, loss=0.7191, acc=0.9600\n",
      "[Meta] Epoch 12/20, loss=0.6933, acc=0.9400\n",
      "[Meta] Epoch 13/20, loss=0.6309, acc=0.9400\n",
      "[Meta] Epoch 14/20, loss=0.5872, acc=0.9200\n",
      "[Meta] Epoch 15/20, loss=0.4917, acc=0.9600\n",
      "[Meta] Epoch 16/20, loss=0.4094, acc=0.9800\n",
      "[Meta] Epoch 17/20, loss=0.3607, acc=0.9800\n",
      "[Meta] Epoch 18/20, loss=0.3454, acc=0.9700\n",
      "[Meta] Epoch 19/20, loss=0.3049, acc=0.9700\n",
      "[Meta] Epoch 20/20, loss=0.2531, acc=0.9800\n",
      "EWC assets saved to ewc_assets\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp6zh0_imc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6zh0_imc/assets\n",
      "2025-08-20 11:50:31.197100: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-20 11:50:31.197171: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-20 11:50:31.198087: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp6zh0_imc\n",
      "2025-08-20 11:50:31.200297: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-20 11:50:31.200329: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp6zh0_imc\n",
      "2025-08-20 11:50:31.207815: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-08-20 11:50:31.209082: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-20 11:50:31.237172: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp6zh0_imc\n",
      "2025-08-20 11:50:31.252581: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 54496 microseconds.\n",
      "2025-08-20 11:50:31.310168: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite: lstm_encoder_contrastive.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpuvpc9l03/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuvpc9l03/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite: meta_lstm_classifier.tflite\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 11:50:33.816061: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-20 11:50:33.816123: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-20 11:50:33.816338: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpuvpc9l03\n",
      "2025-08-20 11:50:33.819921: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-20 11:50:33.819953: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpuvpc9l03\n",
      "2025-08-20 11:50:33.829369: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-20 11:50:33.862749: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpuvpc9l03\n",
      "2025-08-20 11:50:33.881361: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 65022 microseconds.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Meta-learning pipeline with HVAC-aware features and flowering-period focus.\n",
    "- Expects CSV columns: temp, humid, light, ac, heater, dehum, hum, label\n",
    "- Sliding windows -> contrastive learning (unlabeled) + FOMAML with LLL + EWC (labeled)\n",
    "- Encoder: LSTM on continuous features only (temp/humid/light)\n",
    "- Additional HVAC features: mean on/off rate + toggle rate (abs(diff)) over time\n",
    "- Gradient boost on flowering period with abnormal HVAC toggling\n",
    "- TFLite export restricted to TFLITE_BUILTINS\n",
    "- Includes Fisher matrix computation and loading for EWC\n",
    "\"\"\"\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "\n",
    "# =============================\n",
    "# Hyperparameters\n",
    "# =============================\n",
    "DATA_GLOB = \"../../../../lll_data/*.csv\"\n",
    "SEQ_LEN = 10\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_CONTRASTIVE = 10\n",
    "EPOCHS_META = 20\n",
    "INNER_LR = 1e-2\n",
    "META_LR = 1e-3\n",
    "NUM_CLASSES = 3\n",
    "NUM_TASKS = 5\n",
    "SUPPORT_SIZE = 10\n",
    "QUERY_SIZE = 20\n",
    "REPLAY_CAPACITY = 1000\n",
    "REPLAY_WEIGHT = 0.3\n",
    "LAMBDA_EWC = 1e-3\n",
    "FLOWERING_WEIGHT = 2.0  # gradient boost upper bound for flowering-focus\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# =============================\n",
    "# 1) Load CSV -> Sliding windows\n",
    "# =============================\n",
    "X_labeled_list, y_labeled_list = [], []\n",
    "X_unlabeled_list = []\n",
    "\n",
    "for file in sorted(glob.glob(DATA_GLOB)):\n",
    "    df = pd.read_csv(file).fillna(-1)\n",
    "    data = df.values.astype(np.float32)\n",
    "    feats, labels = data[:, :-1], data[:, -1]\n",
    "    for i in range(len(data) - SEQ_LEN + 1):\n",
    "        w_x = feats[i:i + SEQ_LEN]\n",
    "        w_y = labels[i + SEQ_LEN - 1]\n",
    "        if w_y == -1:\n",
    "            X_unlabeled_list.append(w_x)\n",
    "        else:\n",
    "            X_labeled_list.append(w_x)\n",
    "            y_labeled_list.append(int(w_y))\n",
    "\n",
    "X_unlabeled = np.array(X_unlabeled_list, dtype=np.float32) if len(X_unlabeled_list) > 0 else np.empty((0,), dtype=np.float32)\n",
    "if len(X_labeled_list) > 0:\n",
    "    X_labeled = np.array(X_labeled_list, dtype=np.float32)\n",
    "    y_labeled = np.array(y_labeled_list, dtype=np.int32)\n",
    "else:\n",
    "    X_labeled = np.empty((0, SEQ_LEN, X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7), dtype=np.float32)\n",
    "    y_labeled = np.empty((0,), dtype=np.int32)\n",
    "\n",
    "NUM_FEATS = X_labeled.shape[2] if X_labeled.size > 0 else (X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7)\n",
    "\n",
    "if NUM_FEATS < 7:\n",
    "    raise ValueError(\"Expected at least 7 features per timestep: [temp, humid, light, ac, heater, dehum, hum]. Found: %d\" % NUM_FEATS)\n",
    "\n",
    "# Index conventions\n",
    "CONT_IDX = [0, 1, 2]   # temp, humid, light\n",
    "HVAC_IDX = [3, 4, 5, 6]  # ac, heater, dehum, hum\n",
    "\n",
    "# =============================\n",
    "# 2) Contrastive learning\n",
    "# =============================\n",
    "def augment_window(x):\n",
    "    \"\"\"Only perturb continuous channels to keep binary HVAC channels intact.\"\"\"\n",
    "    x_aug = x.copy()\n",
    "    x_aug[:, CONT_IDX] = x[:, CONT_IDX] + np.random.normal(0, 0.01, x[:, CONT_IDX].shape).astype(np.float32)\n",
    "    return x_aug\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for w in X:\n",
    "        anchors.append(w)\n",
    "        positives.append(augment_window(w))\n",
    "    return np.stack(anchors).astype(np.float32), np.stack(positives).astype(np.float32)\n",
    "\n",
    "class NTXentLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.2):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    def call(self, z_i, z_j):\n",
    "        z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "        z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "        logits = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_i)[0])\n",
    "        loss_i = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_j = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        return tf.reduce_mean(loss_i + loss_j)\n",
    "\n",
    "# =============================\n",
    "# 3) LSTM Encoder (unroll=True, continuous only)\n",
    "# =============================\n",
    "def build_lstm_encoder(seq_len, num_feats, feature_dim=FEATURE_DIM):\n",
    "    inp = layers.Input(shape=(seq_len, num_feats))\n",
    "    x_cont = layers.Lambda(lambda z: z[:, :, :3])(inp)  # [B,T,3]\n",
    "    x = layers.LSTM(feature_dim, unroll=True)(x_cont)\n",
    "    out = layers.Dense(feature_dim, activation=\"relu\")(x)\n",
    "    return models.Model(inp, out, name=\"lstm_encoder\")\n",
    "\n",
    "# =============================\n",
    "# 4) Meta Model (Encoder + HVAC features)\n",
    "# =============================\n",
    "def build_meta_model(encoder, num_classes=NUM_CLASSES):\n",
    "    inp = layers.Input(shape=(SEQ_LEN, NUM_FEATS))\n",
    "    z_enc = encoder(inp)  # [B, FEATURE_DIM]\n",
    "\n",
    "    # HVAC slice\n",
    "    hvac = layers.Lambda(lambda z: z[:, :, 3:7])(inp)   # [B,T,4]\n",
    "    hvac_mean = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1))(hvac)  # [B,4]\n",
    "\n",
    "    # Toggle rate via abs(diff)\n",
    "    hvac_shift = layers.Lambda(lambda z: z[:, 1:, :])(hvac)      # [B,T-1,4]\n",
    "    hvac_prev  = layers.Lambda(lambda z: z[:, :-1, :])(hvac)     # [B,T-1,4]\n",
    "    hvac_diff  = layers.Lambda(lambda t: tf.abs(t[0] - t[1]))([hvac_shift, hvac_prev])  # [B,T-1,4]\n",
    "    hvac_toggle_rate = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1))(hvac_diff)    # [B,4]\n",
    "\n",
    "    hvac_feat = layers.Concatenate()([hvac_mean, hvac_toggle_rate])  # [B,8]\n",
    "    hvac_feat = layers.Dense(16, activation=\"relu\")(hvac_feat)\n",
    "\n",
    "    x = layers.Concatenate()([z_enc, hvac_feat])\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"meta_lstm_classifier\")\n",
    "\n",
    "# Build models\n",
    "lstm_encoder = build_lstm_encoder(SEQ_LEN, NUM_FEATS, FEATURE_DIM)\n",
    "contrastive_opt = tf.keras.optimizers.Adam()\n",
    "ntxent = NTXentLoss(temperature=0.2)\n",
    "\n",
    "# Provide unlabeled fallback if none\n",
    "if X_unlabeled.size == 0:\n",
    "    X_unlabeled = np.random.randn(200, SEQ_LEN, NUM_FEATS).astype(np.float32)\n",
    "\n",
    "anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "contrast_ds = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(2048).batch(BATCH_SIZE)\n",
    "\n",
    "# Train contrastive\n",
    "contrastive_loss_history = []\n",
    "for ep in range(EPOCHS_CONTRASTIVE):\n",
    "    for a, p in contrast_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            za = lstm_encoder(a, training=True)\n",
    "            zp = lstm_encoder(p, training=True)\n",
    "            loss = ntxent(za, zp)\n",
    "        grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "        contrastive_opt.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "    contrastive_loss_history.append(float(loss.numpy()))\n",
    "    print(f\"[Contrastive] Epoch {ep+1}/{EPOCHS_CONTRASTIVE}, loss={float(loss.numpy()):.4f}\")\n",
    "\n",
    "# Meta model\n",
    "meta_model = build_meta_model(lstm_encoder, NUM_CLASSES)\n",
    "meta_optimizer = tf.keras.optimizers.Adam(META_LR)\n",
    "\n",
    "def sample_tasks(X, y, num_tasks=NUM_TASKS, support_size=SUPPORT_SIZE, query_size=QUERY_SIZE):\n",
    "    tasks = []\n",
    "    n = len(X)\n",
    "    if n < support_size + query_size:\n",
    "        raise ValueError(f\"Not enough labeled samples to build tasks: need {support_size+query_size}, got {n}\")\n",
    "    for _ in range(num_tasks):\n",
    "        idx = np.random.choice(n, support_size + query_size, replace=False)\n",
    "        X_support, y_support = X[idx[:support_size]], y[idx[:support_size]]\n",
    "        X_query, y_query = X[idx[support_size:]], y[idx[support_size:]]\n",
    "        tasks.append((X_support, y_support, X_query, y_query))\n",
    "    return tasks\n",
    "\n",
    "def inner_update(model, X_support, y_support, lr_inner=INNER_LR):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds_support = model(X_support, training=True)\n",
    "        loss_support = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_support, preds_support))\n",
    "    grads_inner = tape.gradient(loss_support, model.trainable_variables)\n",
    "    updated_vars = [w - lr_inner * g for w, g in zip(model.trainable_variables, grads_inner)]\n",
    "    return updated_vars\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=REPLAY_CAPACITY):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.n_seen = 0\n",
    "    def add(self, X, y):\n",
    "        for xi, yi in zip(X, y):\n",
    "            self.n_seen += 1\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append((xi, yi))\n",
    "            else:\n",
    "                r = np.random.randint(0, self.n_seen)\n",
    "                if r < self.capacity:\n",
    "                    self.buffer[r] = (xi, yi)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        X_s, y_s = zip(*[self.buffer[i] for i in idxs])\n",
    "        return np.array(X_s), np.array(y_s)\n",
    "\n",
    "memory = ReplayBuffer(capacity=REPLAY_CAPACITY)\n",
    "\n",
    "# ===== Fisher Matrix Computation for EWC =====\n",
    "def compute_fisher_matrix(model, X, y, num_samples=100):\n",
    "    fisher = [tf.zeros_like(w) for w in model.trainable_variables]\n",
    "    \n",
    "    # Sample subset of data\n",
    "    idx = np.random.choice(len(X), min(num_samples, len(X)), replace=False)\n",
    "    X_sample = X[idx]\n",
    "    y_sample = y[idx]\n",
    "    \n",
    "    for x, true_label in zip(X_sample, y_sample):\n",
    "        with tf.GradientTape() as tape:\n",
    "            prob = model(np.expand_dims(x, axis=0))[0, true_label]\n",
    "            log_prob = tf.math.log(prob)\n",
    "        grads = tape.gradient(log_prob, model.trainable_variables)\n",
    "        fisher = [f + tf.square(g) for f, g in zip(fisher, grads)]\n",
    "    #print(\"fisher matrix:\",fisher)\n",
    "    return [f / num_samples for f in fisher]\n",
    "\n",
    "# ===== Helpers for flowering focus =====\n",
    "def is_flowering_seq(x_seq, light_idx=2, th_light=550.0):\n",
    "    light_mean = float(np.mean(x_seq[:, light_idx]))\n",
    "    return light_mean >= th_light\n",
    "\n",
    "def hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15):\n",
    "    hv = x_seq[:, hvac_slice]  # [T,4]\n",
    "    if hv.shape[0] < 2:\n",
    "        return 0.0, False\n",
    "    diff = np.abs(hv[1:] - hv[:-1])   # [T-1,4]\n",
    "    rate = float(diff.mean())\n",
    "    return rate, rate >= th_toggle\n",
    "\n",
    "def outer_update_with_lll(meta_model, meta_optimizer, tasks,\n",
    "                          lr_inner=INNER_LR, replay_weight=REPLAY_WEIGHT,\n",
    "                          lambda_ewc=LAMBDA_EWC, prev_weights=None, fisher_matrix=None):\n",
    "    meta_grads = [tf.zeros_like(v) for v in meta_model.trainable_variables]\n",
    "    query_acc_list, query_loss_list = [], []\n",
    "\n",
    "    for X_support, y_support, X_query, y_query in tasks:\n",
    "        orig_vars = [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "        # inner update\n",
    "        updated_vars = inner_update(meta_model, X_support, y_support)\n",
    "        for var, upd in zip(meta_model.trainable_variables, updated_vars):\n",
    "            var.assign(upd)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds_q = meta_model(X_query, training=True)\n",
    "            loss_q = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_query, preds_q))\n",
    "            loss_total = loss_q\n",
    "\n",
    "            # replay\n",
    "            if len(memory) >= 8:\n",
    "                X_old, y_old = memory.sample(batch_size=32)\n",
    "                preds_old = meta_model(X_old, training=True)\n",
    "                replay_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_old, preds_old))\n",
    "                loss_total = (1 - replay_weight) * loss_total + replay_weight * replay_loss\n",
    "            \n",
    "            # EWC (using Fisher matrix)\n",
    "            if prev_weights is not None and fisher_matrix is not None:\n",
    "                ewc_loss = 0.0\n",
    "                for w, pw, f in zip(meta_model.trainable_variables, prev_weights, fisher_matrix):\n",
    "                    ewc_loss += tf.reduce_sum(f * tf.square(w - pw))\n",
    "                loss_total += lambda_ewc * ewc_loss\n",
    "                #for i, f in enumerate(prev_weights):\n",
    "                #    print(f\"Fisher matrix for variable {i} has shape: {f.shape}\")\n",
    "\n",
    "        grads = tape.gradient(loss_total, meta_model.trainable_variables)\n",
    "\n",
    "        # ===== Flowering + HVAC toggling gradient boost =====\n",
    "        flowering_mask = []\n",
    "        toggle_scores = []\n",
    "        for i in range(len(X_query)):\n",
    "            x_seq = X_query[i]  # [T,D]\n",
    "            flw = is_flowering_seq(x_seq, light_idx=2, th_light=550.0)\n",
    "            tscore, tabove = hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15)\n",
    "            flowering_mask.append(bool(flw and tabove))\n",
    "            toggle_scores.append(tscore)\n",
    "\n",
    "        if any(flowering_mask):\n",
    "            ratio = sum(flowering_mask) / len(flowering_mask)\n",
    "            mean_toggle = np.mean([t for m,t in zip(flowering_mask, toggle_scores) if m]) if any(flowering_mask) else 0.0\n",
    "            toggle_boost = min(1.0 + float(mean_toggle)*2.0, FLOWERING_WEIGHT)\n",
    "            boost = 1.0 + (FLOWERING_WEIGHT - 1.0) * ratio\n",
    "            total_boost = float(min(boost * toggle_boost, FLOWERING_WEIGHT))\n",
    "            grads = [g * total_boost for g in grads]\n",
    "\n",
    "        meta_grads = [mg + g / len(tasks) for mg, g in zip(meta_grads, grads)]\n",
    "\n",
    "        q_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(preds_q, axis=1), y_query), tf.float32))\n",
    "        query_acc_list.append(float(q_acc.numpy()))\n",
    "        query_loss_list.append(float(loss_q.numpy()))\n",
    "\n",
    "        # restore original vars\n",
    "        for var, orig in zip(meta_model.trainable_variables, orig_vars):\n",
    "            var.assign(orig)\n",
    "\n",
    "        # update memory\n",
    "        memory.add(X_support, y_support)\n",
    "        memory.add(X_query, y_query)\n",
    "\n",
    "    meta_optimizer.apply_gradients(zip(meta_grads, meta_model.trainable_variables))\n",
    "    return float(np.mean(query_loss_list)), float(np.mean(query_acc_list)), [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "# ======= Train meta-learning =======\n",
    "meta_loss_history, meta_acc_history = [], []\n",
    "prev_weights = None\n",
    "fisher_matrix = None\n",
    "\n",
    "if X_labeled.size > 0:\n",
    "    # Compute Fisher matrix on initial model\n",
    "    fisher_matrix = compute_fisher_matrix(meta_model, X_labeled, y_labeled)\n",
    "    \n",
    "    for ep in range(EPOCHS_META):\n",
    "        tasks = sample_tasks(X_labeled, y_labeled)\n",
    "        loss, acc, prev_weights = outer_update_with_lll(\n",
    "            meta_model, meta_optimizer, tasks, \n",
    "            prev_weights=prev_weights, fisher_matrix=fisher_matrix\n",
    "        )\n",
    "        meta_loss_history.append(loss)\n",
    "        meta_acc_history.append(acc)\n",
    "        print(f\"[Meta] Epoch {ep+1}/{EPOCHS_META}, loss={loss:.4f}, acc={acc:.4f}\")\n",
    "else:\n",
    "    print(\"Skip meta-learning: no labeled data.\")\n",
    "\n",
    "# =============================\n",
    "# 5) Save/Load Fisher Matrix and Model Weights\n",
    "# =============================\n",
    "def save_ewc_assets(model, fisher_matrix, save_dir=\"ewc_assets\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    model.save_weights(os.path.join(save_dir, \"model_weights.h5\"))\n",
    "    \n",
    "    # Save Fisher matrix\n",
    "    fisher_numpy = [f.numpy() for f in fisher_matrix]\n",
    "    np.savez(os.path.join(save_dir, \"fisher_matrix.npz\"), *fisher_numpy)\n",
    "    \n",
    "    print(f\"EWC assets saved to {save_dir}\")\n",
    "\n",
    "def load_ewc_assets(model, save_dir=\"ewc_assets\"):\n",
    "    # Load model weights\n",
    "    model.load_weights(os.path.join(save_dir, \"model_weights.h5\"))\n",
    "    \n",
    "    # Load Fisher matrix\n",
    "    fisher_data = np.load(os.path.join(save_dir, \"fisher_matrix.npz\"))\n",
    "    fisher_matrix = [tf.constant(arr) for arr in fisher_data.values()]\n",
    "    \n",
    "    print(f\"EWC assets loaded from {save_dir}\")\n",
    "    return fisher_matrix\n",
    "\n",
    "# Save assets if we have them\n",
    "if fisher_matrix is not None:\n",
    "    save_ewc_assets(meta_model, fisher_matrix)\n",
    "\n",
    "# Example of loading (commented out since we just saved)\n",
    "# loaded_fisher = load_ewc_assets(meta_model)\n",
    "\n",
    "# =============================\n",
    "# 6) TFLite export (BUILTINS only)\n",
    "# =============================\n",
    "def save_tflite(model, out_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved TFLite:\", out_path)\n",
    "\n",
    "# Save models\n",
    "save_tflite(lstm_encoder, \"lstm_encoder_contrastive.tflite\")\n",
    "if X_labeled.size > 0:\n",
    "    save_tflite(meta_model, \"meta_lstm_classifier.tflite\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74720729-0afd-48e2-901b-1719a4b592c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
