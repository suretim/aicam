{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb87e6da-f5ba-4066-9840-0c278d2cfa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 00:48:43.398826: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-19 00:48:43.400094: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-19 00:48:43.426403: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-19 00:48:43.426988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-19 00:48:44.006127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 5.5613\n",
      "Epoch 2, Loss = 5.1600\n",
      "Epoch 3, Loss = 4.6934\n",
      "Epoch 4, Loss = 4.1651\n",
      "Epoch 5, Loss = 3.5883\n",
      "Epoch 6, Loss = 2.9919\n",
      "Epoch 7, Loss = 2.4253\n",
      "Epoch 8, Loss = 1.9470\n",
      "Epoch 9, Loss = 1.5970\n",
      "Epoch 10, Loss = 1.3774\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Sample embeddings shape: (5, 64)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# =============================\n",
    "# 1) 数据增强 & 构造正样本对\n",
    "# =============================\n",
    "def augment_window(x):\n",
    "    return x + np.random.normal(0, 0.01, x.shape).astype(np.float32)\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for w in X:\n",
    "        anchors.append(w)\n",
    "        positives.append(augment_window(w))\n",
    "    return np.stack(anchors).astype(np.float32), np.stack(positives).astype(np.float32)\n",
    "\n",
    "# =============================\n",
    "# 2) 简单 Encoder 模型\n",
    "# =============================\n",
    "def build_encoder(input_dim, feature_dim=64):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation=\"relu\")(inp)\n",
    "    x = layers.Dense(feature_dim)(x)       # 输出 embedding 向量\n",
    "    x = tf.nn.l2_normalize(x, axis=1)      # 单位化，方便算余弦相似度\n",
    "    return Model(inp, x, name=\"encoder\")\n",
    "\n",
    "# =============================\n",
    "# 3) InfoNCE Loss (对比学习常用)\n",
    "# =============================\n",
    "def info_nce_loss(z_anchor, z_positive, temperature=0.1):\n",
    "    batch_size = tf.shape(z_anchor)[0]\n",
    "\n",
    "    # 拼接所有向量 [2N, d]\n",
    "    z = tf.concat([z_anchor, z_positive], axis=0)\n",
    "\n",
    "    # 余弦相似度 [2N, 2N]\n",
    "    sim = tf.matmul(z, z, transpose_b=True) / temperature\n",
    "\n",
    "    # 构造标签：正样本对应索引\n",
    "    labels = tf.concat([\n",
    "        tf.range(batch_size, batch_size*2),\n",
    "        tf.range(0, batch_size)\n",
    "    ], axis=0)\n",
    "\n",
    "    # 交叉熵 Loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            labels, sim, from_logits=True\n",
    "        )\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# =============================\n",
    "# 4) 训练 Demo\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # 伪造数据 (1000 个样本，每个 32 维)\n",
    "    X = np.random.rand(1000, 32).astype(np.float32)\n",
    "\n",
    "    # 构造 (anchor, positive) pair\n",
    "    anchors, positives = make_contrastive_pairs(X)\n",
    "\n",
    "    # 构建 encoder\n",
    "    encoder = build_encoder(input_dim=32, feature_dim=64)\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "    # 简单训练 10 个 epoch\n",
    "    for epoch in range(10):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_anchor = encoder(anchors, training=True)\n",
    "            z_positive = encoder(positives, training=True)\n",
    "            loss = info_nce_loss(z_anchor, z_positive)\n",
    "\n",
    "        grads = tape.gradient(loss, encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, encoder.trainable_variables))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss = {loss.numpy():.4f}\")\n",
    "\n",
    "    # 训练完成后 encoder 就能提取“更有意义”的 embedding 表示\n",
    "    feats = encoder.predict(X[:5])\n",
    "    print(\"Sample embeddings shape:\", feats.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9309d6-ec4d-4ecc-88f3-475db48c1bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv2)",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
