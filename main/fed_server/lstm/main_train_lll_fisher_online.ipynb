{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93ab9b20-b715-4f33-a24f-5cfab6d51a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contrastive] Epoch 1/10, loss=0.9898\n",
      "[Contrastive] Epoch 2/10, loss=0.5418\n",
      "[Contrastive] Epoch 3/10, loss=0.5915\n",
      "[Contrastive] Epoch 4/10, loss=0.2044\n",
      "[Contrastive] Epoch 5/10, loss=0.2235\n",
      "[Contrastive] Epoch 6/10, loss=0.7443\n",
      "[Contrastive] Epoch 7/10, loss=0.1826\n",
      "[Contrastive] Epoch 8/10, loss=0.2345\n",
      "[Contrastive] Epoch 9/10, loss=0.1065\n",
      "[Contrastive] Epoch 10/10, loss=0.8159\n",
      "‚úÖ Encoder mode: freeze, last_n=N/A\n",
      "\n",
      "üîé [Encoder trainable layers]\n",
      "\n",
      "üîé [Meta model trainable layers]\n",
      "hvac_dense           ‚úÖ trainable\n",
      "meta_dense_64        ‚úÖ trainable\n",
      "meta_dense_32        ‚úÖ trainable\n",
      "Layer 09: hvac_dense           ‚úÖ trainable\n",
      "Layer 11: meta_dense_64        ‚úÖ trainable\n",
      "Layer 12: meta_dense_32        ‚úÖ trainable\n",
      "[Meta] Epoch 1/20, loss=1.1594, acc=0.0300\n",
      "[Meta] Epoch 2/20, loss=1.1195, acc=0.0600\n",
      "[Meta] Epoch 3/20, loss=1.0994, acc=0.4600\n",
      "[Meta] Epoch 4/20, loss=1.0600, acc=0.5800\n",
      "[Meta] Epoch 5/20, loss=1.0472, acc=0.5000\n",
      "[Meta] Epoch 6/20, loss=1.0238, acc=0.9600\n",
      "[Meta] Epoch 7/20, loss=0.9970, acc=0.9700\n",
      "[Meta] Epoch 8/20, loss=0.9656, acc=0.9600\n",
      "[Meta] Epoch 9/20, loss=0.9504, acc=0.9600\n",
      "[Meta] Epoch 10/20, loss=0.9326, acc=0.9600\n",
      "[Meta] Epoch 11/20, loss=0.8993, acc=0.9600\n",
      "[Meta] Epoch 12/20, loss=0.8932, acc=0.9400\n",
      "[Meta] Epoch 13/20, loss=0.8684, acc=0.9400\n",
      "[Meta] Epoch 14/20, loss=0.8449, acc=0.9200\n",
      "[Meta] Epoch 15/20, loss=0.8239, acc=0.9600\n",
      "[Meta] Epoch 16/20, loss=0.7860, acc=0.9800\n",
      "[Meta] Epoch 17/20, loss=0.7668, acc=0.9800\n",
      "[Meta] Epoch 18/20, loss=0.7419, acc=0.9700\n",
      "[Meta] Epoch 19/20, loss=0.7264, acc=0.9700\n",
      "[Meta] Epoch 20/20, loss=0.7022, acc=0.9800\n",
      "layer_shapes.json saved!\n",
      "‚úÖ Trainable weights + Fisher matrix saved to ewc_assets\n",
      "  Total arrays saved: 12\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp9wxs56_p/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9wxs56_p/assets\n",
      "2025-08-22 13:00:06.054167: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-22 13:00:06.054233: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-22 13:00:06.054397: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp9wxs56_p\n",
      "2025-08-22 13:00:06.056299: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-22 13:00:06.056318: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp9wxs56_p\n",
      "2025-08-22 13:00:06.061688: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-22 13:00:06.081824: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp9wxs56_p\n",
      "2025-08-22 13:00:06.094994: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 40595 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite: lstm_encoder_contrastive.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpu2igp4bn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpu2igp4bn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite: meta_lstm_classifier.tflite\n",
      "Trainable: meta_lstm_classifier/meta_dense_64/BiasAdd/ReadVariableOp, index=1, shape=[64]\n",
      "Trainable: meta_lstm_classifier/meta_dense_32/BiasAdd/ReadVariableOp, index=2, shape=[32]\n",
      "Trainable: meta_lstm_classifier/hvac_dense/BiasAdd/ReadVariableOp, index=5, shape=[16]\n",
      "Trainable: meta_lstm_classifier/hvac_dense/MatMul1, index=18, shape=[16  8]\n",
      "Trainable: meta_lstm_classifier/meta_dense_64/MatMul, index=22, shape=[64 80]\n",
      "Trainable: meta_lstm_classifier/meta_dense_32/MatMul, index=23, shape=[32 64]\n",
      "trainable_tensor_indices = [1, 2, 5, 18, 22, 23]\n",
      "meta_lstm_classifier.tflite Done.\n",
      "‚úÖ Ê∏¨Ë©¶ÂâçÂêëËº∏Âá∫ shape: (1, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 13:00:08.226736: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-22 13:00:08.226810: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-22 13:00:08.226999: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpu2igp4bn\n",
      "2025-08-22 13:00:08.230027: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-22 13:00:08.230055: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpu2igp4bn\n",
      "2025-08-22 13:00:08.239346: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-22 13:00:08.270839: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpu2igp4bn\n",
      "2025-08-22 13:00:08.288502: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 61502 microseconds.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Meta-learning pipeline with HVAC-aware features and flowering-period focus.\n",
    "- Expects CSV columns: temp, humid, light, ac, heater, dehum, hum, label\n",
    "- Sliding windows -> contrastive learning (unlabeled) + FOMAML with LLL + EWC (labeled)\n",
    "- Encoder: LSTM on continuous features only (temp/humid/light)\n",
    "- Additional HVAC features: mean on/off rate + toggle rate (abs(diff)) over time\n",
    "- Gradient boost on flowering period with abnormal HVAC toggling\n",
    "- TFLite export restricted to TFLITE_BUILTINS\n",
    "- Includes Fisher matrix computation and loading for EWC\n",
    "\"\"\"\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "import argparse \n",
    "import datetime\n",
    "import json\n",
    "# =============================\n",
    "# Hyperparameters\n",
    "# =============================\n",
    "DATA_GLOB = \"../../../../lll_data/*.csv\"\n",
    "SEQ_LEN = 10\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_CONTRASTIVE = 10\n",
    "EPOCHS_META = 20\n",
    "INNER_LR = 1e-2\n",
    "META_LR = 1e-3\n",
    "NUM_CLASSES = 3\n",
    "NUM_TASKS = 5\n",
    "SUPPORT_SIZE = 10\n",
    "QUERY_SIZE = 20\n",
    "REPLAY_CAPACITY = 1000\n",
    "REPLAY_WEIGHT = 0.3\n",
    "LAMBDA_EWC = 1e-3\n",
    "NUM_FEATS=7\n",
    "#ENCODER_MODE = finetune  freeze last_n\n",
    "ENCODER_MODE =\"freeze\"\n",
    "LAST_N= 1\n",
    "FLOWERING_WEIGHT = 2.0  # gradient boost upper bound for flowering-focus\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "# Index conventions\n",
    "CONT_IDX = [0, 1, 2]   # temp, humid, light\n",
    "HVAC_IDX = [3, 4, 5, 6]  # ac, heater, dehum, hum\n",
    " \n",
    "\n",
    "def make_indices(model_path=\"meta_lstm_classifier.tflite\", header_path=\"trainable_tensor_indices.h\"):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "    trainable_indices = []\n",
    "\n",
    "    for t in tensor_details:\n",
    "        name = t['name']\n",
    "        idx  = t['index']\n",
    "        shape = t['shape']\n",
    "\n",
    "        # ÂÉÖÈÅ∏ Dense Â±§Êàñ hvac_dense ÁöÑ kernel/bias\n",
    "        if (\"meta_dense\" in name or \"hvac_dense\" in name):\n",
    "            # ÈÅéÊøæÊéâ fused/activation tensor\n",
    "            if \"Relu\" in name or \";\" in name:\n",
    "                continue\n",
    "\n",
    "            # bias ‰∏ÄËà¨ÊòØ 1DÔºåkernel ‰∏ÄËà¨ÊòØ 2D\n",
    "            if len(shape) == 1 or len(shape) == 2:\n",
    "                print(f\"Trainable: {name}, index={idx}, shape={shape}\")\n",
    "                trainable_indices.append(idx)\n",
    "\n",
    "    # ÁîüÊàê C Â§¥Êñá‰ª∂\n",
    "    with open(header_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\")\n",
    "        f.write(f\"const int trainable_tensor_indices[] = {{{', '.join(map(str, trainable_indices))}}};\\n\")\n",
    "        f.write(f\"const int trainable_tensor_count = {len(trainable_indices)};\\n\")\n",
    "    print(\"trainable_tensor_indices =\", trainable_indices)\n",
    "    return trainable_indices\n",
    "  \n",
    "def generate_trainable_tensor_indices0(model, tflite_model_path, header_path=\"trainable_tensor_indices.h\"):\n",
    "    variable_names = [v.name for v in model.trainable_variables]\n",
    "    print(\"Python trainable variables:\")\n",
    "    for i, name in enumerate(variable_names):\n",
    "        print(i, name)\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "    trainable_tensor_indices = []\n",
    "\n",
    "    for v_name in variable_names:\n",
    "        v_name_clean = v_name.split(':')[0]  # ÂéªÊéâ \":0\"\n",
    "        matched = False\n",
    "\n",
    "        v_last = v_name_clean.split('/')[-1]  # kernel Êàñ bias\n",
    "        for t in tensor_details:\n",
    "            t_last = t['name'].split('/')[-1]\n",
    "            if v_last == t_last:\n",
    "                trainable_tensor_indices.append(t['index'])\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            print(f\"Warning: variable {v_name_clean} not found in tflite tensors!\")\n",
    "\n",
    "    print(\"trainable_tensor_indices =\", trainable_tensor_indices)\n",
    "\n",
    "    # ÂèØÈÄâÔºöÁîüÊàê C Â§¥Êñá‰ª∂\n",
    "    with open(header_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\")\n",
    "        f.write(f\"const int trainable_tensor_indices[] = {{{', '.join(map(str, trainable_tensor_indices))}}};\\n\")\n",
    "        f.write(f\"const int trainable_tensor_count = {len(trainable_tensor_indices)};\\n\")\n",
    " \n",
    "\n",
    "\n",
    "def build_csv_data(data_glob):    \n",
    "    \n",
    "    # =============================\n",
    "    # 1) Load CSV -> Sliding windows\n",
    "    # =============================\n",
    "    X_labeled_list, y_labeled_list = [], []\n",
    "    X_unlabeled_list = []\n",
    "    \n",
    "    for file in sorted(glob.glob(data_glob)):\n",
    "        df = pd.read_csv(file).fillna(-1)\n",
    "        data = df.values.astype(np.float32)\n",
    "        feats, labels = data[:, :-1], data[:, -1]\n",
    "        for i in range(len(data) - SEQ_LEN + 1):\n",
    "            w_x = feats[i:i + SEQ_LEN]\n",
    "            w_y = labels[i + SEQ_LEN - 1]\n",
    "            if w_y == -1:\n",
    "                X_unlabeled_list.append(w_x)\n",
    "            else:\n",
    "                X_labeled_list.append(w_x)\n",
    "                y_labeled_list.append(int(w_y))\n",
    "    \n",
    "    X_unlabeled = np.array(X_unlabeled_list, dtype=np.float32) if len(X_unlabeled_list) > 0 else np.empty((0,), dtype=np.float32)\n",
    "    if len(X_labeled_list) > 0:\n",
    "        X_labeled = np.array(X_labeled_list, dtype=np.float32)\n",
    "        y_labeled = np.array(y_labeled_list, dtype=np.int32)\n",
    "    else:\n",
    "        X_labeled = np.empty((0, SEQ_LEN, X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7), dtype=np.float32)\n",
    "        y_labeled = np.empty((0,), dtype=np.int32)\n",
    "    \n",
    "    NUM_FEATS = X_labeled.shape[2] if X_labeled.size > 0 else (X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7)\n",
    "    \n",
    "    if NUM_FEATS < 7:\n",
    "        raise ValueError(\"Expected at least 7 features per timestep: [temp, humid, light, ac, heater, dehum, hum]. Found: %d\" % NUM_FEATS)\n",
    "\n",
    "    # Provide unlabeled fallback if none\n",
    "    if X_unlabeled.size == 0:\n",
    "        X_unlabeled = np.random.randn(200, SEQ_LEN, NUM_FEATS).astype(np.float32)\n",
    " \n",
    "    return X_unlabeled,X_labeled, y_labeled\n",
    "# =============================\n",
    "# 2) Contrastive learning\n",
    "# =============================\n",
    "def augment_window(x):\n",
    "    \"\"\"Only perturb continuous channels to keep binary HVAC channels intact.\"\"\"\n",
    "    x_aug = x.copy()\n",
    "    x_aug[:, CONT_IDX] = x[:, CONT_IDX] + np.random.normal(0, 0.01, x[:, CONT_IDX].shape).astype(np.float32)\n",
    "    return x_aug\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for w in X:\n",
    "        anchors.append(w)\n",
    "        positives.append(augment_window(w))\n",
    "    return np.stack(anchors).astype(np.float32), np.stack(positives).astype(np.float32)\n",
    "\n",
    "class NTXentLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.2):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    def call(self, z_i, z_j):\n",
    "        z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "        z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "        logits = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_i)[0])\n",
    "        loss_i = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_j = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        return tf.reduce_mean(loss_i + loss_j)\n",
    "\n",
    "# =============================\n",
    "# 3) LSTM Encoder (unroll=True, continuous only)\n",
    "# =============================\n",
    "   \n",
    "def build_lstm_encoder(seq_len, num_feats, feature_dim=FEATURE_DIM):\n",
    "    inp = layers.Input(shape=(seq_len, num_feats), name=\"encoder_input\")\n",
    "    x_cont = layers.Lambda(lambda z: z[:, :, :3], name=\"encoder_lambda\")(inp)  # [B,T,3]\n",
    "    x = layers.LSTM(feature_dim, unroll=True, name=\"encoder_lstm\")(x_cont)\n",
    "    out = layers.Dense(feature_dim, activation=\"relu\", name=\"encoder_dense\")(x)\n",
    "    return models.Model(inp, out, name=\"lstm_encoder\")\n",
    "# =============================\n",
    "# 4) Meta Model (Encoder + HVAC features)\n",
    "# =============================\n",
    "def build_meta_model(encoder, num_classes=NUM_CLASSES):\n",
    "    inp = layers.Input(shape=(SEQ_LEN, NUM_FEATS), name=\"meta_input\")\n",
    "    z_enc = encoder(inp)  # [B, FEATURE_DIM]\n",
    "\n",
    "    # HVAC slice\n",
    "    hvac = layers.Lambda(lambda z: z[:, :, 3:7], name=\"hvac_slice\")(inp)   # [B,T,4]\n",
    "    hvac_mean = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1), name=\"hvac_mean\")(hvac)  # [B,4]\n",
    "\n",
    "    # Toggle rate via abs(diff)\n",
    "    hvac_shift = layers.Lambda(lambda z: z[:, 1:, :], name=\"hvac_shift\")(hvac)      # [B,T-1,4]\n",
    "    hvac_prev  = layers.Lambda(lambda z: z[:, :-1, :], name=\"hvac_prev\")(hvac)     # [B,T-1,4]\n",
    "    hvac_diff  = layers.Lambda(lambda t: tf.abs(t[0] - t[1]), name=\"hvac_diff\")([hvac_shift, hvac_prev])  # [B,T-1,4]\n",
    "    hvac_toggle_rate = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1), name=\"hvac_toggle_rate\")(hvac_diff)    # [B,4]\n",
    "\n",
    "    hvac_feat = layers.Concatenate(name=\"hvac_concat\")([hvac_mean, hvac_toggle_rate])  # [B,8]\n",
    "    hvac_feat = layers.Dense(16, activation=\"relu\", name=\"hvac_dense\")(hvac_feat)\n",
    "\n",
    "    x = layers.Concatenate(name=\"encoder_hvac_concat\")([z_enc, hvac_feat])\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"meta_dense_64\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\", name=\"meta_dense_32\")(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\", name=\"meta_out\")(x)\n",
    "\n",
    "    return models.Model(inp, out, name=\"meta_lstm_classifier\")\n",
    "\n",
    "\n",
    "def sample_tasks(X, y, num_tasks=NUM_TASKS, support_size=SUPPORT_SIZE, query_size=QUERY_SIZE):\n",
    "    tasks = []\n",
    "    n = len(X)\n",
    "    if n < support_size + query_size:\n",
    "        raise ValueError(f\"Not enough labeled samples to build tasks: need {support_size+query_size}, got {n}\")\n",
    "    for _ in range(num_tasks):\n",
    "        idx = np.random.choice(n, support_size + query_size, replace=False)\n",
    "        X_support, y_support = X[idx[:support_size]], y[idx[:support_size]]\n",
    "        X_query, y_query = X[idx[support_size:]], y[idx[support_size:]]\n",
    "        tasks.append((X_support, y_support, X_query, y_query))\n",
    "    return tasks\n",
    "\n",
    "def inner_update(model, X_support, y_support, lr_inner=INNER_LR):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds_support = model(X_support, training=True)\n",
    "        loss_support = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_support, preds_support))\n",
    "    grads_inner = tape.gradient(loss_support, model.trainable_variables)\n",
    "    updated_vars = [w - lr_inner * g for w, g in zip(model.trainable_variables, grads_inner)]\n",
    "    return updated_vars\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=REPLAY_CAPACITY):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.n_seen = 0\n",
    "    def add(self, X, y):\n",
    "        for xi, yi in zip(X, y):\n",
    "            self.n_seen += 1\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append((xi, yi))\n",
    "            else:\n",
    "                r = np.random.randint(0, self.n_seen)\n",
    "                if r < self.capacity:\n",
    "                    self.buffer[r] = (xi, yi)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        X_s, y_s = zip(*[self.buffer[i] for i in idxs])\n",
    "        return np.array(X_s), np.array(y_s)\n",
    "\n",
    "\n",
    "# ===== Fisher Matrix Computation for EWC =====\n",
    "def compute_fisher_matrix(model, X, y, num_samples=100):\n",
    "    fisher = [tf.zeros_like(w) for w in model.trainable_variables]\n",
    "    \n",
    "    # Sample subset of data\n",
    "    idx = np.random.choice(len(X), min(num_samples, len(X)), replace=False)\n",
    "    X_sample = X[idx]\n",
    "    y_sample = y[idx]\n",
    "    \n",
    "    for x, true_label in zip(X_sample, y_sample):\n",
    "        with tf.GradientTape() as tape:\n",
    "            prob = model(np.expand_dims(x, axis=0))[0, true_label]\n",
    "            log_prob = tf.math.log(prob)\n",
    "        grads = tape.gradient(log_prob, model.trainable_variables)\n",
    "        fisher = [f + tf.square(g) for f, g in zip(fisher, grads)]\n",
    "    #print(\"fisher matrix:\",fisher)\n",
    "    return [f / num_samples for f in fisher]\n",
    "\n",
    "# ===== Helpers for flowering focus =====\n",
    "def is_flowering_seq(x_seq, light_idx=2, th_light=550.0):\n",
    "    light_mean = float(np.mean(x_seq[:, light_idx]))\n",
    "    return light_mean >= th_light\n",
    "\n",
    "def hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15):\n",
    "    hv = x_seq[:, hvac_slice]  # [T,4]\n",
    "    if hv.shape[0] < 2:\n",
    "        return 0.0, False\n",
    "    diff = np.abs(hv[1:] - hv[:-1])   # [T-1,4]\n",
    "    rate = float(diff.mean())\n",
    "    return rate, rate >= th_toggle\n",
    "\n",
    "def outer_update_with_lll(memory,meta_model, meta_optimizer, tasks,\n",
    "                          lr_inner=INNER_LR, replay_weight=REPLAY_WEIGHT,\n",
    "                          lambda_ewc=LAMBDA_EWC, prev_weights=None, fisher_matrix=None):\n",
    "    meta_grads = [tf.zeros_like(v) for v in meta_model.trainable_variables]\n",
    "    query_acc_list, query_loss_list = [], []\n",
    "\n",
    "    for X_support, y_support, X_query, y_query in tasks:\n",
    "        orig_vars = [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "        # inner update\n",
    "        updated_vars = inner_update(meta_model, X_support, y_support,lr_inner=lr_inner,)\n",
    "        for var, upd in zip(meta_model.trainable_variables, updated_vars):\n",
    "            var.assign(upd)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds_q = meta_model(X_query, training=True)\n",
    "            loss_q = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_query, preds_q))\n",
    "            loss_total = loss_q\n",
    "\n",
    "            # replay\n",
    "            if len(memory) >= 8:\n",
    "                X_old, y_old = memory.sample(batch_size=32)\n",
    "                preds_old = meta_model(X_old, training=True)\n",
    "                replay_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_old, preds_old))\n",
    "                loss_total = (1 - replay_weight) * loss_total + replay_weight * replay_loss\n",
    "            \n",
    "            # EWC (using Fisher matrix)\n",
    "            if prev_weights is not None and fisher_matrix is not None:\n",
    "                ewc_loss = 0.0\n",
    "\n",
    "                for w, pw, f in zip(meta_model.trainable_variables, prev_weights, fisher_matrix):\n",
    "                    ewc_loss += tf.reduce_sum(f * tf.square(w - pw))\n",
    "                loss_total += lambda_ewc * ewc_loss\n",
    "                #for i, f in enumerate(prev_weights):\n",
    "                #    print(f\"Fisher matrix for variable {i} has shape: {f.shape}\")\n",
    "\n",
    "        grads = tape.gradient(loss_total, meta_model.trainable_variables)\n",
    "\n",
    "        # ===== Flowering + HVAC toggling gradient boost =====\n",
    "        flowering_mask = []\n",
    "        toggle_scores = []\n",
    "        for i in range(len(X_query)):\n",
    "            x_seq = X_query[i]  # [T,D]\n",
    "            flw = is_flowering_seq(x_seq, light_idx=2, th_light=550.0)\n",
    "            tscore, tabove = hvac_toggle_score(x_seq, hvac_slice=slice(3,7), th_toggle=0.15)\n",
    "            flowering_mask.append(bool(flw and tabove))\n",
    "            toggle_scores.append(tscore)\n",
    "\n",
    "        if any(flowering_mask):\n",
    "            ratio = sum(flowering_mask) / len(flowering_mask)\n",
    "            mean_toggle = np.mean([t for m,t in zip(flowering_mask, toggle_scores) if m]) if any(flowering_mask) else 0.0\n",
    "            toggle_boost = min(1.0 + float(mean_toggle)*2.0, FLOWERING_WEIGHT)\n",
    "            boost = 1.0 + (FLOWERING_WEIGHT - 1.0) * ratio\n",
    "            total_boost = float(min(boost * toggle_boost, FLOWERING_WEIGHT))\n",
    "            grads = [g * total_boost for g in grads]\n",
    "\n",
    "        meta_grads = [mg + g / len(tasks) for mg, g in zip(meta_grads, grads)]\n",
    "\n",
    "        q_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(preds_q, axis=1), y_query), tf.float32))\n",
    "        query_acc_list.append(float(q_acc.numpy()))\n",
    "        query_loss_list.append(float(loss_q.numpy()))\n",
    "\n",
    "        # restore original vars\n",
    "        for var, orig in zip(meta_model.trainable_variables, orig_vars):\n",
    "            var.assign(orig)\n",
    "\n",
    "        # update memory\n",
    "        memory.add(X_support, y_support)\n",
    "        memory.add(X_query, y_query)\n",
    "\n",
    "    meta_optimizer.apply_gradients(zip(meta_grads, meta_model.trainable_variables))\n",
    "    return float(np.mean(query_loss_list)), float(np.mean(query_acc_list)), [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "# =============================\n",
    "# 5) Save/Load Fisher Matrix and Model Weights\n",
    "# =============================\n",
    " \n",
    "\n",
    "def save_fisher_and_weights(model, fisher_matrix, save_dir=\"ewc_assets\"):\n",
    "    \"\"\"\n",
    "    model: Â∑≤Á∂ì trainable ÁöÑ meta_model\n",
    "    fisher_matrix: Â∞çÊáâ model.trainable_variables ÁöÑ Fisher matrix (list of tf.Tensor)\n",
    "    \"\"\"\n",
    "    trainable_vars = model.trainable_variables\n",
    "    weights = [v.numpy() for v in trainable_vars]\n",
    "    fisher = [f.numpy() for f in fisher_matrix]\n",
    "    # ÂÅáË®≠ weights Â∑≤Á∂ìÂèñÂæó\n",
    "    #weights = [w.numpy() for w in model.trainable_weights]\n",
    "    \n",
    "    # ÁîüÊàê layer_shapes\n",
    "    layer_shapes = [list(w.shape) for w in weights]\n",
    "    \n",
    "    # Â∞á layer_shapes ËΩâÊàê JSON bytes\n",
    "    layer_shapes_json = json.dumps(layer_shapes)\n",
    "    # ÂØ´ÂÖ•Ê™îÊ°à [[8, 16], [16], [80, 64], [64], [64, 32], [32]]\n",
    "    with open(os.path.join(save_dir,\"layer_shapes.json\") , \"w\") as f:\n",
    "        f.write(layer_shapes_json)\n",
    "    \n",
    "    print(\"layer_shapes.json saved!\")\n",
    "    layer_shapes_bytes = layer_shapes_json.encode('utf-8')\n",
    "    # ÂÑ≤Â≠òÊàê .npz\n",
    "    np.savez(os.path.join(save_dir,\"ewc_assets.npz\"), *weights, *fisher)\n",
    "    #ewc_buffer = np.concatenate([trainable_weights_flattened..., fisher_matrices_flattened...])\n",
    "\n",
    "    print(f\"‚úÖ Trainable weights + Fisher matrix saved to {save_dir}\")\n",
    "    print(f\"  Total arrays saved: {len(weights) + len(fisher)}\")\n",
    "\n",
    "# ÁØÑ‰æã\n",
    "# fisher_matrix = compute_fisher_matrix(meta_model, X_labeled, y_labeled)\n",
    "# save_fisher_and_weights(meta_model, fisher_matrix, \"ewc_assets.npz\")\n",
    "\n",
    "\n",
    "def save_ewc_assets(model, fisher_matrix, save_dir=\"ewc_assets\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    model.save_weights(os.path.join(save_dir, \"model_weights.h5\"))\n",
    "    \n",
    "    # Save Fisher matrix\n",
    "    fisher_numpy = [f.numpy() for f in fisher_matrix]\n",
    "    np.savez(os.path.join(save_dir, \"fisher_matrix.npz\"), *fisher_numpy)\n",
    "    \n",
    "    print(f\"EWC assets saved to {save_dir}\")\n",
    "\n",
    "def load_ewc_assets(model, save_dir=\"ewc_assets\"):\n",
    "    # Load model weights\n",
    "    model.load_weights(os.path.join(save_dir, \"model_weights.h5\"))\n",
    "    \n",
    "    # Load Fisher matrix\n",
    "    fisher_data = np.load(os.path.join(save_dir, \"fisher_matrix.npz\"))\n",
    "    fisher_matrix = [tf.constant(arr) for arr in fisher_data.values()]\n",
    "    \n",
    "    print(f\"EWC assets loaded from {save_dir}\")\n",
    "    return model,fisher_matrix\n",
    "\n",
    "# Example of loading (commented out since we just saved)\n",
    "# loaded_fisher = load_ewc_assets(meta_model)\n",
    "\n",
    "# =============================\n",
    "# 6) TFLite export (BUILTINS only)\n",
    "# =============================\n",
    "def save_tflite(model, out_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved TFLite:\", out_path)\n",
    "    \n",
    "def set_trainable_layers(encoder, meta_model, encoder_mode=\"finetune\", last_n=1):\n",
    "    \"\"\"\n",
    "    Ë®≠ÂÆö encoder Ëàá meta_model Â±§ÁöÑ trainable ÁãÄÊÖã\n",
    "\n",
    "    encoder_mode: \"finetune\" / \"freeze\" / \"last_n\"\n",
    "    last_n: Âè™ÊúâÂú® encoder_mode=\"last_n\" ÊâçÊúâÊÑèÁæ©\n",
    "    \"\"\"\n",
    "    # ===== Encoder =====\n",
    "    if encoder_mode == \"finetune\":\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = True\n",
    "    elif encoder_mode == \"freeze\":\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = False\n",
    "    elif encoder_mode == \"last_n\":\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = False\n",
    "        if last_n is not None:\n",
    "            for layer in encoder.layers[-last_n:]:\n",
    "                layer.trainable = True\n",
    "\n",
    "    # ===== Meta Model =====\n",
    "    # ÂÅáË®≠ meta_model ÁöÑ Dense Â±§ÈÉΩÂèØ‰ª•ÂñÆÁç® trainable\n",
    "    for layer in meta_model.layers:\n",
    "        # Â¶ÇÊûúÊòØ encoder ÁöÑÂ≠êÊ®°ÂûãÔºå‰∏çÊîπËÆäÔºàÁî± encoder ÁÆ°ÁêÜÔºâ\n",
    "        #if layer.name.startswith(\"lstm_encoder\") or layer.name.startswith(\"encoder_\"):\n",
    "        #    continue\n",
    "        # ÂÖ∂‰ªñÂ±§ÂÖ®ÈÉ®ÂèØË®ìÁ∑¥\n",
    "        if layer.name.startswith(\"meta_dense\") or layer.name.startswith(\"hvac_dense\"):\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    print(f\"‚úÖ Encoder mode: {encoder_mode}, last_n={last_n if encoder_mode=='last_n' else 'N/A'}\")\n",
    "\n",
    "    # ÂàóÂç∞ÂØ¶Èöõ trainable Â±§ÔºàÂè™ÂàóÂá∫ÊúâÊ¨äÈáçÁöÑÔºâ\n",
    "    print(\"\\nüîé [Encoder trainable layers]\")\n",
    "    for layer in encoder.layers:\n",
    "        if layer.trainable_weights:\n",
    "            print(f\"{layer.name:<20} {'‚úÖ trainable' if layer.trainable else '‚ùå frozen'}\")\n",
    "\n",
    "    print(\"\\nüîé [Meta model trainable layers]\")\n",
    "    for layer in meta_model.layers:\n",
    "        if layer.trainable_weights:\n",
    "            print(f\"{layer.name:<20} {'‚úÖ trainable' if layer.trainable else '‚ùå frozen'}\")\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"trainable_layers_{timestamp}.txt\"\n",
    "    \n",
    "    log_lines = []\n",
    "    log_lines.append(f\"‚öôÔ∏è Encoder mode: {ENCODER_MODE}, last_n={LAST_N if ENCODER_MODE=='last_n' else 'N/A'}\\n\")\n",
    "    \n",
    "      \n",
    "    for i, layer in enumerate(meta_model.layers):\n",
    "        # Âè™È°ØÁ§∫Êúâ trainable Ê¨äÈáçÁöÑÂ±§\n",
    "        if layer.trainable_weights:\n",
    "            status = \"‚úÖ trainable\" if layer.trainable else \"‚ùå frozen\"\n",
    "            line = f\"Layer {i:02d}: {layer.name:<20} {status}\"\n",
    "            print(line)\n",
    "            log_lines.append(line)\n",
    "\n",
    "    #with open(log_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    #    f.write(\"\\n\".join(log_lines))\n",
    "    \n",
    "    #print(f\"\\nüìù trainable Â±§Ê∏ÖÂñÆÂ∑≤ÂÑ≤Â≠òÂà∞ {log_filename}\\n\")  \n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Êõ¥Êñ∞ÂÖ®Â±ÄÂèòÈáè\n",
    "    global  ENCODER_MODE , LAST_N \n",
    "    ENCODER_MODE = args.encoder_mode \n",
    "    LAST_N = args.last_n \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # ======= Train meta-learning ======= \n",
    "        # Build models\n",
    "        X_unlabeled,X_labeled, y_labeled=build_csv_data(data_glob=DATA_GLOB)\n",
    "        lstm_encoder = build_lstm_encoder(SEQ_LEN, NUM_FEATS, FEATURE_DIM)\n",
    "        contrastive_opt = tf.keras.optimizers.Adam()\n",
    "        ntxent = NTXentLoss(temperature=0.2)\n",
    "        \n",
    "        \n",
    "        anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "        contrast_ds = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(2048).batch(BATCH_SIZE)\n",
    "        \n",
    "        # Train contrastive\n",
    "        contrastive_loss_history = []\n",
    "        for ep in range(EPOCHS_CONTRASTIVE):\n",
    "            for a, p in contrast_ds:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    za = lstm_encoder(a, training=True)\n",
    "                    zp = lstm_encoder(p, training=True)\n",
    "                    loss = ntxent(za, zp)\n",
    "                grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "                contrastive_opt.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "            contrastive_loss_history.append(float(loss.numpy()))\n",
    "            print(f\"[Contrastive] Epoch {ep+1}/{EPOCHS_CONTRASTIVE}, loss={float(loss.numpy()):.4f}\")\n",
    "        \n",
    "        # Meta model\n",
    "        meta_model = build_meta_model(lstm_encoder, NUM_CLASSES)\n",
    "        meta_optimizer = tf.keras.optimizers.Adam(META_LR)\n",
    "        set_trainable_layers(lstm_encoder, meta_model, ENCODER_MODE, LAST_N)\n",
    "\n",
    "        memory = ReplayBuffer(capacity=REPLAY_CAPACITY)\n",
    "        meta_loss_history, meta_acc_history = [], []\n",
    "        prev_weights = None\n",
    "        fisher_matrix = None\n",
    "        \n",
    "        if X_labeled.size > 0:\n",
    "            # Compute Fisher matrix on initial model\n",
    "            fisher_matrix = compute_fisher_matrix(meta_model, X_labeled, y_labeled)\n",
    "            \n",
    "            for ep in range(EPOCHS_META):\n",
    "                tasks = sample_tasks(X_labeled, y_labeled)\n",
    "                loss, acc, prev_weights = outer_update_with_lll(memory=memory,\n",
    "                    meta_model=meta_model, meta_optimizer=meta_optimizer, tasks=tasks, \n",
    "                    lr_inner=INNER_LR,prev_weights=prev_weights, fisher_matrix=fisher_matrix\n",
    "                )\n",
    "                meta_loss_history.append(loss)\n",
    "                meta_acc_history.append(acc)\n",
    "                print(f\"[Meta] Epoch {ep+1}/{EPOCHS_META}, loss={loss:.4f}, acc={acc:.4f}\")\n",
    "        else:\n",
    "            print(\"Skip meta-learning: no labeled data.\")\n",
    "        \n",
    "        # Save assets if we have them\n",
    "        if fisher_matrix is not None:\n",
    "            save_fisher_and_weights(model=meta_model, fisher_matrix=fisher_matrix)\n",
    "            #save_ewc_assets(meta_model, fisher_matrix)\n",
    "\n",
    "\n",
    "        # Save models\n",
    "        save_tflite(lstm_encoder, \"lstm_encoder_contrastive.tflite\")\n",
    "        if X_labeled.size > 0:\n",
    "            save_tflite(meta_model, \"meta_lstm_classifier.tflite\")\n",
    "            make_indices(model_path=\"meta_lstm_classifier.tflite\")\n",
    "            #model = tf.keras.models.load_model(\"your_keras_model.h5\")\n",
    "            #generate_trainable_tensor_indices(meta_model, \"meta_lstm_classifier.tflite\")\n",
    "\n",
    "            print(\"meta_lstm_classifier.tflite Done.\")\n",
    "        \n",
    "        # ============ Ê∏¨Ë©¶Ë∑ë‰∏ÄÊ¨°ÂâçÂêë ============\n",
    "         \n",
    "        dummy_x = np.random.rand(1, 10, 7).astype(np.float32)\n",
    "        dummy_y = lstm_encoder(dummy_x)\n",
    "        print(\"‚úÖ Ê∏¨Ë©¶ÂâçÂêëËº∏Âá∫ shape:\", dummy_y.shape)\n",
    "\n",
    "\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Skip meta-learning: no labeled data.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser() \n",
    "    parser.add_argument(\"--encoder_mode\", type=str, default=ENCODER_MODE) \n",
    "    parser.add_argument(\"--last_n\", type=int, default=LAST_N) \n",
    "    #args = parser.parse_args()\n",
    "    # ============ Ëß£ÊûêÂèÉÊï∏ ============\n",
    "    args, unknown = parser.parse_known_args()\n",
    "  \n",
    "    \n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae8a4be7-c98b-4b9c-ac14-097228760806",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
