#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Meta-learning pipeline with HVAC-aware features and flowering-period focus.
- Expects CSV columns: temp, humid, light, ac, heater, dehum, hum, label
- Sliding windows -> contrastive learning (unlabeled) + FOMAML with LLL + EWC (labeled)
- Encoder: LSTM on continuous features only (temp/humid/light)
- Additional HVAC features: mean on/off rate + toggle rate (abs(diff)) over time
- Gradient boost on flowering period with abnormal HVAC toggling
- TFLite export restricted to TFLITE_BUILTINS
- Includes Fisher matrix computation and loading for EWC
- V1.1 (cleaned & runnable)
"""

import os
import glob
import json
import random
import datetime
from typing import List, Tuple
from copy import deepcopy

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models
import keras
DATA_GLOB = "../../../../data/lll_data/*.csv"

NUM_FEATS = 7


ENCODER_MODE = "freeze"  # one of {"finetune","freeze","last_n"}
LAST_N = 1

np.random.seed(42)
tf.random.set_seed(42)
random.seed(42)

# Index conventions
CONT_IDX = [0, 1, 2]   # temp, humid, light
HVAC_IDX = [3, 4, 5, 6]  # ac, heater, dehum, hum
REPLAY_CAPACITY = 1000

SEQ_LEN = 10
FEATURE_DIM = 64
BATCH_SIZE = 32
EPOCHS_CONTRASTIVE = 10
EPOCHS_META = 20
INNER_LR = 1e-2
META_LR = 1e-3
NUM_CLASSES = 3
NUM_TASKS = 5
SUPPORT_SIZE = 10
QUERY_SIZE = 20
REPLAY_WEIGHT = 0.3
NUM_CLASSES_OLD = 2
NUM_CLASSES_NEW = 3
EPOCHS_OLD = 5
EPOCHS_NEW = 5
LAMBDA_EWC = 1.0
FLOWERING_WEIGHT = 2.0  # gradient boost upper bound for flowering-focus


# ------------------------------------------------------------
# Data utils
# ------------------------------------------------------------

# ---------------- Data Utils ----------------


# ------------------------------------------------------------
# Replay Buffer
# ------------------------------------------------------------

class ReplayBuffer:
    def __init__(self, capacity: int = REPLAY_CAPACITY):
        self.buffer: List[Tuple[np.ndarray, int]] = []
        self.capacity = capacity
        self.n_seen = 0
    def add(self, X: np.ndarray, y: np.ndarray):
        for xi, yi in zip(X, y):
            self.n_seen += 1
            if len(self.buffer) < self.capacity:
                self.buffer.append((xi, yi))
            else:
                r = np.random.randint(0, self.n_seen)
                if r < self.capacity:
                    self.buffer[r] = (xi, yi)
    def __len__(self):
        return len(self.buffer)
    def sample(self, batch_size: int):
        batch_size = min(batch_size, len(self.buffer))
        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)
        X_s, y_s = zip(*[self.buffer[i] for i in idxs])
        return np.array(X_s), np.array(y_s)

# ------------------------------------------------------------
# Contrastive loss (SimCLR-style, pairwise)
# ------------------------------------------------------------

class NTXentLoss(tf.keras.losses.Loss):
    def __init__(self, temperature: float = 0.2):
        super().__init__()
        self.temperature = temperature
    def call(self, z_i: tf.Tensor, z_j: tf.Tensor):
        z_i = tf.math.l2_normalize(z_i, axis=1)
        z_j = tf.math.l2_normalize(z_j, axis=1)
        logits = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature
        labels = tf.range(tf.shape(z_i)[0])
        loss_i = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)
        loss_j = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)
        return tf.reduce_mean(loss_i + loss_j)

# ------------------------------------------------------------
# Meta model
# ------------------------------------------------------------

class MetaModel:
    def __init__(self,num_classes=2, lambda_ewc=0.4, feature_dim: int = FEATURE_DIM):
        self.feature_dim = feature_dim
        self.lambda_ewc = lambda_ewc


        self.old_params = None
        self.fisher = None
        self.encoder = self.build_lstm_encoder(seq_len=SEQ_LEN, num_feats=NUM_FEATS)
        self.model = self.build_meta_model(self.encoder)
        self.num_classes = num_classes
        self.classifier = self.build_classifier(self.encoder, num_classes)
        self.old_params = None
        self.fisher = None
    @staticmethod
    def build_lstm_encoder(seq_len=SEQ_LEN, num_feats=NUM_FEATS, feature_dim=FEATURE_DIM):
        inp = layers.Input(shape=(seq_len, num_feats), dtype=tf.float32)
        # LSTM å±•å¼€ï¼Œunroll=True
        x = layers.LSTM(feature_dim, return_sequences=True, unroll=True)(inp)
        x = layers.LSTM(feature_dim, unroll=True)(x)
        out = layers.Dense(feature_dim, activation='relu')(x)
        return models.Model(inp, out, name="lstm_encoder")

    @staticmethod
    def build_classifier(encoder, num_classes=NUM_CLASSES_NEW):
        inp = layers.Input(shape=(SEQ_LEN, NUM_FEATS))
        z = encoder(inp)
        out = layers.Dense(num_classes, activation='softmax')(z)
        return models.Model(inp, out, name="meta_model")



    @staticmethod
    def ewc_loss(model, old_params, fisher, lambda_ewc=LAMBDA_EWC):
        if old_params is None or fisher is None:
            return 0.0
        loss = 0.0
        for w, w_old, f in zip(model.trainable_variables, old_params, fisher):
            loss += tf.reduce_sum(f * tf.square(w - w_old))
        return lambda_ewc * loss


    @staticmethod
    def save_ewc_assets(model: tf.keras.Model, fisher_matrix: List[tf.Tensor], save_dir: str = "ewc_assets"):
        os.makedirs(save_dir, exist_ok=True)
        model.save_weights(os.path.join(save_dir, "model_weights.h5"))
        fisher_numpy = [f.numpy() for f in fisher_matrix]
        np.savez(os.path.join(save_dir, "fisher_matrix.npz"), *fisher_numpy)
        print(f"EWC assets saved to {save_dir}")

    @staticmethod
    def load_ewc_assets(model: tf.keras.Model, save_dir: str = "ewc_assets"):
        model.load_weights(os.path.join(save_dir, "model_weights.h5"))
        fisher_data = np.load(os.path.join(save_dir, "fisher_matrix.npz"))
        fisher_matrix = [tf.constant(arr) for arr in fisher_data.values()]
        print(f"EWC assets loaded from {save_dir}")
        return model, fisher_matrix



    # ------ Classifier with HVAC features ------
    def build_meta_model(self, encoder: tf.keras.Model, num_classes: int = NUM_CLASSES):
        inp = layers.Input(shape=(SEQ_LEN, NUM_FEATS), name="meta_input")
        z_enc = encoder(inp)  # [B, FEATURE_DIM]

        # HVAC slice
        hvac = layers.Lambda(lambda z: z[:, :, 3:7], name="hvac_slice")(inp)  # [B,T,4]
        hvac_mean = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1), name="hvac_mean")(hvac)  # [B,4]

        # Toggle rate via abs(diff)
        hvac_shift = layers.Lambda(lambda z: z[:, 1:, :], name="hvac_shift")(hvac)  # [B,T-1,4]
        hvac_prev  = layers.Lambda(lambda z: z[:, :-1, :], name="hvac_prev")(hvac)  # [B,T-1,4]
        hvac_diff  = layers.Lambda(lambda t: tf.abs(t[0] - t[1]), name="hvac_diff")([hvac_shift, hvac_prev])  # [B,T-1,4]
        hvac_toggle_rate = layers.Lambda(lambda z: tf.reduce_mean(z, axis=1), name="hvac_toggle_rate")(hvac_diff)    # [B,4]

        hvac_feat = layers.Concatenate(name="hvac_concat")([hvac_mean, hvac_toggle_rate])  # [B,8]
        hvac_feat = layers.Dense(16, activation="relu", name="hvac_dense")(hvac_feat)

        x = layers.Concatenate(name="encoder_hvac_concat")([z_enc, hvac_feat])
        x = layers.Dense(64, activation="relu", name="meta_dense_64")(x)
        x = layers.Dense(32, activation="relu", name="meta_dense_32")(x)
        out = layers.Dense(num_classes, activation="softmax", name="meta_out")(x)

        return models.Model(inp, out, name="meta_lstm_classifier")

    # ------ Inner update (FOMAML) ------
    @staticmethod
    def inner_update(model: tf.keras.Model, X_support: np.ndarray, y_support: np.ndarray, lr_inner: float = INNER_LR):
        with tf.GradientTape() as tape:
            preds_support = model(X_support, training=True)
            loss_support = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_support, preds_support))
        grads_inner = tape.gradient(loss_support, model.trainable_variables)
        # Replace None grads if any
        grads_inner = [g if g is not None else tf.zeros_like(v) for g, v in zip(grads_inner, model.trainable_variables)]
        updated_vars = [w - lr_inner * g for w, g in zip(model.trainable_variables, grads_inner)]
        return updated_vars

    # ------ Fisher Matrix for EWC ------
    @staticmethod
    def compute_fisher_matrix0(model: tf.keras.Model, X: np.ndarray, y: np.ndarray, num_samples: int = 100):
        fisher = [tf.zeros_like(w) for w in model.trainable_variables]
        if len(X) == 0:
            return fisher
        idx = np.random.choice(len(X), min(num_samples, len(X)), replace=False)
        X_sample, y_sample = X[idx], y[idx]
        for x, true_label in zip(X_sample, y_sample):
            with tf.GradientTape() as tape:
                prob = model(np.expand_dims(x, axis=0))[0, true_label]
                log_prob = tf.math.log(tf.maximum(prob, 1e-8))
            grads = tape.gradient(log_prob, model.trainable_variables)
            grads = [g if g is not None else tf.zeros_like(v) for g, v in zip(grads, model.trainable_variables)]
            fisher = [f + tf.square(g) for f, g in zip(fisher, grads)]
        return [f / max(1, len(X_sample)) for f in fisher]

    # ------ Flowering period helpers ------
    @staticmethod
    def is_flowering_seq(x_seq: np.ndarray, light_idx: int = 2, th_light: float = 550.0) -> bool:
        light_mean = float(np.mean(x_seq[:, light_idx]))
        return light_mean >= th_light

    @staticmethod
    def hvac_toggle_score(x_seq: np.ndarray, hvac_slice=slice(3, 7), th_toggle: float = 0.15):
        hv = x_seq[:, hvac_slice]  # [T,4]
        if hv.shape[0] < 2:
            return 0.0, False
        diff = np.abs(hv[1:] - hv[:-1])   # [T-1,4]
        rate = float(diff.mean())
        return rate, rate >= th_toggle



    @staticmethod
    def compute_fisher_matrix(model, X, y, num_samples=200, batch_size=16):
        """
        ä»¥ EWC æ–¹å¼è¨ˆç®— Fisherï¼šå°çœŸå¯¦æ¨™ç±¤çš„ log-prob åšæ¢¯åº¦ï¼Œç´¯åŠ æ¢¯åº¦å¹³æ–¹çš„æœŸæœ›ã€‚
        å›å‚³ä¸€å€‹å¼µé‡åˆ—è¡¨ï¼Œå½¢ç‹€èˆ‡ model.trainable_variables å°æ‡‰ã€‚
        """
        # è‹¥æ²’æœ‰æ¨™è¨»è³‡æ–™ï¼Œå›å‚³å…¨é›¶
        if X is None or len(X) == 0 or y is None or len(y) == 0:
            return [tf.zeros_like(v) for v in model.trainable_variables]

        # å–æ¨£ä»¥åŠ é€Ÿ
        n = len(X)
        k = min(num_samples, n)
        idx = np.random.choice(n, k, replace=False)
        Xs = X[idx]
        ys = y[idx]

        ds = tf.data.Dataset.from_tensor_slices((Xs, ys)).batch(batch_size)
        fisher = [tf.zeros_like(v) for v in model.trainable_variables]
        count = 0

        # ç´¯åŠ æ¯å€‹ batch çš„æ¢¯åº¦å¹³æ–¹
        for xb, yb in ds:
            with tf.GradientTape() as tape:
                logits = model(xb, training=False)  # [B, C]
                log_prob = tf.nn.log_softmax(logits, axis=-1)
                # å–å‡ºçœŸå¯¦é¡åˆ¥çš„ log_probï¼Œç­‰åƒ¹æ–¼ sum(one_hot * log_prob)
                yb = tf.cast(yb, tf.int32)
                gathered = tf.gather(log_prob, yb, batch_dims=1)  # [B]
                loss = tf.reduce_mean(gathered)

            grads = tape.gradient(loss, model.trainable_variables)
            for i, g in enumerate(grads):
                if g is not None:
                    fisher[i] = fisher[i] + tf.square(g)
            count += 1

        # å–å¹³å‡
        fisher = [f / float(max(count, 1)) for f in fisher]
        return fisher

    def outer_update_with_lll(
            self,
            memory,
            meta_model,  # é€™è£¡å°±æ˜¯ self.model
            meta_optimizer,
            tasks,
            lr_inner=INNER_LR,
            replay_weight=REPLAY_WEIGHT,
            lambda_ewc=LAMBDA_EWC,
            prev_weights=None,  # ä¸Šä¸€è¼ªï¼ˆæˆ–ä¸Šä¸€ä»»å‹™ï¼‰åƒè€ƒæ¬Šé‡
            fisher_matrix=None  # èˆ‡ prev_weights å°æ‡‰çš„ Fisher
    ):
        """
        FOMAML outer-loop + replay + EWCï¼ˆä½¿ç”¨å¤–éƒ¨å‚³å…¥çš„ prev_weights èˆ‡ fisher_matrixï¼‰
        """
        # èšåˆæ‰€æœ‰ä»»å‹™çš„ meta æ¢¯åº¦
        meta_grads = [tf.zeros_like(v) for v in meta_model.trainable_variables]
        query_acc_list, query_loss_list = [], []

        for X_support, y_support, X_query, y_query in tasks:
            # ä¿å­˜åŸå§‹æ¬Šé‡
            orig_vars = [tf.identity(v) for v in meta_model.trainable_variables]

            # ------- inner updateï¼ˆå–®æ­¥ FOMAMLï¼‰-------
            with tf.GradientTape() as tape:
                preds_s = meta_model(X_support, training=True)
                loss_s = tf.reduce_mean(
                    tf.keras.losses.sparse_categorical_crossentropy(y_support, preds_s)
                )
            inner_grads = tape.gradient(loss_s, meta_model.trainable_variables)
            inner_grads = [g if g is not None else tf.zeros_like(v)
                           for g, v in zip(inner_grads, meta_model.trainable_variables)]
            updated_vars = [w - lr_inner * g for w, g in zip(meta_model.trainable_variables, inner_grads)]
            for var, upd in zip(meta_model.trainable_variables, updated_vars):
                var.assign(upd)

            # ------- query + replay + EWC -------
            with tf.GradientTape() as tape:
                preds_q = meta_model(X_query, training=True)
                loss_q = tf.reduce_mean(
                    tf.keras.losses.sparse_categorical_crossentropy(y_query, preds_q)
                )
                total_loss = loss_q

                # Replay
                if len(memory) >= 8:
                    X_old, y_old = memory.sample(32)
                    preds_old = meta_model(X_old, training=True)
                    replay_loss = tf.reduce_mean(
                        tf.keras.losses.sparse_categorical_crossentropy(y_old, preds_old)
                    )
                    total_loss = (1.0 - replay_weight) * total_loss + replay_weight * replay_loss

                # EWC
                if (prev_weights is not None) and (fisher_matrix is not None):
                    ewc = 0.0
                    for f, w, w_old in zip(fisher_matrix, meta_model.trainable_variables, prev_weights):
                        # ç¢ºä¿é¡å‹ä¸€è‡´ï¼ˆæŸäº›ç’°å¢ƒ fisher å¯èƒ½æ˜¯ numpyï¼‰
                        if not isinstance(f, tf.Tensor):
                            f = tf.convert_to_tensor(f, dtype=w.dtype)
                        ewc += tf.reduce_sum(f * tf.square(w - w_old))
                    total_loss = total_loss + lambda_ewc * ewc

            grads = tape.gradient(total_loss, meta_model.trainable_variables)
            grads = [g if g is not None else tf.zeros_like(v)
                     for g, v in zip(grads, meta_model.trainable_variables)]
            meta_grads = [mg + g / len(tasks) for mg, g in zip(meta_grads, grads)]

            # è¨˜éŒ„æŸ¥è©¢è¡¨ç¾
            q_acc = tf.reduce_mean(
                tf.cast(tf.equal(tf.argmax(preds_q, axis=1), y_query), tf.float32)
            )
            query_acc_list.append(float(q_acc.numpy()))
            query_loss_list.append(float(loss_q.numpy()))

            # é‚„åŸåˆ° inner update å‰çš„æ¬Šé‡ï¼ˆFOMAMLï¼‰
            for var, orig in zip(meta_model.trainable_variables, orig_vars):
                var.assign(orig)

            # æ›´æ–°è¨˜æ†¶é‡æ”¾
            memory.add(X_support, y_support)
            memory.add(X_query, y_query)

        # ------- åŸ·è¡Œ outer æ›´æ–° -------
        meta_optimizer.apply_gradients(zip(meta_grads, meta_model.trainable_variables))
        # å›å‚³æœ¬è¼ªå¹³å‡çš„ query loss/accï¼Œä»¥åŠä¸€ä»½ç•¶å‰æ¬Šé‡ï¼ˆå¯ä½œç‚ºä¸‹è¼ª prev_weightsï¼‰
        return float(np.mean(query_loss_list)), float(np.mean(query_acc_list)), [tf.identity(v) for v in
                                                                                 meta_model.trainable_variables]


    # ------ Trainable control ------
    @staticmethod
    def set_trainable_layers(encoder: tf.keras.Model, meta_model: tf.keras.Model, encoder_mode: str = "finetune", last_n: int = 1):
        # Encoder
        if encoder_mode == "finetune":
            for layer in encoder.layers:
                layer.trainable = True
        elif encoder_mode == "freeze":
            for layer in encoder.layers:
                layer.trainable = False
        elif encoder_mode == "last_n":
            for layer in encoder.layers:
                layer.trainable = False
            if last_n is not None:
                for layer in encoder.layers[-last_n:]:
                    layer.trainable = True

        # Classifier
        for layer in meta_model.layers:
            if layer.name.startswith("meta_dense") or layer.name.startswith("hvac_dense"):
                layer.trainable = True
            else:
                # keep encoder subgraph frozen according to encoder_mode
                if layer.name.startswith("lstm_encoder") or layer.name.startswith("encoder_"):
                    pass
                else:
                    layer.trainable = False

        print(f"âœ… Encoder mode: {encoder_mode}, last_n={last_n if encoder_mode=='last_n' else 'N/A'}")
        print("\nğŸ” [Encoder trainable layers]")
        for layer in encoder.layers:
            if layer.trainable_weights:
                print(f"{layer.name:<20} {'âœ… trainable' if layer.trainable else 'âŒ frozen'}")
        print("\nğŸ” [Meta model trainable layers]")
        for layer in meta_model.layers:
            if layer.trainable_weights:
                print(f"{layer.name:<20} {'âœ… trainable' if layer.trainable else 'âŒ frozen'}")

    # ------ Augment + contrastive pairs ------
    @staticmethod
    def augment_window(x: np.ndarray):
        x_aug = x.copy()
        x_aug[:, CONT_IDX] = x[:, CONT_IDX] + np.random.normal(0, 0.01, x[:, CONT_IDX].shape).astype(np.float32)
        return x_aug

    @classmethod
    def make_contrastive_pairs(cls, X: np.ndarray):
        anchors, positives = [], []
        for w in X:
            anchors.append(w)
            positives.append(cls.augment_window(w))
        return np.stack(anchors).astype(np.float32), np.stack(positives).astype(np.float32)

    # ------ Save / Load helpers ------
    @staticmethod
    def save_fisher_and_weights(model: tf.keras.Model, fisher_matrix: List[tf.Tensor], save_dir: str = "ewc_assets"):
        os.makedirs(save_dir, exist_ok=True)
        trainable_vars = model.trainable_variables
        weights = [v.numpy() for v in trainable_vars]
        fisher = [f.numpy() for f in fisher_matrix]
        #layer_shapes = [list(w.shape) for w in weights]
        #with open(os.path.join(save_dir, "layer_shapes.json"), "w") as f:
        #    f.write(json.dumps(layer_shapes))
        np.savez(os.path.join(save_dir, "ewc_assets.npz"), *weights, *fisher)
        print(f"âœ… Saved trainable weights + Fisher matrix to {save_dir} (arrays={len(weights) + len(fisher)})")

    @staticmethod
    def save_tflite(model, out_path):
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
        tflite_model = converter.convert()
        with open(out_path, "wb") as f: f.write(tflite_model)
        print("Saved TFLite:", out_path)



    # ------ Service (orchestrates the pipeline) ------
    def serv(self, encoder_mode: str, last_n: int, data_glob: str):
        global ENCODER_MODE, LAST_N, DATA_GLOB, NUM_FEATS
        ENCODER_MODE = encoder_mode
        LAST_N = last_n
        DATA_GLOB = data_glob

        # ===== Load data =====
        X_unlabeled, X_labeled, y_labeled, num_feats = build_csv_data(DATA_GLOB)
        NUM_FEATS = num_feats  # sync global for model input shapes

        # ===== Build encoder =====
        lstm_encoder = self.build_lstm_encoder(seq_len=SEQ_LEN, num_feats=NUM_FEATS, feature_dim=FEATURE_DIM)

        # ===== Contrastive pretraining =====
        contrastive_opt = tf.keras.optimizers.Adam()
        ntxent = NTXentLoss(temperature=0.2)
        anchors, positives = self.make_contrastive_pairs(X_unlabeled)
        contrast_ds = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(2048).batch(BATCH_SIZE)
        for ep in range(EPOCHS_CONTRASTIVE):
            for a, p in contrast_ds:
                with tf.GradientTape() as tape:
                    za = lstm_encoder(a, training=True)
                    zp = lstm_encoder(p, training=True)
                    loss = ntxent(za, zp)
                grads = tape.gradient(loss, lstm_encoder.trainable_variables)
                grads = [g if g is not None else tf.zeros_like(v) for g, v in zip(grads, lstm_encoder.trainable_variables)]
                contrastive_opt.apply_gradients(zip(grads, lstm_encoder.trainable_variables))
            print(f"[Contrastive] Epoch {ep + 1}/{EPOCHS_CONTRASTIVE}, loss={float(loss.numpy()):.4f}")

        # ===== Meta model =====
        meta_model = self.build_meta_model(lstm_encoder, NUM_CLASSES)
        meta_optimizer = tf.keras.optimizers.Adam(META_LR)
        self.set_trainable_layers(lstm_encoder, meta_model, ENCODER_MODE, LAST_N)

        memory = ReplayBuffer(capacity=REPLAY_CAPACITY)
        prev_weights = None
        fisher_matrix = None

        if X_labeled.size > 0:
            fisher_matrix = self.compute_fisher_matrix(meta_model, X_labeled, y_labeled)
            for ep in range(EPOCHS_META):
                tasks = sample_tasks(X_labeled, y_labeled)
                loss, acc, prev_weights = self.outer_update_with_lll(
                    memory=memory,
                    meta_model=meta_model,
                    meta_optimizer=meta_optimizer,
                    tasks=tasks,
                    lr_inner=INNER_LR,
                    prev_weights=prev_weights,
                    fisher_matrix=fisher_matrix,
                )
                print(f"[Meta ] Epoch {ep + 1}/{EPOCHS_META}, loss={loss:.4f}, acc={acc:.4f}")
        else:
            print("Skip meta-learning: no labeled data.")

        # ===== Save assets =====
        if fisher_matrix is not None:
            self.save_fisher_and_weights(model=meta_model, fisher_matrix=fisher_matrix)
        if X_labeled.size > 0:
            self.save_tflite(meta_model, "meta_lstm_classifier.tflite")
            print("meta_lstm_classifier.tflite Done.")

        # quick forward test
        dummy_x = np.random.rand(1, SEQ_LEN, NUM_FEATS).astype(np.float32)
        dummy_y = lstm_encoder(dummy_x)
        print("âœ… Forward test shape:", dummy_y.shape)



# ------------------------------------------------------------
# (Optional) Generate trainable tensor indices for ESP32 usage
# ------------------------------------------------------------

def make_indices(model_path: str = "meta_lstm_classifier.tflite", header_path: str = "trainable_tensor_indices.h"):
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()
    tensor_details = interpreter.get_tensor_details()
    trainable_indices = []
    for t in tensor_details:
        name, idx, shape = t['name'], t['index'], t['shape']
        if ("meta_dense" in name or "hvac_dense" in name):
            if "Relu" in name or ";" in name:
                continue
            if len(shape) in (1, 2):
                print(f"Trainable: {name}, index={idx}, shape={shape}")
                trainable_indices.append(idx)
    with open(header_path, "w") as f:
        f.write("#pragma once\n")
        f.write(f"const int trainable_tensor_indices[] = {{{', '.join(map(str, trainable_indices))}}};\n")
        f.write(f"const int trainable_tensor_count = {len(trainable_indices)};\n")
    print("trainable_tensor_indices =", trainable_indices)
    return trainable_indices




def build_csv_data(data_glob: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:
    """Load CSVs and build sliding windows.
    Returns: X_unlabeled, X_labeled, y_labeled, num_feats
    """
    X_labeled_list, y_labeled_list, X_unlabeled_list = [], [], []

    files = sorted(glob.glob(data_glob))
    if not files:
        print(f"âš ï¸ No CSV files matched: {data_glob}. Using random fallback for unlabeled pretraining.")

    for file in files:
        df = pd.read_csv(file).fillna(-1)
        data = df.values.astype(np.float32)
        feats, labels = data[:, :-1], data[:, -1]
        for i in range(len(data) - SEQ_LEN + 1):
            w_x = feats[i:i + SEQ_LEN]
            w_y = labels[i + SEQ_LEN - 1]
            if w_y == -1:
                X_unlabeled_list.append(w_x)
            else:
                X_labeled_list.append(w_x)
                y_labeled_list.append(int(w_y))

    X_unlabeled = np.array(X_unlabeled_list, dtype=np.float32) if len(X_unlabeled_list) > 0 else np.empty((0,), dtype=np.float32)

    if len(X_labeled_list) > 0:
        X_labeled = np.array(X_labeled_list, dtype=np.float32)
        y_labeled = np.array(y_labeled_list, dtype=np.int32)
    else:
        X_labeled = np.empty((0, SEQ_LEN, X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7), dtype=np.float32)
        y_labeled = np.empty((0,), dtype=np.int32)

    num_feats = X_labeled.shape[2] if X_labeled.size > 0 else (X_unlabeled.shape[2] if X_unlabeled.size > 0 else 7)

    if num_feats < 7:
        raise ValueError(
            "Expected at least 7 features per timestep: [temp, humid, light, ac, heater, dehum, hum]. Found: %d" % num_feats
        )

    if X_unlabeled.size == 0:
        # provide unlabeled fallback if none
        X_unlabeled = np.random.randn(200, SEQ_LEN, num_feats).astype(np.float32)

    return X_unlabeled, X_labeled, y_labeled, num_feats


def sample_tasks(X: np.ndarray, y: np.ndarray, num_tasks: int = NUM_TASKS,
                 support_size: int = SUPPORT_SIZE, query_size: int = QUERY_SIZE):
    tasks = []
    n = len(X)
    if n < support_size + query_size:
        raise ValueError(f"Not enough labeled samples to build tasks: need {support_size+query_size}, got {n}")
    for _ in range(num_tasks):
        idx = np.random.choice(n, support_size + query_size, replace=False)
        X_support, y_support = X[idx[:support_size]], y[idx[:support_size]]
        X_query, y_query = X[idx[support_size:]], y[idx[support_size:]]
        tasks.append((X_support, y_support, X_query, y_query))
    return tasks

