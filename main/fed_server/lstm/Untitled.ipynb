{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cada91b-d27d-40b1-b335-a665f8ea4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "X shape: (30, 128, 3)\n",
      "Model: \"lstm_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sensor_seq (InputLayer)     [(None, 128, 3)]          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                17408     \n",
      "                                                                 \n",
      " feature (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21568 (84.25 KB)\n",
      "Trainable params: 21568 (84.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1082, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1046, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 138\u001b[0m\n\u001b[1;32m    134\u001b[0m         save_tflite(encoder, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_encoder_int8.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    135\u001b[0m                     quant_int8\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rep_data\u001b[38;5;241m=\u001b[39mrep_data)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 123\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# 占位训练（可换成对比学习）\u001b[39;00m\n\u001b[1;32m    122\u001b[0m encoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(LR), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 123\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# 保存 Keras 模型\u001b[39;00m\n\u001b[1;32m    126\u001b[0m encoder\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_encoder.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebbgn6hy9.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1082, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/opt/conda/envs/myenv/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1046, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# ========= 可调参数 =========\n",
    "SEQ_LEN     = 128\n",
    "NUM_FEATS   = 3\n",
    "LSTM_UNITS  = 64\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE  = 64\n",
    "EPOCHS      = 5\n",
    "LR          = 1e-3\n",
    "\n",
    "DATA_GLOB   = \"./data/*.csv\"\n",
    "SAVE_DIR    = \"./lstm_sensor_out\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ========= 数据处理 =========\n",
    "def load_csvs(glob_pattern):\n",
    "    files = sorted(glob.glob(glob_pattern))\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        X = df[[\"temp\",\"humid\",\"light\"]].values.astype(np.float32)\n",
    "        dfs.append(X)\n",
    "    return dfs\n",
    "\n",
    "def zscore_norm(x, mean=None, std=None, eps=1e-6):\n",
    "    if mean is None:\n",
    "        mean = x.mean(axis=0, keepdims=True)\n",
    "    if std is None:\n",
    "        std = x.std(axis=0, keepdims=True)\n",
    "    std = np.maximum(std, eps)\n",
    "    return (x - mean) / std, mean, std\n",
    "\n",
    "def make_windows(X, seq_len=SEQ_LEN, stride=None):\n",
    "    if stride is None:\n",
    "        stride = seq_len // 2\n",
    "    xs = []\n",
    "    n = len(X)\n",
    "    for start in range(0, n - seq_len + 1, stride):\n",
    "        end = start + seq_len\n",
    "        xs.append(X[start:end])\n",
    "    return np.stack(xs, axis=0).astype(np.float32)\n",
    "\n",
    "def build_dataset(glob_pattern):\n",
    "    all_X = []\n",
    "    for X in load_csvs(glob_pattern):\n",
    "        # 缺失值前向填充 + 均值填充\n",
    "        if np.isnan(X).any():\n",
    "            for c in range(X.shape[1]):\n",
    "                col = X[:, c]\n",
    "                idx = np.where(np.isnan(col))[0]\n",
    "                for i in idx:\n",
    "                    col[i] = col[i-1] if i>0 else np.nan\n",
    "                if np.isnan(col).any():\n",
    "                    col[np.isnan(col)] = np.nanmean(col)\n",
    "                X[:, c] = col\n",
    "        Xn, _, _ = zscore_norm(X)\n",
    "        xs = make_windows(Xn)\n",
    "        all_X.append(xs)\n",
    "    return np.concatenate(all_X, axis=0)\n",
    "\n",
    "# ========= LSTM 编码器 =========\n",
    "def build_lstm_encoder(num_feats=NUM_FEATS, seq_len=SEQ_LEN,\n",
    "                       lstm_units=LSTM_UNITS, feature_dim=FEATURE_DIM):\n",
    "    inp = tf.keras.Input(shape=(seq_len, num_feats), name=\"sensor_seq\")\n",
    "    x = tf.keras.layers.LSTM(\n",
    "        units=lstm_units,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        return_sequences=False,\n",
    "        use_bias=True,\n",
    "        name=\"lstm\"\n",
    "    )(inp)\n",
    "    feat = tf.keras.layers.Dense(feature_dim, activation=None, name=\"feature\")(x)\n",
    "    return tf.keras.Model(inp, feat, name=\"lstm_encoder\")\n",
    "\n",
    "# ========= TFLite 导出 =========\n",
    "def save_tflite(keras_model, out_path, quant_int8=False, rep_data=None):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    # FP32/INT8 都加入 SELECT_TF_OPS 解决 LSTM TensorListReserve\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "    if quant_int8:\n",
    "        assert rep_data is not None\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        def rep_dataset():\n",
    "            for i in range(min(200, len(rep_data))):\n",
    "                yield [rep_data[i:i+1]]\n",
    "        converter.representative_dataset = rep_dataset\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved:\", out_path, \" size:\", os.path.getsize(out_path)/1024, \"KB\")\n",
    "\n",
    "# ========= 主流程 =========\n",
    "def main():\n",
    "    print(\"Loading data ...\")\n",
    "    X = build_dataset(DATA_GLOB)\n",
    "    print(\"X shape:\", X.shape)\n",
    "\n",
    "    # 训练/验证划分（无监督模式全部训练）\n",
    "    X_train, X_val = X, X[:0]\n",
    "\n",
    "    # 构建编码器\n",
    "    encoder = build_lstm_encoder()\n",
    "    encoder.summary()\n",
    "\n",
    "    # 占位训练（可换成对比学习）\n",
    "    encoder.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=None)\n",
    "    encoder.fit(X_train, X_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # 保存 Keras 模型\n",
    "    encoder.save(os.path.join(SAVE_DIR, \"lstm_encoder.h5\"))\n",
    "\n",
    "    # 导出 FP32\n",
    "    save_tflite(encoder, os.path.join(SAVE_DIR, \"lstm_encoder_fp32.tflite\"))\n",
    "\n",
    "    # 导出 Int8\n",
    "    rep_data = X[:256]\n",
    "    if len(rep_data) > 0:\n",
    "        save_tflite(encoder, os.path.join(SAVE_DIR, \"lstm_encoder_int8.tflite\"),\n",
    "                    quant_int8=True, rep_data=rep_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77de338f-781d-4654-b576-1756dc1dc38f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_unlabeled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(anchors), np\u001b[38;5;241m.\u001b[39mstack(positives)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 假设 X_unlabeled 是未标注的传感器序列\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m anchors, positives \u001b[38;5;241m=\u001b[39m make_contrastive_pairs(\u001b[43mX_unlabeled\u001b[49m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 对比学习损失（简单 InfoNCE）\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mContrastiveLoss\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mLoss):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_unlabeled' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 假设你已有 LSTM encoder\n",
    "encoder = build_lstm_encoder(seq_len=SEQ_LEN, num_feats=NUM_FEATS, lstm_units=LSTM_UNITS, feature_dim=FEATURE_DIM)\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ 无监督对比学习预训练\n",
    "# -----------------------------\n",
    "def augment_window(x):\n",
    "    \"\"\"简单增强示例：加噪声\"\"\"\n",
    "    noise = np.random.normal(0, 0.01, size=x.shape)\n",
    "    return x + noise\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for x in X:\n",
    "        a = x\n",
    "        p = augment_window(x)\n",
    "        anchors.append(a)\n",
    "        positives.append(p)\n",
    "    return np.stack(anchors), np.stack(positives)\n",
    "\n",
    "# 假设 X_unlabeled 是未标注的传感器序列\n",
    "anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "\n",
    "# 对比学习损失（简单 InfoNCE）\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, z1, z2):\n",
    "        z1 = tf.math.l2_normalize(z1, axis=1)\n",
    "        z2 = tf.math.l2_normalize(z2, axis=1)\n",
    "        logits = tf.matmul(z1, z2, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(logits)[0])\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z1 = encoder(anchors, training=True)\n",
    "        z2 = encoder(positives, training=True)\n",
    "        loss = ContrastiveLoss()(z1, z2)\n",
    "    grads = tape.gradient(loss, encoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, encoder.trainable_variables))\n",
    "    print(f\"Epoch {epoch} contrastive loss: {loss.numpy():.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ 有监督三类分类训练\n",
    "# -----------------------------\n",
    "NUM_CLASSES = 3\n",
    "# 构建分类头\n",
    "inputs = tf.keras.Input(shape=(SEQ_LEN, NUM_FEATS))\n",
    "features = encoder(inputs, training=False)  # 冻结 encoder\n",
    "logits = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(features)\n",
    "clf_model = tf.keras.Model(inputs, logits)\n",
    "\n",
    "clf_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 假设 X_labeled, y_labeled 是有标签数据（健康/不健康/非植物）\n",
    "clf_model.fit(X_labeled, y_labeled, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 训练完成后可保存 encoder 和分类头\n",
    "encoder.save(\"lstm_encoder_pretrained.h5\")\n",
    "clf_model.save(\"lstm_encoder_with_head.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1df2f9-f542-4086-939a-f14f4b7b6532",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_labeled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(anchors), np\u001b[38;5;241m.\u001b[39mstack(positives)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 2. LSTM 编码器\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m SEQ_LEN \u001b[38;5;241m=\u001b[39m \u001b[43mX_labeled\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     25\u001b[0m NUM_FEATS \u001b[38;5;241m=\u001b[39m X_labeled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     26\u001b[0m FEATURE_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_labeled' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ----------------------\n",
    "# 1. 数据增强示例\n",
    "# ----------------------\n",
    "def augment_window(x, noise_level=0.01):\n",
    "    \"\"\"简单数据增强：加噪声\"\"\"\n",
    "    return x + noise_level * np.random.randn(*x.shape)\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for x in X:\n",
    "        a = x\n",
    "        p = augment_window(x)\n",
    "        anchors.append(a)\n",
    "        positives.append(p)\n",
    "    return np.stack(anchors), np.stack(positives)\n",
    "\n",
    "# ----------------------\n",
    "# 2. LSTM 编码器\n",
    "# ----------------------\n",
    "SEQ_LEN = X_labeled.shape[1]\n",
    "NUM_FEATS = X_labeled.shape[2]\n",
    "FEATURE_DIM = 64\n",
    "\n",
    "lstm_encoder = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, NUM_FEATS)),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(FEATURE_DIM)  # 输出 feature_dim\n",
    "])\n",
    "\n",
    "# ----------------------\n",
    "# 3. 对比学习训练\n",
    "# ----------------------\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, z_a, z_p):\n",
    "        # L2 归一化\n",
    "        z_a = tf.math.l2_normalize(z_a, axis=1)\n",
    "        z_p = tf.math.l2_normalize(z_p, axis=1)\n",
    "        logits = tf.matmul(z_a, z_p, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_a)[0])\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))\n",
    "\n",
    "anchors, positives = make_contrastive_pairs(X_labeled)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "# 简单训练循环\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(1024).batch(BATCH_SIZE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for a_batch, p_batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_a = lstm_encoder(a_batch, training=True)\n",
    "            z_p = lstm_encoder(p_batch, training=True)\n",
    "            loss = ContrastiveLoss()(z_a, z_p)\n",
    "        grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "    print(f\"Epoch {epoch+1}, contrastive loss: {loss.numpy():.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# 4. 特征提取\n",
    "# ----------------------\n",
    "features = lstm_encoder.predict(X_labeled)\n",
    "\n",
    "# ----------------------\n",
    "# 5. 分类头训练\n",
    "# ----------------------\n",
    "y_labeled = tf.keras.utils.to_categorical(y_labeled, num_classes=3)\n",
    "classifier_input = layers.Input(shape=(FEATURE_DIM,))\n",
    "classifier_output = layers.Dense(3, activation='softmax')(classifier_input)\n",
    "classifier_model = models.Model(classifier_input, classifier_output)\n",
    "\n",
    "classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_model.fit(features, y_labeled, batch_size=32, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f11632ec-8e81-4a26-af31-438c328d1441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有标签样本: (21850, 64, 3)\n",
      "无标签样本: (0,)\n",
      "没有无标签数据，生成随机数据用于对比学习\n",
      "Epoch 1/10, loss=0.0444\n",
      "Epoch 2/10, loss=0.0828\n",
      "Epoch 3/10, loss=0.1132\n",
      "Epoch 4/10, loss=0.0061\n",
      "Epoch 5/10, loss=0.0170\n",
      "Epoch 6/10, loss=0.0033\n",
      "Epoch 7/10, loss=0.0030\n",
      "Epoch 8/10, loss=0.0056\n",
      "Epoch 9/10, loss=0.0038\n",
      "Epoch 10/10, loss=0.0009\n",
      "683/683 [==============================] - 14s 21ms/step\n",
      "Epoch 1/20\n",
      "547/547 [==============================] - 3s 4ms/step - loss: 0.6830 - accuracy: 0.5771 - val_loss: 0.6396 - val_accuracy: 0.5666\n",
      "Epoch 2/20\n",
      "547/547 [==============================] - 2s 3ms/step - loss: 0.5944 - accuracy: 0.7058 - val_loss: 0.5368 - val_accuracy: 0.9078\n",
      "Epoch 3/20\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.4733 - accuracy: 0.8989 - val_loss: 0.4209 - val_accuracy: 0.8723\n",
      "Epoch 4/20\n",
      "547/547 [==============================] - 2s 3ms/step - loss: 0.3532 - accuracy: 0.9527 - val_loss: 0.3038 - val_accuracy: 0.9506\n",
      "Epoch 5/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.2706 - accuracy: 0.9523 - val_loss: 0.2410 - val_accuracy: 0.9432\n",
      "Epoch 6/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.2200 - accuracy: 0.9506 - val_loss: 0.1980 - val_accuracy: 0.9561\n",
      "Epoch 7/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9551 - val_loss: 0.1729 - val_accuracy: 0.9641\n",
      "Epoch 8/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1647 - accuracy: 0.9550 - val_loss: 0.1524 - val_accuracy: 0.9568\n",
      "Epoch 9/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1478 - accuracy: 0.9569 - val_loss: 0.1387 - val_accuracy: 0.9609\n",
      "Epoch 10/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1362 - accuracy: 0.9580 - val_loss: 0.1288 - val_accuracy: 0.9570\n",
      "Epoch 11/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9590 - val_loss: 0.1277 - val_accuracy: 0.9595\n",
      "Epoch 12/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1205 - accuracy: 0.9590 - val_loss: 0.1167 - val_accuracy: 0.9551\n",
      "Epoch 13/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1140 - accuracy: 0.9589 - val_loss: 0.1095 - val_accuracy: 0.9584\n",
      "Epoch 14/20\n",
      "547/547 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.1051 - val_accuracy: 0.9664\n",
      "Epoch 15/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1065 - accuracy: 0.9606 - val_loss: 0.1061 - val_accuracy: 0.9551\n",
      "Epoch 16/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1025 - accuracy: 0.9621 - val_loss: 0.1020 - val_accuracy: 0.9618\n",
      "Epoch 17/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1008 - accuracy: 0.9602 - val_loss: 0.0966 - val_accuracy: 0.9648\n",
      "Epoch 18/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.0966 - accuracy: 0.9619 - val_loss: 0.0992 - val_accuracy: 0.9600\n",
      "Epoch 19/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.0962 - accuracy: 0.9621 - val_loss: 0.0948 - val_accuracy: 0.9590\n",
      "Epoch 20/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.0939 - accuracy: 0.9625 - val_loss: 0.0902 - val_accuracy: 0.9657\n",
      "INFO:tensorflow:Assets written to: /tmp/tmptsmczowc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptsmczowc/assets\n",
      "2025-08-16 13:19:09.670200: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-16 13:19:09.670258: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-16 13:19:09.670433: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmptsmczowc\n",
      "2025-08-16 13:19:09.676125: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-16 13:19:09.676156: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmptsmczowc\n",
      "2025-08-16 13:19:09.693957: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-16 13:19:09.729499: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmptsmczowc\n",
      "2025-08-16 13:19:09.757406: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 86972 microseconds.\n",
      "2025-08-16 13:19:09.862035: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<1x?x64xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite model: lstm_encoder.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp2a9orrqm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2a9orrqm/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite model: classifier.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 13:19:10.206821: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-16 13:19:10.206906: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-16 13:19:10.207066: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp2a9orrqm\n",
      "2025-08-16 13:19:10.207474: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-16 13:19:10.207483: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp2a9orrqm\n",
      "2025-08-16 13:19:10.208812: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-16 13:19:10.229718: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp2a9orrqm\n",
      "2025-08-16 13:19:10.236348: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 29282 microseconds.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# -----------------------\n",
    "# 参数\n",
    "# -----------------------\n",
    "DATA_GLOB = \"./data/*.csv\"  # 数据路径\n",
    "SEQ_LEN = 64\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_CONTRASTIVE = 10\n",
    "EPOCHS_CLASSIFIER = 20\n",
    "\n",
    "# -----------------------\n",
    "# 1. 加载 CSV 数据\n",
    "# -----------------------\n",
    "X_labeled_list, y_labeled_list = [], []\n",
    "X_unlabeled_list = []\n",
    "\n",
    "for file in glob.glob(DATA_GLOB):\n",
    "    df = pd.read_csv(file).fillna(-1)  # NaN 当作无标签\n",
    "    data = df.values.astype(np.float32)\n",
    "    \n",
    "    for i in range(len(data) - SEQ_LEN + 1):\n",
    "        window = data[i:i+SEQ_LEN, :-1]\n",
    "        label = data[i+SEQ_LEN-1, -1]\n",
    "        if label == -1:  # 无标签\n",
    "            X_unlabeled_list.append(window)\n",
    "        else:           # 有标签\n",
    "            X_labeled_list.append(window)\n",
    "            y_labeled_list.append(int(label))\n",
    "\n",
    "X_labeled = np.array(X_labeled_list)\n",
    "y_labeled = np.array(y_labeled_list)\n",
    "X_unlabeled = np.array(X_unlabeled_list)\n",
    "\n",
    "print(\"有标签样本:\", X_labeled.shape)\n",
    "print(\"无标签样本:\", X_unlabeled.shape)\n",
    "\n",
    "# -----------------------\n",
    "# 2. 对比学习辅助函数\n",
    "# -----------------------\n",
    "def augment_window(x):\n",
    "    return x + np.random.normal(0, 0.01, x.shape)\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for x in X:\n",
    "        anchors.append(x)\n",
    "        positives.append(augment_window(x))\n",
    "    return np.stack(anchors), np.stack(positives)\n",
    "\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, z_i, z_j):\n",
    "        z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "        z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "        logits = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_i)[0])\n",
    "        loss_i = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_j = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        return tf.reduce_mean(loss_i + loss_j)\n",
    "\n",
    "# -----------------------\n",
    "# 3. LSTM 编码器\n",
    "# -----------------------\n",
    "NUM_FEATS = X_labeled.shape[2] if len(X_labeled) > 0 else 10  # 没有有标签时默认10\n",
    "lstm_encoder = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, NUM_FEATS)),\n",
    "    layers.LSTM(FEATURE_DIM, return_sequences=False),\n",
    "    layers.Dense(FEATURE_DIM, activation='relu')\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# 4. 对比学习训练（可选）\n",
    "# -----------------------\n",
    "if len(X_unlabeled) == 0:\n",
    "    print(\"没有无标签数据，生成随机数据用于对比学习\")\n",
    "    X_unlabeled = np.random.randn(100, SEQ_LEN, NUM_FEATS).astype(np.float32)\n",
    "\n",
    "anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(1024).batch(BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(EPOCHS_CONTRASTIVE):\n",
    "    for a, p in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_a = lstm_encoder(a, training=True)\n",
    "            z_p = lstm_encoder(p, training=True)\n",
    "            loss = ContrastiveLoss()(z_a, z_p)\n",
    "        grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_CONTRASTIVE}, loss={loss.numpy():.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 5. 有监督特征 + 分类头训练\n",
    "# -----------------------\n",
    "if len(X_labeled) > 0:\n",
    "    features_labeled = lstm_encoder.predict(X_labeled)\n",
    "    classifier = models.Sequential([\n",
    "        layers.Input(shape=(FEATURE_DIM,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    classifier.compile(optimizer='adam',\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "    classifier.fit(features_labeled, y_labeled,\n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   epochs=EPOCHS_CLASSIFIER,\n",
    "                   validation_split=0.2)\n",
    "\n",
    "# -----------------------\n",
    "# 6. TFLite 导出\n",
    "# -----------------------\n",
    "def save_tflite(model, out_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                           tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved TFLite model:\", out_path)\n",
    "\n",
    "save_tflite(lstm_encoder, \"lstm_encoder.tflite\")\n",
    "if len(X_labeled) > 0:\n",
    "    save_tflite(classifier, \"classifier.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e22c65-0a2d-49d5-bef4-e70b07751c2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./data/sensor_data_0.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_1.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_2.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_3.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_4.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_5.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_6.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_7.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_8.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_9.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_10.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_11.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_12.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_13.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_14.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_15.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_16.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_17.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_18.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_19.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_20.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_21.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_22.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_23.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_24.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_25.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_26.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_27.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_28.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_29.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_30.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_31.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_32.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_33.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_34.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_35.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_36.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_37.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_38.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_39.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_40.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_41.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_42.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_43.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_44.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_45.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_46.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_47.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_48.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_49.csv, shape: (500, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==== 参数 ====\n",
    "SAVE_DIR = \"./data\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "NUM_FILES = 50       # 生成多少个 CSV 文件\n",
    "SEQ_LEN = 500       # 每个文件的长度\n",
    "NUM_CLASSES = 3     # 分类标签数量，0/1\n",
    "NOISE_STD = 0.05    # 模拟噪声大小\n",
    "\n",
    "# ==== 随机生成传感器数据 ====\n",
    "for i in range(NUM_FILES):\n",
    "    # 模拟温度、湿度、光照\n",
    "    t = 20 + 5 * np.sin(np.linspace(0, 10, SEQ_LEN)) + np.random.randn(SEQ_LEN) * NOISE_STD\n",
    "    h = 50 + 10 * np.cos(np.linspace(0, 5, SEQ_LEN)) + np.random.randn(SEQ_LEN) * NOISE_STD\n",
    "    l = 300 + 50 * np.sin(np.linspace(0, 3, SEQ_LEN)) + np.random.randn(SEQ_LEN) * NOISE_STD\n",
    "\n",
    "    # 简单生成标签：假设 temp > 22 就标 1，否则 0（仅作示例）\n",
    "    label = (t > 22).astype(int)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"temp\": t,\n",
    "        \"humid\": h,\n",
    "        \"light\": l,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    file_path = os.path.join(SAVE_DIR, f\"sensor_data_{i}.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}, shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc97df-3931-48bb-8599-5c5e8aa138fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
