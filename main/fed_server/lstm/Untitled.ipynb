{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cada91b-d27d-40b1-b335-a665f8ea4991",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ========= 可调参数 =========\u001b[39;00m\n\u001b[1;32m     10\u001b[0m SEQ_LEN     \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# ========= 可调参数 =========\n",
    "SEQ_LEN     = 128\n",
    "NUM_FEATS   = 3\n",
    "LSTM_UNITS  = 64\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE  = 64\n",
    "EPOCHS      = 5\n",
    "LR          = 1e-3\n",
    "\n",
    "DATA_GLOB   = \"./data/*.csv\"\n",
    "SAVE_DIR    = \"./lstm_sensor_out\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ========= 数据处理 =========\n",
    "def load_csvs(glob_pattern):\n",
    "    files = sorted(glob.glob(glob_pattern))\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        X = df[[\"temp\",\"humid\",\"light\"]].values.astype(np.float32)\n",
    "        dfs.append(X)\n",
    "    return dfs\n",
    "\n",
    "def zscore_norm(x, mean=None, std=None, eps=1e-6):\n",
    "    if mean is None:\n",
    "        mean = x.mean(axis=0, keepdims=True)\n",
    "    if std is None:\n",
    "        std = x.std(axis=0, keepdims=True)\n",
    "    std = np.maximum(std, eps)\n",
    "    return (x - mean) / std, mean, std\n",
    "\n",
    "def make_windows(X, seq_len=SEQ_LEN, stride=None):\n",
    "    if stride is None:\n",
    "        stride = seq_len // 2\n",
    "    xs = []\n",
    "    n = len(X)\n",
    "    for start in range(0, n - seq_len + 1, stride):\n",
    "        end = start + seq_len\n",
    "        xs.append(X[start:end])\n",
    "    return np.stack(xs, axis=0).astype(np.float32)\n",
    "\n",
    "def build_dataset(glob_pattern):\n",
    "    all_X = []\n",
    "    for X in load_csvs(glob_pattern):\n",
    "        # 缺失值前向填充 + 均值填充\n",
    "        if np.isnan(X).any():\n",
    "            for c in range(X.shape[1]):\n",
    "                col = X[:, c]\n",
    "                idx = np.where(np.isnan(col))[0]\n",
    "                for i in idx:\n",
    "                    col[i] = col[i-1] if i>0 else np.nan\n",
    "                if np.isnan(col).any():\n",
    "                    col[np.isnan(col)] = np.nanmean(col)\n",
    "                X[:, c] = col\n",
    "        Xn, _, _ = zscore_norm(X)\n",
    "        xs = make_windows(Xn)\n",
    "        all_X.append(xs)\n",
    "    return np.concatenate(all_X, axis=0)\n",
    "\n",
    "# ========= LSTM 编码器 =========\n",
    "def build_lstm_encoder(num_feats=NUM_FEATS, seq_len=SEQ_LEN,\n",
    "                       lstm_units=LSTM_UNITS, feature_dim=FEATURE_DIM):\n",
    "    inp = tf.keras.Input(shape=(seq_len, num_feats), name=\"sensor_seq\")\n",
    "    x = tf.keras.layers.LSTM(\n",
    "        units=lstm_units,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        return_sequences=False,\n",
    "        use_bias=True,\n",
    "        name=\"lstm\"\n",
    "    )(inp)\n",
    "    feat = tf.keras.layers.Dense(feature_dim, activation=None, name=\"feature\")(x)\n",
    "    return tf.keras.Model(inp, feat, name=\"lstm_encoder\")\n",
    "\n",
    "# ========= TFLite 导出 =========\n",
    "def save_tflite(keras_model, out_path, quant_int8=False, rep_data=None):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    # FP32/INT8 都加入 SELECT_TF_OPS 解决 LSTM TensorListReserve\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "    if quant_int8:\n",
    "        assert rep_data is not None\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        def rep_dataset():\n",
    "            for i in range(min(200, len(rep_data))):\n",
    "                yield [rep_data[i:i+1]]\n",
    "        converter.representative_dataset = rep_dataset\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved:\", out_path, \" size:\", os.path.getsize(out_path)/1024, \"KB\")\n",
    "\n",
    "# ========= 主流程 =========\n",
    "def main():\n",
    "    print(\"Loading data ...\")\n",
    "    X = build_dataset(DATA_GLOB)\n",
    "    print(\"X shape:\", X.shape)\n",
    "\n",
    "    # 训练/验证划分（无监督模式全部训练）\n",
    "    X_train, X_val = X, X[:0]\n",
    "\n",
    "    # 构建编码器\n",
    "    encoder = build_lstm_encoder()\n",
    "    encoder.summary()\n",
    "\n",
    "    # 占位训练（可换成对比学习）\n",
    "    encoder.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=None)\n",
    "    encoder.fit(X_train, X_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # 保存 Keras 模型\n",
    "    encoder.save(os.path.join(SAVE_DIR, \"lstm_encoder.h5\"))\n",
    "\n",
    "    # 导出 FP32\n",
    "    save_tflite(encoder, os.path.join(SAVE_DIR, \"lstm_encoder_fp32.tflite\"))\n",
    "\n",
    "    # 导出 Int8\n",
    "    rep_data = X[:256]\n",
    "    if len(rep_data) > 0:\n",
    "        save_tflite(encoder, os.path.join(SAVE_DIR, \"lstm_encoder_int8.tflite\"),\n",
    "                    quant_int8=True, rep_data=rep_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77de338f-781d-4654-b576-1756dc1dc38f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_unlabeled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(anchors), np\u001b[38;5;241m.\u001b[39mstack(positives)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 假设 X_unlabeled 是未标注的传感器序列\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m anchors, positives \u001b[38;5;241m=\u001b[39m make_contrastive_pairs(\u001b[43mX_unlabeled\u001b[49m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 对比学习损失（简单 InfoNCE）\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mContrastiveLoss\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mLoss):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_unlabeled' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 假设你已有 LSTM encoder\n",
    "encoder = build_lstm_encoder(seq_len=SEQ_LEN, num_feats=NUM_FEATS, lstm_units=LSTM_UNITS, feature_dim=FEATURE_DIM)\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ 无监督对比学习预训练\n",
    "# -----------------------------\n",
    "def augment_window(x):\n",
    "    \"\"\"简单增强示例：加噪声\"\"\"\n",
    "    noise = np.random.normal(0, 0.01, size=x.shape)\n",
    "    return x + noise\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for x in X:\n",
    "        a = x\n",
    "        p = augment_window(x)\n",
    "        anchors.append(a)\n",
    "        positives.append(p)\n",
    "    return np.stack(anchors), np.stack(positives)\n",
    "\n",
    "# 假设 X_unlabeled 是未标注的传感器序列\n",
    "anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "\n",
    "# 对比学习损失（简单 InfoNCE）\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, z1, z2):\n",
    "        z1 = tf.math.l2_normalize(z1, axis=1)\n",
    "        z2 = tf.math.l2_normalize(z2, axis=1)\n",
    "        logits = tf.matmul(z1, z2, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(logits)[0])\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z1 = encoder(anchors, training=True)\n",
    "        z2 = encoder(positives, training=True)\n",
    "        loss = ContrastiveLoss()(z1, z2)\n",
    "    grads = tape.gradient(loss, encoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, encoder.trainable_variables))\n",
    "    print(f\"Epoch {epoch} contrastive loss: {loss.numpy():.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ 有监督三类分类训练\n",
    "# -----------------------------\n",
    "NUM_CLASSES = 3\n",
    "# 构建分类头\n",
    "inputs = tf.keras.Input(shape=(SEQ_LEN, NUM_FEATS))\n",
    "features = encoder(inputs, training=False)  # 冻结 encoder\n",
    "logits = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(features)\n",
    "clf_model = tf.keras.Model(inputs, logits)\n",
    "\n",
    "clf_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 假设 X_labeled, y_labeled 是有标签数据（健康/不健康/非植物）\n",
    "clf_model.fit(X_labeled, y_labeled, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 训练完成后可保存 encoder 和分类头\n",
    "encoder.save(\"lstm_encoder_pretrained.h5\")\n",
    "clf_model.save(\"lstm_encoder_with_head.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1df2f9-f542-4086-939a-f14f4b7b6532",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_labeled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(anchors), np\u001b[38;5;241m.\u001b[39mstack(positives)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 2. LSTM 编码器\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m SEQ_LEN \u001b[38;5;241m=\u001b[39m \u001b[43mX_labeled\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     25\u001b[0m NUM_FEATS \u001b[38;5;241m=\u001b[39m X_labeled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     26\u001b[0m FEATURE_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_labeled' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ----------------------\n",
    "# 1. 数据增强示例\n",
    "# ----------------------\n",
    "def augment_window(x, noise_level=0.01):\n",
    "    \"\"\"简单数据增强：加噪声\"\"\"\n",
    "    return x + noise_level * np.random.randn(*x.shape)\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for x in X:\n",
    "        a = x\n",
    "        p = augment_window(x)\n",
    "        anchors.append(a)\n",
    "        positives.append(p)\n",
    "    return np.stack(anchors), np.stack(positives)\n",
    "\n",
    "# ----------------------\n",
    "# 2. LSTM 编码器\n",
    "# ----------------------\n",
    "SEQ_LEN = X_labeled.shape[1]\n",
    "NUM_FEATS = X_labeled.shape[2]\n",
    "FEATURE_DIM = 64\n",
    "\n",
    "lstm_encoder = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, NUM_FEATS)),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(FEATURE_DIM)  # 输出 feature_dim\n",
    "])\n",
    "\n",
    "# ----------------------\n",
    "# 3. 对比学习训练\n",
    "# ----------------------\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, z_a, z_p):\n",
    "        # L2 归一化\n",
    "        z_a = tf.math.l2_normalize(z_a, axis=1)\n",
    "        z_p = tf.math.l2_normalize(z_p, axis=1)\n",
    "        logits = tf.matmul(z_a, z_p, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_a)[0])\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))\n",
    "\n",
    "anchors, positives = make_contrastive_pairs(X_labeled)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "# 简单训练循环\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(1024).batch(BATCH_SIZE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for a_batch, p_batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_a = lstm_encoder(a_batch, training=True)\n",
    "            z_p = lstm_encoder(p_batch, training=True)\n",
    "            loss = ContrastiveLoss()(z_a, z_p)\n",
    "        grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "    print(f\"Epoch {epoch+1}, contrastive loss: {loss.numpy():.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# 4. 特征提取\n",
    "# ----------------------\n",
    "features = lstm_encoder.predict(X_labeled)\n",
    "\n",
    "# ----------------------\n",
    "# 5. 分类头训练\n",
    "# ----------------------\n",
    "y_labeled = tf.keras.utils.to_categorical(y_labeled, num_classes=3)\n",
    "classifier_input = layers.Input(shape=(FEATURE_DIM,))\n",
    "classifier_output = layers.Dense(3, activation='softmax')(classifier_input)\n",
    "classifier_model = models.Model(classifier_input, classifier_output)\n",
    "\n",
    "classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_model.fit(features, y_labeled, batch_size=32, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f11632ec-8e81-4a26-af31-438c328d1441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有标签样本: (21850, 64, 3)\n",
      "无标签样本: (0,)\n",
      "没有无标签数据，生成随机数据用于对比学习\n",
      "Epoch 1/10, loss=0.0444\n",
      "Epoch 2/10, loss=0.0828\n",
      "Epoch 3/10, loss=0.1132\n",
      "Epoch 4/10, loss=0.0061\n",
      "Epoch 5/10, loss=0.0170\n",
      "Epoch 6/10, loss=0.0033\n",
      "Epoch 7/10, loss=0.0030\n",
      "Epoch 8/10, loss=0.0056\n",
      "Epoch 9/10, loss=0.0038\n",
      "Epoch 10/10, loss=0.0009\n",
      "683/683 [==============================] - 14s 21ms/step\n",
      "Epoch 1/20\n",
      "547/547 [==============================] - 3s 4ms/step - loss: 0.6830 - accuracy: 0.5771 - val_loss: 0.6396 - val_accuracy: 0.5666\n",
      "Epoch 2/20\n",
      "547/547 [==============================] - 2s 3ms/step - loss: 0.5944 - accuracy: 0.7058 - val_loss: 0.5368 - val_accuracy: 0.9078\n",
      "Epoch 3/20\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.4733 - accuracy: 0.8989 - val_loss: 0.4209 - val_accuracy: 0.8723\n",
      "Epoch 4/20\n",
      "547/547 [==============================] - 2s 3ms/step - loss: 0.3532 - accuracy: 0.9527 - val_loss: 0.3038 - val_accuracy: 0.9506\n",
      "Epoch 5/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.2706 - accuracy: 0.9523 - val_loss: 0.2410 - val_accuracy: 0.9432\n",
      "Epoch 6/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.2200 - accuracy: 0.9506 - val_loss: 0.1980 - val_accuracy: 0.9561\n",
      "Epoch 7/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9551 - val_loss: 0.1729 - val_accuracy: 0.9641\n",
      "Epoch 8/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1647 - accuracy: 0.9550 - val_loss: 0.1524 - val_accuracy: 0.9568\n",
      "Epoch 9/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1478 - accuracy: 0.9569 - val_loss: 0.1387 - val_accuracy: 0.9609\n",
      "Epoch 10/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1362 - accuracy: 0.9580 - val_loss: 0.1288 - val_accuracy: 0.9570\n",
      "Epoch 11/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9590 - val_loss: 0.1277 - val_accuracy: 0.9595\n",
      "Epoch 12/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1205 - accuracy: 0.9590 - val_loss: 0.1167 - val_accuracy: 0.9551\n",
      "Epoch 13/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1140 - accuracy: 0.9589 - val_loss: 0.1095 - val_accuracy: 0.9584\n",
      "Epoch 14/20\n",
      "547/547 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.1051 - val_accuracy: 0.9664\n",
      "Epoch 15/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1065 - accuracy: 0.9606 - val_loss: 0.1061 - val_accuracy: 0.9551\n",
      "Epoch 16/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1025 - accuracy: 0.9621 - val_loss: 0.1020 - val_accuracy: 0.9618\n",
      "Epoch 17/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.1008 - accuracy: 0.9602 - val_loss: 0.0966 - val_accuracy: 0.9648\n",
      "Epoch 18/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.0966 - accuracy: 0.9619 - val_loss: 0.0992 - val_accuracy: 0.9600\n",
      "Epoch 19/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.0962 - accuracy: 0.9621 - val_loss: 0.0948 - val_accuracy: 0.9590\n",
      "Epoch 20/20\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 0.0939 - accuracy: 0.9625 - val_loss: 0.0902 - val_accuracy: 0.9657\n",
      "INFO:tensorflow:Assets written to: /tmp/tmptsmczowc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptsmczowc/assets\n",
      "2025-08-16 13:19:09.670200: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-16 13:19:09.670258: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-16 13:19:09.670433: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmptsmczowc\n",
      "2025-08-16 13:19:09.676125: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-16 13:19:09.676156: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmptsmczowc\n",
      "2025-08-16 13:19:09.693957: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-16 13:19:09.729499: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmptsmczowc\n",
      "2025-08-16 13:19:09.757406: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 86972 microseconds.\n",
      "2025-08-16 13:19:09.862035: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<1x?x64xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite model: lstm_encoder.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp2a9orrqm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2a9orrqm/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite model: classifier.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 13:19:10.206821: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-08-16 13:19:10.206906: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-08-16 13:19:10.207066: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp2a9orrqm\n",
      "2025-08-16 13:19:10.207474: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-08-16 13:19:10.207483: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp2a9orrqm\n",
      "2025-08-16 13:19:10.208812: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-08-16 13:19:10.229718: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp2a9orrqm\n",
      "2025-08-16 13:19:10.236348: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 29282 microseconds.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# -----------------------\n",
    "# 参数\n",
    "# -----------------------\n",
    "DATA_GLOB = \"./data/*.csv\"  # 数据路径\n",
    "SEQ_LEN = 64\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_CONTRASTIVE = 10\n",
    "EPOCHS_CLASSIFIER = 20\n",
    "\n",
    "# -----------------------\n",
    "# 1. 加载 CSV 数据\n",
    "# -----------------------\n",
    "X_labeled_list, y_labeled_list = [], []\n",
    "X_unlabeled_list = []\n",
    "\n",
    "for file in glob.glob(DATA_GLOB):\n",
    "    df = pd.read_csv(file).fillna(-1)  # NaN 当作无标签\n",
    "    data = df.values.astype(np.float32)\n",
    "    \n",
    "    for i in range(len(data) - SEQ_LEN + 1):\n",
    "        window = data[i:i+SEQ_LEN, :-1]\n",
    "        label = data[i+SEQ_LEN-1, -1]\n",
    "        if label == -1:  # 无标签\n",
    "            X_unlabeled_list.append(window)\n",
    "        else:           # 有标签\n",
    "            X_labeled_list.append(window)\n",
    "            y_labeled_list.append(int(label))\n",
    "\n",
    "X_labeled = np.array(X_labeled_list)\n",
    "y_labeled = np.array(y_labeled_list)\n",
    "X_unlabeled = np.array(X_unlabeled_list)\n",
    "\n",
    "print(\"有标签样本:\", X_labeled.shape)\n",
    "print(\"无标签样本:\", X_unlabeled.shape)\n",
    "\n",
    "# -----------------------\n",
    "# 2. 对比学习辅助函数\n",
    "# -----------------------\n",
    "def augment_window(x):\n",
    "    return x + np.random.normal(0, 0.01, x.shape)\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for x in X:\n",
    "        anchors.append(x)\n",
    "        positives.append(augment_window(x))\n",
    "    return np.stack(anchors), np.stack(positives)\n",
    "\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, z_i, z_j):\n",
    "        z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "        z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "        logits = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_i)[0])\n",
    "        loss_i = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_j = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        return tf.reduce_mean(loss_i + loss_j)\n",
    "\n",
    "# -----------------------\n",
    "# 3. LSTM 编码器\n",
    "# -----------------------\n",
    "NUM_FEATS = X_labeled.shape[2] if len(X_labeled) > 0 else 10  # 没有有标签时默认10\n",
    "lstm_encoder = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, NUM_FEATS)),\n",
    "    layers.LSTM(FEATURE_DIM, return_sequences=False),\n",
    "    layers.Dense(FEATURE_DIM, activation='relu')\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# 4. 对比学习训练（可选）\n",
    "# -----------------------\n",
    "if len(X_unlabeled) == 0:\n",
    "    print(\"没有无标签数据，生成随机数据用于对比学习\")\n",
    "    X_unlabeled = np.random.randn(100, SEQ_LEN, NUM_FEATS).astype(np.float32)\n",
    "\n",
    "anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(1024).batch(BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(EPOCHS_CONTRASTIVE):\n",
    "    for a, p in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_a = lstm_encoder(a, training=True)\n",
    "            z_p = lstm_encoder(p, training=True)\n",
    "            loss = ContrastiveLoss()(z_a, z_p)\n",
    "        grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_CONTRASTIVE}, loss={loss.numpy():.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 5. 有监督特征 + 分类头训练\n",
    "# -----------------------\n",
    "if len(X_labeled) > 0:\n",
    "    features_labeled = lstm_encoder.predict(X_labeled)\n",
    "    classifier = models.Sequential([\n",
    "        layers.Input(shape=(FEATURE_DIM,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    classifier.compile(optimizer='adam',\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "    classifier.fit(features_labeled, y_labeled,\n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   epochs=EPOCHS_CLASSIFIER,\n",
    "                   validation_split=0.2)\n",
    "\n",
    "# -----------------------\n",
    "# 6. TFLite 导出\n",
    "# -----------------------\n",
    "def save_tflite(model, out_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                           tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved TFLite model:\", out_path)\n",
    "\n",
    "save_tflite(lstm_encoder, \"lstm_encoder.tflite\")\n",
    "if len(X_labeled) > 0:\n",
    "    save_tflite(classifier, \"classifier.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e22c65-0a2d-49d5-bef4-e70b07751c2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./data/sensor_data_0.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_1.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_2.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_3.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_4.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_5.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_6.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_7.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_8.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_9.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_10.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_11.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_12.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_13.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_14.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_15.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_16.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_17.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_18.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_19.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_20.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_21.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_22.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_23.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_24.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_25.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_26.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_27.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_28.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_29.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_30.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_31.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_32.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_33.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_34.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_35.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_36.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_37.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_38.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_39.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_40.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_41.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_42.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_43.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_44.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_45.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_46.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_47.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_48.csv, shape: (500, 4)\n",
      "Saved ./data/sensor_data_49.csv, shape: (500, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==== 参数 ====\n",
    "SAVE_DIR = \"./data\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "NUM_FILES = 50       # 生成多少个 CSV 文件\n",
    "SEQ_LEN = 500       # 每个文件的长度\n",
    "NUM_CLASSES = 3     # 分类标签数量，0/1\n",
    "NOISE_STD = 0.05    # 模拟噪声大小\n",
    "\n",
    "# ==== 随机生成传感器数据 ====\n",
    "for i in range(NUM_FILES):\n",
    "    # 模拟温度、湿度、光照\n",
    "    t = 20 + 5 * np.sin(np.linspace(0, 10, SEQ_LEN)) + np.random.randn(SEQ_LEN) * NOISE_STD\n",
    "    h = 50 + 10 * np.cos(np.linspace(0, 5, SEQ_LEN)) + np.random.randn(SEQ_LEN) * NOISE_STD\n",
    "    l = 300 + 50 * np.sin(np.linspace(0, 3, SEQ_LEN)) + np.random.randn(SEQ_LEN) * NOISE_STD\n",
    "\n",
    "    # 简单生成标签：假设 temp > 22 就标 1，否则 0（仅作示例）\n",
    "    label = (t > 22).astype(int)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"temp\": t,\n",
    "        \"humid\": h,\n",
    "        \"light\": l,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    file_path = os.path.join(SAVE_DIR, f\"sensor_data_{i}.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}, shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03fc97df-3931-48bb-8599-5c5e8aa138fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./data/plant_seq_with_insect_0.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_1.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_2.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_3.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_4.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_5.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_6.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_7.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_8.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_9.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_10.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_11.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_12.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_13.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_14.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_15.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_16.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_17.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_18.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_19.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_20.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_21.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_22.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_23.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_24.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_25.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_26.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_27.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_28.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_29.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_30.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_31.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_32.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_33.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_34.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_35.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_36.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_37.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_38.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_39.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_40.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_41.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_42.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_43.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_44.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_45.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_46.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_47.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_48.csv, shape: (1000, 4)\n",
      "Saved ./data/plant_seq_with_insect_49.csv, shape: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SAVE_DIR = \"./data\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "NUM_FILES = 50        # 生成几个时序文件\n",
    "SEQ_LEN = 1000       # 每个文件的长度\n",
    "NOISE_STD = 0.5      # 噪声强度\n",
    "\n",
    "def generate_plant_sequence(seq_len=1000, noise_std=0.5, insect_prob=0.3):\n",
    "    t, h, l, labels = [], [], [], []\n",
    "\n",
    "    # 随机选择是否引入虫害\n",
    "    insect_event = np.random.rand() < insect_prob\n",
    "    insect_start = np.random.randint(300, 800) if insect_event else -1\n",
    "    insect_end   = insect_start + np.random.randint(50, 150) if insect_event else -1\n",
    "\n",
    "    for step in range(seq_len):\n",
    "        # ========= 生命周期阶段 =========\n",
    "        if step < 200:   # 育苗期\n",
    "            base_t, base_h, base_l = 22, 65, 250\n",
    "        elif step < 600: # 生长期\n",
    "            base_t, base_h, base_l = 25, 58, 400\n",
    "        else:            # 开花期\n",
    "            base_t, base_h, base_l = 28, 48, 600\n",
    "\n",
    "        # 基础波动 + 噪声\n",
    "        ti = base_t + np.sin(step/50) + np.random.randn() * noise_std\n",
    "        hi = base_h + np.cos(step/70) + np.random.randn() * noise_std\n",
    "        li = base_l + np.sin(step/100) * 20 + np.random.randn() * noise_std * 5\n",
    "\n",
    "        # ========= 虫害事件 =========\n",
    "        if insect_event and insect_start <= step <= insect_end:\n",
    "            li *= np.random.uniform(0.6, 0.8)  # 光照下降\n",
    "            hi += np.random.uniform(-5, 5)     # 湿度异常波动\n",
    "            label = 2   # 虫害也标为 \"不健康\"\n",
    "        else:\n",
    "            # ========= 标签 =========\n",
    "            if (ti < 10) or (li < 100):\n",
    "                label = 1  # 非植物\n",
    "            elif (ti < 15) or (ti > 35) or (hi < 30) or (hi > 80) or (li > 800):\n",
    "                label = 2  # 不健康\n",
    "            else:\n",
    "                label = 0  # 健康\n",
    "\n",
    "        t.append(ti)\n",
    "        h.append(hi)\n",
    "        l.append(li)\n",
    "        labels.append(label)\n",
    "\n",
    "    return pd.DataFrame({\"temp\": t, \"humid\": h, \"light\": l, \"label\": labels})\n",
    "\n",
    "# ==== 批量生成 ====\n",
    "for i in range(NUM_FILES):\n",
    "    df = generate_plant_sequence(SEQ_LEN, NOISE_STD)\n",
    "    file_path = os.path.join(SAVE_DIR, f\"plant_seq_with_insect_{i}.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}, shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c940c-7587-4626-b06a-c68902849ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv2)",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
