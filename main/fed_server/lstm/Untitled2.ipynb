{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1b7dcc-a31c-40cc-9377-35365cfde9ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# =============================\n",
    "# 超参数\n",
    "# =============================\n",
    "DATA_GLOB = \"./data/*.csv\"\n",
    "SEQ_LEN = 64\n",
    "FEATURE_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_CONTRASTIVE = 10\n",
    "EPOCHS_META = 20\n",
    "INNER_LR = 1e-2\n",
    "META_LR = 1e-3\n",
    "NUM_CLASSES = 3\n",
    "NUM_TASKS = 5\n",
    "SUPPORT_SIZE = 10\n",
    "QUERY_SIZE = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# =============================\n",
    "# 1) 加载 CSV -> 滑窗样本\n",
    "# =============================\n",
    "X_labeled_list, y_labeled_list = [], []\n",
    "X_unlabeled_list = []\n",
    "\n",
    "for file in sorted(glob.glob(DATA_GLOB)):\n",
    "    df = pd.read_csv(file).fillna(-1)\n",
    "    data = df.values.astype(np.float32)\n",
    "    feats, labels = data[:, :-1], data[:, -1]\n",
    "    for i in range(len(data) - SEQ_LEN + 1):\n",
    "        w_x = feats[i:i + SEQ_LEN]\n",
    "        w_y = labels[i + SEQ_LEN - 1]\n",
    "        if w_y == -1:\n",
    "            X_unlabeled_list.append(w_x)\n",
    "        else:\n",
    "            X_labeled_list.append(w_x)\n",
    "            y_labeled_list.append(int(w_y))\n",
    "\n",
    "X_unlabeled = np.array(X_unlabeled_list, dtype=np.float32) if len(X_unlabeled_list) > 0 else np.empty((0,), dtype=np.float32)\n",
    "if len(X_labeled_list) > 0:\n",
    "    X_labeled = np.array(X_labeled_list, dtype=np.float32)\n",
    "    y_labeled = np.array(y_labeled_list, dtype=np.int32)\n",
    "else:\n",
    "    X_labeled = np.empty((0, SEQ_LEN, X_unlabeled.shape[2] if X_unlabeled.size > 0 else 3), dtype=np.float32)\n",
    "    y_labeled = np.empty((0,), dtype=np.int32)\n",
    "\n",
    "NUM_FEATS = X_labeled.shape[2] if X_labeled.size > 0 else (X_unlabeled.shape[2] if X_unlabeled.size > 0 else 3)\n",
    "\n",
    "# =============================\n",
    "# 2) 对比学习\n",
    "# =============================\n",
    "def augment_window(x):\n",
    "    return x + np.random.normal(0, 0.01, x.shape).astype(np.float32)\n",
    "\n",
    "def make_contrastive_pairs(X):\n",
    "    anchors, positives = [], []\n",
    "    for w in X:\n",
    "        anchors.append(w)\n",
    "        positives.append(augment_window(w))\n",
    "    return np.stack(anchors).astype(np.float32), np.stack(positives).astype(np.float32)\n",
    "\n",
    "class NTXentLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    def call(self, z_i, z_j):\n",
    "        z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "        z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "        logits = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature\n",
    "        labels = tf.range(tf.shape(z_i)[0])\n",
    "        loss_i = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_j = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        return tf.reduce_mean(loss_i + loss_j)\n",
    "\n",
    "def build_lstm_encoder(seq_len, num_feats, feature_dim=FEATURE_DIM):\n",
    "    inp = layers.Input(shape=(seq_len, num_feats))\n",
    "    x = layers.LSTM(feature_dim)(inp)\n",
    "    out = layers.Dense(feature_dim, activation=\"relu\")(x)\n",
    "    return models.Model(inp, out, name=\"lstm_encoder\")\n",
    "\n",
    "lstm_encoder = build_lstm_encoder(SEQ_LEN, NUM_FEATS, FEATURE_DIM)\n",
    "contrastive_opt = tf.keras.optimizers.Adam()\n",
    "ntxent = NTXentLoss(temperature=0.2)\n",
    "\n",
    "# 若无无标签数据，生成一些随机样本以便跑通流程\n",
    "if X_unlabeled.size == 0:\n",
    "    X_unlabeled = np.random.randn(200, SEQ_LEN, NUM_FEATS).astype(np.float32)\n",
    "\n",
    "anchors, positives = make_contrastive_pairs(X_unlabeled)\n",
    "contrast_ds = tf.data.Dataset.from_tensor_slices((anchors, positives)).shuffle(2048).batch(BATCH_SIZE)\n",
    "\n",
    "contrastive_loss_history = []\n",
    "for ep in range(EPOCHS_CONTRASTIVE):\n",
    "    for a, p in contrast_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            za = lstm_encoder(a, training=True)\n",
    "            zp = lstm_encoder(p, training=True)\n",
    "            loss = ntxent(za, zp)\n",
    "        grads = tape.gradient(loss, lstm_encoder.trainable_variables)\n",
    "        contrastive_opt.apply_gradients(zip(grads, lstm_encoder.trainable_variables))\n",
    "    contrastive_loss_history.append(float(loss.numpy()))\n",
    "    print(f\"[Contrastive] Epoch {ep+1}/{EPOCHS_CONTRASTIVE}, loss={float(loss.numpy()):.4f}\")\n",
    "\n",
    "# =============================\n",
    "# 3) FOMAML 元学习 + LLL Replay\n",
    "# =============================\n",
    "def build_meta_model(encoder, num_classes=NUM_CLASSES):\n",
    "    inp = layers.Input(shape=(SEQ_LEN, NUM_FEATS))\n",
    "    x = encoder(inp)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"meta_lstm_classifier\")\n",
    "\n",
    "meta_model = build_meta_model(lstm_encoder, NUM_CLASSES)\n",
    "meta_optimizer = tf.keras.optimizers.Adam(META_LR)\n",
    "\n",
    "def sample_tasks(X, y, num_tasks=NUM_TASKS, support_size=SUPPORT_SIZE, query_size=QUERY_SIZE):\n",
    "    tasks = []\n",
    "    n = len(X)\n",
    "    if n < support_size + query_size:\n",
    "        raise ValueError(f\"样本不足以构建任务：需要 {support_size+query_size}，但只有 {n}\")\n",
    "    for _ in range(num_tasks):\n",
    "        idx = np.random.choice(n, support_size + query_size, replace=False)\n",
    "        X_support, y_support = X[idx[:support_size]], y[idx[:support_size]]\n",
    "        X_query, y_query = X[idx[support_size:]], y[idx[support_size:]]\n",
    "        tasks.append((X_support, y_support, X_query, y_query))\n",
    "    return tasks\n",
    "\n",
    "# ---------- inner update (FOMAML / 一阶 MAML) ----------\n",
    "def inner_update(model, X_support, y_support, lr_inner=INNER_LR):\n",
    "    # 仅计算一阶梯度，不保留二阶\n",
    "    with tf.GradientTape() as tape_inner:\n",
    "        preds_support = model(X_support, training=True)\n",
    "        loss_support = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(y_support, preds_support)\n",
    "        )\n",
    "    grads_inner = tape_inner.gradient(loss_support, model.trainable_variables)\n",
    "    updated_vars = [w - lr_inner * g for w, g in zip(model.trainable_variables, grads_inner)]\n",
    "    return updated_vars, float(loss_support.numpy())\n",
    "\n",
    "# ============== LLL Replay Buffer ==============\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=500):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def add(self, X, y):\n",
    "        for xi, yi in zip(X, y):\n",
    "            self.buffer.append((xi, yi))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    def sample(self, batch_size=32):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        idxs = np.random.choice(len(self.buffer), size=batch_size, replace=False)\n",
    "        X, y = zip(*[self.buffer[i] for i in idxs])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# 初始化 buffer\n",
    "memory = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# ------------- 外环 + LLL 正则 -------------\n",
    "def outer_update_with_lll(meta_model, meta_optimizer, tasks,\n",
    "                          lr_inner=INNER_LR, replay_weight=0.3, replay_bs=32):\n",
    "    # 累积 meta 梯度\n",
    "    meta_grads = [tf.zeros_like(v) for v in meta_model.trainable_variables]\n",
    "    query_acc_list, query_loss_list = [], []\n",
    "\n",
    "    for (X_support, y_support, X_query, y_query) in tasks:\n",
    "        # 备份原始权重\n",
    "        orig_vars = [tf.identity(v) for v in meta_model.trainable_variables]\n",
    "\n",
    "        # --- inner update on support ---\n",
    "        updated_vars, support_loss = inner_update(meta_model, X_support, y_support, lr_inner=lr_inner)\n",
    "\n",
    "        # 将更新后的权重写回（FOMAML：不跟踪内层更新的二阶梯度）\n",
    "        for var, upd in zip(meta_model.trainable_variables, updated_vars):\n",
    "            var.assign(upd)\n",
    "\n",
    "        # --- 计算 query loss (+ LLL replay) 并对 updated_vars 求梯度 ---\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds_q = meta_model(X_query, training=True)\n",
    "            loss_q = tf.reduce_mean(\n",
    "                tf.keras.losses.sparse_categorical_crossentropy(y_query, preds_q)\n",
    "            )\n",
    "\n",
    "            # LLL replay regularization（与当前 updated_vars 的输出一致性）\n",
    "            if len(memory) >= 8:  # 足量才抽样\n",
    "                X_old, y_old = memory.sample(batch_size=replay_bs)\n",
    "                preds_old = meta_model(X_old, training=True)\n",
    "                replay_loss = tf.reduce_mean(\n",
    "                    tf.keras.losses.sparse_categorical_crossentropy(y_old, preds_old)\n",
    "                )\n",
    "                loss_total = (1.0 - replay_weight) * loss_q + replay_weight * replay_loss\n",
    "            else:\n",
    "                loss_total = loss_q\n",
    "\n",
    "        grads = tape.gradient(loss_total, meta_model.trainable_variables)\n",
    "        meta_grads = [mg + g / len(tasks) for mg, g in zip(meta_grads, grads)]\n",
    "\n",
    "        # 统计 query acc/loss\n",
    "        q_acc = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(preds_q, axis=1), y_query), tf.float32)\n",
    "        )\n",
    "        query_acc_list.append(float(q_acc.numpy()))\n",
    "        query_loss_list.append(float(loss_q.numpy()))\n",
    "\n",
    "        # 恢复原始权重，以便处理下一任务（累积梯度仍保留）\n",
    "        for var, orig in zip(meta_model.trainable_variables, orig_vars):\n",
    "            var.assign(orig)\n",
    "\n",
    "        # 更新 LLL 记忆（用原始样本即可）\n",
    "        memory.add(X_support, y_support)\n",
    "        memory.add(X_query, y_query)\n",
    "\n",
    "    # 应用一次 meta 更新\n",
    "    meta_optimizer.apply_gradients(zip(meta_grads, meta_model.trainable_variables))\n",
    "\n",
    "    # 返回平均指标\n",
    "    return float(np.mean(query_loss_list)), float(np.mean(query_acc_list))\n",
    "\n",
    "# ======= 训练元学习 =======\n",
    "meta_loss_history, meta_acc_history = [], []\n",
    "\n",
    "if X_labeled.size > 0:\n",
    "    for ep in range(EPOCHS_META):\n",
    "        tasks = sample_tasks(X_labeled, y_labeled)\n",
    "        loss, q_acc = outer_update_with_lll(meta_model, meta_optimizer, tasks)\n",
    "        meta_loss_history.append(loss)\n",
    "        meta_acc_history.append(q_acc)\n",
    "        print(f\"[Meta] Epoch {ep+1}/{EPOCHS_META}, query_loss={loss:.4f}, query_acc={q_acc:.4f}\")\n",
    "else:\n",
    "    print(\"跳过元学习：没有有标签数据。\")\n",
    "\n",
    "# =============================\n",
    "# 4) 绘制效果图\n",
    "# =============================\n",
    "plt.figure()\n",
    "plt.plot(contrastive_loss_history, label=\"Contrastive Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Contrastive Learning Loss\")\n",
    "plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "if len(meta_loss_history) > 0:\n",
    "    plt.figure()\n",
    "    plt.plot(meta_loss_history, label=\"Query Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"FOMAML+LLL - Query Loss\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(meta_acc_history, label=\"Query Accuracy\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"FOMAML+LLL - Query Accuracy\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "# =============================\n",
    "# 5) TFLite 导出\n",
    "# =============================\n",
    "def save_tflite(model, out_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Saved TFLite:\", out_path)\n",
    "\n",
    "save_tflite(lstm_encoder, \"lstm_encoder_contrastive.tflite\")\n",
    "if X_labeled.size > 0:\n",
    "    save_tflite(meta_model, \"meta_lstm_classifier.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099fa1c6-4074-4758-bf64-ba443eda9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../../../../data/plant_seq_with_insect_0.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_1.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_2.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_3.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_4.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_5.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_6.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_7.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_8.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_9.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_10.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_11.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_12.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_13.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_14.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_15.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_16.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_17.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_18.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_19.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_20.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_21.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_22.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_23.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_24.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_25.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_26.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_27.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_28.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_29.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_30.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_31.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_32.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_33.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_34.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_35.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_36.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_37.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_38.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_39.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_40.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_41.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_42.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_43.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_44.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_45.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_46.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_47.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_48.csv, shape: (1000, 4)\n",
      "Saved ../../../../data/plant_seq_with_insect_49.csv, shape: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SAVE_DIR = \"../../../../data\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "NUM_FILES = 50        # 生成几个时序文件\n",
    "SEQ_LEN = 1000       # 每个文件的长度\n",
    "NOISE_STD = 0.5      # 噪声强度\n",
    "\n",
    "def generate_plant_sequence(seq_len=1000, noise_std=0.5, insect_prob=0.3):\n",
    "    t, h, l, labels = [], [], [], []\n",
    "\n",
    "    # 随机选择是否引入虫害\n",
    "    insect_event = np.random.rand() < insect_prob\n",
    "    insect_start = np.random.randint(300, 800) if insect_event else -1\n",
    "    insect_end   = insect_start + np.random.randint(50, 150) if insect_event else -1\n",
    "\n",
    "    for step in range(seq_len):\n",
    "        # ========= 生命周期阶段 =========\n",
    "        if step < 200:   # 育苗期\n",
    "            base_t, base_h, base_l = 22, 65, 250\n",
    "        elif step < 600: # 生长期\n",
    "            base_t, base_h, base_l = 25, 58, 400\n",
    "        else:            # 开花期\n",
    "            base_t, base_h, base_l = 28, 48, 600\n",
    "\n",
    "        # 基础波动 + 噪声\n",
    "        ti = base_t + np.sin(step/50) + np.random.randn() * noise_std\n",
    "        hi = base_h + np.cos(step/70) + np.random.randn() * noise_std\n",
    "        li = base_l + np.sin(step/100) * 20 + np.random.randn() * noise_std * 5\n",
    "\n",
    "        # ========= 虫害事件 =========\n",
    "        if insect_event and insect_start <= step <= insect_end:\n",
    "            li *= np.random.uniform(0.6, 0.8)  # 光照下降\n",
    "            hi += np.random.uniform(-5, 5)     # 湿度异常波动\n",
    "            label = 2   # 虫害也标为 \"不健康\"\n",
    "        else:\n",
    "            # ========= 标签 =========\n",
    "            if (ti < 10) or (li < 100):\n",
    "                label = 1  # 非植物\n",
    "            elif (ti < 15) or (ti > 35) or (hi < 30) or (hi > 80) or (li > 800):\n",
    "                label = 2  # 不健康\n",
    "            else:\n",
    "                label = 0  # 健康\n",
    "\n",
    "        t.append(ti)\n",
    "        h.append(hi)\n",
    "        l.append(li)\n",
    "        labels.append(label)\n",
    "\n",
    "    return pd.DataFrame({\"temp\": t, \"humid\": h, \"light\": l, \"label\": labels})\n",
    "\n",
    "# ==== 批量生成 ====\n",
    "for i in range(NUM_FILES):\n",
    "    df = generate_plant_sequence(SEQ_LEN, NOISE_STD)\n",
    "    file_path = os.path.join(SAVE_DIR, f\"plant_seq_with_insect_{i}.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}, shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e57b1-d400-40a1-99f3-816d745cf733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
